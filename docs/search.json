[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Business Analytics & Coding",
    "section": "",
    "text": "VORWORT\nBei diesem Skript handelt es sich um Selbstlernmaterial zum Thema Datenanalyse. Noch konkreter beschäftigt sich dieses Skript mit Business Analytics. Das Skript wurde neu konzipiert und richtet sich explizit an Studierende der Betriebswirtschaftlehre. Aktuell “atmet” das Dokument noch, d.h. Aktualisierungen und Ergänzungen sind sehr wahrscheinlich. Auch werden perspektivisch fortgeschrittenere Themenblöcke zum Bereich der Predictive Analytics ergänzt werden.\nDas Skript ist explizit mit dem Ziel verfasst, als Selbstlernmaterial verwendet zu werden. Zumindest ist dies beim Verfassen des Skripts immer meine ausdrückliche Intention gewesen. Alle Code-Beispiele können via Knopfdruck selbst ausprobiert und adaptiert werden. Das Dokument ist somit zu Teilen interaktiv und sollte deshalb auch nicht nur passiv konsumiert werden.\nOb mein Vorhaben tatsächlich gelungen ist, können schlussendlich nur Sie beurteilen. Sollten Sie also Fragen oder Anmerkungen bzw. technische Probleme haben oder - und dies bleibt leider nie aus - Fehler entdecken, dann bin ich dankbar für Ihr Feedback. Sprechen Sie mich entweder in den Veranstaltungen zum Modul direkt an oder schreiben mir eine Email (felix.zeidler[at]fh-bielefeld.de). Auch wenn es mir vermutlich nicht immer gelingen wird, Ihnen direkt zu antworten, bin ich dankbar für jede Art von Rückmeldung.\nViel Spaß beim Lesen und Ausprobieren!\nProf. Dr. Felix Zeidler"
  },
  {
    "objectID": "Chapters/01_Chapter/Einleitung.html",
    "href": "Chapters/01_Chapter/Einleitung.html",
    "title": "EINLEITUNG",
    "section": "",
    "text": "In diesm Kapitel beschäftigen wir uns mit drei Kernfragen:\n\n\nWas ist “Business Analytics”?\nWelche Rolle spielt “Coding” in der Datenanalyse?\nWieso Python die geeignete Programmiersprache für unsere Zwecke ist?"
  },
  {
    "objectID": "Chapters/01_Chapter/Was_ist_BA.html#was-ist-business-analytics",
    "href": "Chapters/01_Chapter/Was_ist_BA.html#was-ist-business-analytics",
    "title": "Business Analytics",
    "section": "Was ist Business Analytics?",
    "text": "Was ist Business Analytics?\nBevor wir uns mit Business Analytics inhaltlich weitergehend beschäftigen, müssen wir zunächst definieren, was der Begriff konkret meint. Im Kontext dieser Veranstaltung definieren wir Business Analytics als\n\n“Ein Prozess zur Analyse von Daten mit dem Ziel unternehmerische Entscheidungen zu unterstützen und zu verbessern.”\n\nDer Begriff Business Analytics wird in der Literatur unterschiedlich definiert.1 Für manche Autoren umfasst er alle Formen der Analyse von Unternehmensdaten, während andere eine engere Definition bevorzugen, die sich auf die Verwendung von maschinellen Lernverfahren und anderen fortgeschrittenen Analysemethoden konzentriert. Trotz dieser Unterschiede in der Definition haben alle Ansätze zu Business Analytics eines gemeinsam: Sie zielen darauf ab, durch die Analyse von Daten unternehmerische Entscheidungen zu unterstützen und zu verbessern. Business Analytics ist daher als wichtige Disziplin zu betrachten, die Unternehmen dabei hilft, ihre Leistung zu steigern und ihre Entscheidungen datengestützt zu treffen.1 siehe z.B. Seiter (2019) oder [TODO]\nDer Fokus von Business Analytics auf unternehmerische Entscheidungen macht es zu einem wichtigen Thema für kaufmännische Funktionen innerhalb von Unternehmen. Denn gerade in diesen Bereichen werden häufig Entscheidungen getroffen, die den Erfolg und die Leistung des Unternehmens beeinflussen. Business Analytics kann dabei helfen, wichtige Daten und Insights zu sammeln und zu analysieren, um die Entscheidungsfindung zu unterstützen und die Effektivität von Strategien und Maßnahmen zu verbessern. Daher ist es wichtig, dass Führungskräfte und Mitarbeiter in kaufmännischen Funktionen sich mit den Konzepten und Methoden von Business Analytics auseinandersetzen und lernen, wie sie diese in ihrer täglichen Arbeit anwenden können.\nIn der untenstehenden Tabelle 1.1 sind ein paar beispielhafte Anwendungsfälle dargestellt, die verdeutlichen, wie breit die Anwendung von Business Analytics ist:\n\n\nTabelle 1.1: Beispiele für Business Analytics\n\n\n\n\n\n\n\nFunktion\nAnwendungsfall\nBeispiel\n\n\n\n\nMarketing\nWirksamkeit von Marketingkampagnen analysieren und optimieren\nein Unternehmen könnte die Conversion-Rate von Landing Pages oder die Klickrate von Email-Kampagnen analysieren, um zu verstehen, welche Maßnahmen am effektivsten sind\n\n\nFinanzen\nfinanzielle Leistung analysieren und optimieren\nein Unternehmen könnte die Rentabilität von einzelnen Produkten oder Geschäftsbereichen analysieren, um Ressourcen gezielt einzusetzen und die Profitabilität zu erhöhen\n\n\nPersonal\nMitarbeiterleistung analysieren und verbessern\nein Unternehmen könnte Daten zu Mitarbeiterfeedback, Absentismus und Fluktuation analysieren, um die Mitarbeiterzufriedenheit und -fluktuation zu erhöhen\n\n\nLogistik\nLeistung der Lieferkette analysieren und optimieren\nein Unternehmen könnte Daten zu Lieferzeiten, Bestandsniveaus und Transportkosten analysieren, um die Effizienz der Lieferkette zu erhöhen und Lieferprobleme zu minimieren\n\n\n\n\nEs wird außerdem deutlich, dass sich abhängig vom jeweiligen Anwendungsfall auch die verwendeten Daten und Analysetechniken unterscheiden können."
  },
  {
    "objectID": "Chapters/01_Chapter/Was_ist_BA.html#vorteile-durch-business-analytics",
    "href": "Chapters/01_Chapter/Was_ist_BA.html#vorteile-durch-business-analytics",
    "title": "Business Analytics",
    "section": "Vorteile durch Business Analytics?",
    "text": "Vorteile durch Business Analytics?\nAusgehend von der o.g. Definition kann die Datenanalyse auf sowohl strategischer, als auch auf operativer Ebene vorteilhaft sein.\nAuf operativer Ebene kann Der Einsatz von Business Analytics vorteilhaft sein, da er es Unternehmen ermöglicht, Daten zu sammeln und zu analysieren, um z.B. ihre Prozesse zu optimieren und/oder Kosten zu senken. Auf Basis dieser Verbesserungen kann man sich vom Wettbewerb unterscheiden und absetzen. Der Einsatz von Business Analytics kann auf strategischer Ebene vorteilhaft sein, da er es Unternehmen ermöglicht, Trends zu erkennen und auf sie zu reagieren, um sich einen Wettbewerbsvorteil zu verschaffen. Die Analyse von Daten kann Unternehmen dabei helfen, tiefgreifende Einblicke in die Märkte und Kundenbedürfnisse zu gewinnen und diese Informationen zu nutzen, um ihre Strategien anzupassen und sich von ihren Wettbewerbern abzuheben. Der Einsatz von Business Analytics kann somit dazu beitragen, dass Unternehmen ihre Wettbewerbsposition verbessern und ihre Leistung steigern.\nSo ist es nicht verwunderlich, dass es auch Belege dafür gibt, dass Unternehmen durch den Einsatz von Business Analytics erfolgreicher sind. So zeigen z.B. Shanks u. a. (2010) theoretisch auf, dass Business Analytics einen Einfluss auf die Strategie und die Performance von Unternehmen hat. Die vergleichende empirische Analyse von Popovič u. a. (2018) betont hingegen, dass Unternehmen, die hohen “Business Analytics”-Fähigkeiten haben, in der Lage sind bessere Entscheidungen zu treffen und so einen höheren Unternehmenswert generieren. In einer weiteren empirischen Analyse zeigen Almazmomi, Ilmudeen, und Qaffas (2021), dass der erhöhte Einsatz von Business Analytics einen Wettbewerbsvorteil darstellt."
  },
  {
    "objectID": "Chapters/01_Chapter/Was_ist_BA.html#drei-arten-der-datenanalyse",
    "href": "Chapters/01_Chapter/Was_ist_BA.html#drei-arten-der-datenanalyse",
    "title": "Business Analytics",
    "section": "Drei Arten der Datenanalyse",
    "text": "Drei Arten der Datenanalyse\nDas Feld von Business Analytics ist sehr weit. Es macht deshalb Sinn, das breite Themengebiet zu unterteilen. Wir werden im Rahmen dieses Kurses Business Analytics in die folgenden drei Analyse-Kategorien unterteilen:\n\ndeskriptive Analyse\ndiagnostische Analyse\nprädiktive Analyse\n\nIch gebe zu: die Unterteilung ist ein Stück weit beliebig und auch nicht MECE (mutual exclusive, collectively exhaustive). Ich halte Sie aus didaktischen Gründen dennoch geeignet sich dem breiten Themengebiet Business Analytics Schritt für Schritt zu nähern.22 Für eine ausführlichere Diskussion der unterschiedlichen Arten von Business Analytics siehe z.B. Gluchowski (2016).\nDie deskriptive Analytik ist die Art von Business Analytics, die sich auf die Vergangenheit konzentriert. Sie zielt darauf ab, ein besseres Verständnis dafür zu entwickeln, was in der Vergangenheit passiert ist, indem sie Daten sammelt und analysiert. Zum Beispiel könnte ein Unternehmen die Verkaufszahlen der vergangenen Jahre analysieren, um herauszufinden, welche Produkte am beliebtesten waren und wie sich die Verkäufe im Laufe der Zeit entwickelt haben. Die deskriptive Analytik kann dazu beitragen, Muster und Trends zu erkennen und die Leistung des Unternehmens besser zu verstehen.\nDie diagnostische Analytik geht einen Schritt weiter und versucht, die Ursachen für bestimmte Ereignisse oder Muster zu untersuchen. Sie nutzt Vergangenheitsdaten und bewährte statistiche Verfahren (z.B. lineare Regression), um Muster und Trends zu identifizieren und die Gründe für bestimmte Ereignisse oder Muster zu erklären. Zum Beispiel könnte ein Unternehmen die diagnostische Analytik nutzen, um herauszufinden, warum bestimmte Kunden häufiger Produkte zurückgeben oder warum die Umsätze in bestimmten Filialen niedriger sind als in anderen.\nDie prädiktive Analytik geht noch einen Schritt weiter und versucht, die Zukunft vorherzusagen. Sie nutzt Vergangenheitsdaten, bewährte statistische Verfahren und teilweise auch Verfahren des maschinellen Lernens, um Muster und Trends zu identifizieren und Vorhersagen für zukünftige Ereignisse zu treffen. Zum Beispiel könnte ein Unternehmen die prädiktive Analytik nutzen, um zu prognostizieren, wie sich die Nachfrage nach einem bestimmten Produkt in der Zukunft entwickeln wird, oder um Kundenverhalten vorherzusagen, um gezielte Marketingkampagnen zu erstellen.\nJede dieser Arten von Business Analytics kann Unternehmen dabei helfen, bessere Entscheidungen zu treffen. Oft werden die Analyseformen auch kombiniert bzw. es wird im Rahmen einer Analyse gar nicht unterschieden, um welche Kategorie es sich handelt. Im Rahmen des Moduls werden wir uns allen drei Arten widmen und uns Schritt für Schritt auch (vermeintlich) komplexen Analysen widmen."
  },
  {
    "objectID": "Chapters/01_Chapter/Was_ist_BA.html#sec-analyseprozess",
    "href": "Chapters/01_Chapter/Was_ist_BA.html#sec-analyseprozess",
    "title": "Business Analytics",
    "section": "Der Analyseprozess",
    "text": "Der Analyseprozess\nEingangs des Kapitels haben wir Business Analytics definiert und dabei festgehalten, dass es sich um einen Prozess handelt. Lassen Sie uns an dieser Stelle den Prozess der Analyse kurz beschreiben. Denn auch wenn dieser sich im Detail bei jeder Analyse natürlich unterscheidet, sind die grundsätzlichen Analyseschritte ganz unabhängig von der Art der Analyse und dem Analysezweck immer identisch. Die wesentlichen Prozessschritte lauten:\nDer Datenanalyse-Prozess umfasst eine Reihe von Schritten, die dazu dienen, ein bestimmtes Analyseproblem zu lösen. Die Schritte sind in der Regel untereinander abhängig und müssen in der richtigen Reihenfolge ausgeführt werden, um zu sinnvollen Ergebnissen zu gelangen.\n\nFormulierung der Problemstellung: Der erste Schritt bei der Datenanalyse ist die Definition des Analyseproblems. Dabei geht es darum, die Frage oder das Problem zu formulieren, das mit der Analyse beantwortet oder gelöst werden soll. Zudem muss festgelegt werden, welche Daten dazu benötigt werden und wo diese gefunden werden können. Beispielsweise könnte das Analyseproblem lauten: “Welche Faktoren beeinflussen die Kundenzufriedenheit in unserem Unternehmen?” Um dieses Problem zu lösen, müssen Daten zu Kundenzufriedenheit und möglichen Einflussfaktoren gesammelt werden.\nFinden und Einlesen der Daten: Nachdem das Analyseproblem definiert wurde, müssen die benötigten Daten gefunden und eingelesen werden. Diese können entweder aus internen Quellen, wie beispielsweise einer Unternehmensdatenbank, oder aus externen Quellen, wie öffentlich zugänglichen Datensätzen oder Daten, die von Kunden oder anderen Stakeholdern bereitgestellt werden, stammen. Es ist wichtig, sicherzustellen, dass die Daten korrekt und vollständig sind und dass alle relevanten Informationen enthalten sind.\nAufbereiten der Daten: Sobald die Daten eingelesen wurden, müssen sie aufbereitet werden. Dieser Schritt beinhaltet die Säuberung und Strukturierung der Daten, damit sie für die weitere Analyse nutzen können. Dazu gehört beispielsweise das Entfernen von fehlerhaften oder inkonsistenten Daten, das Umcodieren (z.B. ja/nein zu 1/0), die Umbennung von Spalten. Kurzum: Es geht darum, die Daten so aufzubereiten, dass sie für die Analyse nutzbar sind.\nTransformieren der Daten: Mit diesem Schritt beginnt der eigentliche Erkenntnisprozess, d.h. wir beginnen Daten zu analysierem, um daraus Erkenntnisse zu ziehen und Dinge zu lernen. Dafür werden z.B. aus bestehenden Daten neue Daten berechnet oder wir aggregiern, gruppieren und filtern Daten, um Erkenntnisse zu generieren. Wir transformieren den bestehenden Datensatz mit dem Ziel neue Informationen zu generieren, die uns bei der Beantwortung unserer Problemstellung helfen.\nVisualisieren der Daten: die Visualisierung von Daten geht oft mit der der Transformation (und auch mit der Modellierung) einher. Die Visualisierung hilft uns, Muster und Zusammenhänge zu erkennen und Ergebnisse zu veranschaulichen. Es gibt verschiedene Möglichkeiten, Daten zu visualisieren, wie beispielsweise Tabellen, Diagramme oder Karten. Das Ziel ist es, die Daten so darzustellen, dass sie leicht verständlich und interpretierbar sind.\nModellieren der Daten: In diesem Schritt werden Mathematische Modelle erstellt, um die Daten zu analysieren und Vorhersagen zu treffen. Dazu können beispielsweise Regressionsanalysen oder Klassifikationsmodelle verwendet werden. Das Ziel ist es, Zusammenhänge und Muster in den Daten zu erkennen und Vorhersagen für die Zukunft zu treffen.\nKommunizieren der Daten: Zum Schluss müssen die Ergebnisse der Datenanalyse kommuniziert werden. Dies kann in Form von Berichten, Präsentationen oder anderen Dokumenten geschehen, die die Ergebnisse verständlich und nachvollziehbar darstellen. Das Ziel ist es, die Ergebnisse der Analyse der relevanten Stakeholder zugänglich zu machen und sie in die Entscheidungsfindung einzubeziehen.\n\nDie folgende Darstellung soll den Analyseprozess in Anlehnung an Wickham und Grolemund (2016) abbilden. Die Schritte 4, 5 und 6 bezeichnen wir auch als explorative Analyse. Damit soll betont werden, dass es meist nicht um einen rein sequenziellen Prozess handelt, sondern die Schritte oft iterativ durchlaufen werden.\n\n\n\nDer Analyse-Prozess\n\n\nDie Abbildung verdeutlicht, dass es sich beim Analyse-Prozess um einen iterativen und teilweise explorativen Prozess handelt. Die Schritte “Transformation”, “Visualisierung” und “Modellierung” sind häufig iterative Prozessschritte, bei denen man auch explorativ vorgeht. Das bedeutet auch, man sich bewusst die Freiheit nimmt, unterschiedliche Verfahren auszuprobieren, um neue Erkenntnisse zu gewinnen oder dass man offen für neue Ideen und Ansätze bleibt und sich nicht zu früh auf eine bestimmte Vorgehensweise festlegt. Durch die iterative Durchführung dieser Schritte und das explorative Vorgehen kann man neue Erkenntnisse gewinnen und die Analyse vertiefen.\nZudem zeigt sich, dass die Schritte “Transformation”, “Visualisierung” und “Modellierung” im Datenanalyse-Prozess nicht unabhängig voneinander betrachtet werden können, sondern dass sie sich gegenseitig beeinflussen und bedingen. Beispielsweise könnte es bei der Transformation der Daten notwendig sein, mehrere Veränderungen an den Daten vorzunehmen, um sie für die geplante Analyse geeignet zu machen. Durch die Visualisierung der Daten kann es jedoch möglich sein, neue Muster oder Zusammenhänge zu erkennen, die dazu führen, dass die Daten anders transformiert werden müssen. Ähnlich verhält es sich bei der Modellierung der Daten. Hier können verschiedene Modelle verglichen und iterativ verbessert werden, um die besten Vorhersagen zu erhalten. Durch die Visualisierung der Ergebnisse kann es jedoch möglich sein, dass neue Erkenntnisse gewonnen werden, die dazu führen, dass das Modell anders aufgebaut werden muss. Es ist daher wichtig, dass diese Schritte nicht unabhängig voneinander betrachtet werden, sondern dass man sich bewusst die Freiheit nimmt, explorativ vorzugehen und die Schritte iterativ durchzuführen, um zu sinnvollen Ergebnissen zu gelangen.\n\n\n\n\nAlmazmomi, Najah, Aboobucker Ilmudeen, und Alaa A. Qaffas. 2021. „The impact of business analytics capability on data-driven culture and exploration: achieving a competitive advantage“. Benchmarking: An International Journal 29 (4): 1264–83. https://doi.org/10.1108/BIJ-01-2021-0021.\n\n\nGluchowski, Peter. 2016. „Business Analytics – Grundlagen, Methoden und Einsatzpotenziale“. HMD Praxis der Wirtschaftsinformatik 53 (3): 273–86. https://doi.org/10.1365/s40702-015-0206-5.\n\n\nPopovič, Aleš, Ray Hackney, Rana Tassabehji, und Mauro Castelli. 2018. „The impact of big data analytics on firms’ high value business performance“. Information Systems Frontiers 20 (2): 209–22. https://doi.org/10.1007/s10796-016-9720-4.\n\n\nSeiter, Mischa. 2019. Business Analytics: Wie Sie Daten für die Steuerung von Unternehmen nutzen. 2., komplett überarbeitete und erweiterte. München: Vahlen.\n\n\nShanks, Graeme, Rajeev Sharma, Peter Seddon, und Peter Reynolds. 2010. „The Impact of Strategy and Maturity on Business Analytics and Firm Performance: A Review and Research Agenda“. ACIS 2010 Proceedings, Januar. https://aisel.aisnet.org/acis2010/51.\n\n\nWickham, Hadley, und Garrett Grolemund. 2016. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. \"O’Reilly Media, Inc.\"."
  },
  {
    "objectID": "Chapters/01_Chapter/Coding.html#warum-programmieren-hilfreich-ist",
    "href": "Chapters/01_Chapter/Coding.html#warum-programmieren-hilfreich-ist",
    "title": "Werkzeug: Coding!",
    "section": "Warum Programmieren hilfreich ist",
    "text": "Warum Programmieren hilfreich ist\nWir glauben, dass die Fähigkeit zu Programmieren eine Grundlagenkompetenz ist, die jeder haben sollte. Diese Fähigkeit ist insbesondere, aber nicht nur, für die Datenanalyse hilfreich. Aus unserer Sicht gibt es drei Kernargumente für die Einführung von Programmierkenntnissen, denn Programmieren hilft:\n\nDaten effizient für Entscheidungsunterstützung aufbereiten und analysieren zu können\nrepetitive Aufgaben zu automatisieren\nProblemlösungsfähigkeiten zu verbessern\n\nLassen Sie uns die drei Punkte genauer betrachten.\n\nDaten effizient aufbereiten und analysieren\nProgrammierkenntnisse sind bisher kein Muss für BWLer. Jedoch verändert sich der Arbeitsalltag rasant. Die Analyse von Daten und Informationen zur Entscheidungsunterstützung ist immer eine Kernaufgabe in kaufmännischen Bereichen gewesen. Diese Aufgabe verändert sich aufgrund der verfügbaren Datenmengen, die für potentielle Analysen zur Verfügung stehen. Die Fähigkeit Daten effizient aufzubereiten und zu analysieren wird deshalb wichtiger. Programmierkenntnisse helfen hier, diesen Datenaufbereitungs- und analyseprozess zu beschleunigen und teilweise überhaupt erst zu ermöglichen.\nWarum nicht Excel?\nNun mag an dieser Stelle der Einwand kommen, dass es bereits ein Werkzeug dafür gibt, welches täglich von vielen genutzt wird: Excel (oder ein anderes Tabellenkalkulationsprogramm). In vielen Bereichen wir Excel seit Jahrzehnten für die Datenaufbereitung und Analyse mehr oder weniger sinnvoll eingesetzt. Warum also kein Kurs “Business Analytics mit Excel”? Der Hauptgrund ist, dass Excel nicht für die Analyse und insbesondere Aufbereitung von sehr großen Datenmengen entwickelt wurde. Große Datenmengen lassen sich wesentlich besser mit geeignetem “Skripten” bearbeiten. Zum einen, weil diese wesentlich schneller und stabiler in der Ausführung sind. Zum anderen weil durch die Programmierung eine klare Trennung zwischen Daten und Analyse gemacht wird. Diese Trennung gibt es in Excel nicht, da jede Tabelle typischerweise Daten und Analyselogik kombiniert. Dieses Vorgehen hat einen großen Vorteil - es unterstützt die explorative Analyse - jedoch erschwert es Dritten den Aufbereitungs- und Analyseprozess nachzuvollziehen. Da wir jedoch davon ausgehen, dass die meisten Leser*innen dieses Skriptes Excel beherrschen, arbeiten wir oft mit Excel-Analogien und Einführungsbeispielen.\nWarum nicht Business Intelligence Software?\nFür die Analyse und Aufbereitung von großen Datenmengen gibt es seit einigen Jahren gängige BI-Software (z.B. Microsoft Power BI oder Tableau). Auch diese Tools sind gut nutzbar und potentiell hilfreich für die Datenanlyse. Dennoch glauben wir, dass es wesentliche Argumente dafür gibt, diesen Kurs zu flankieren mit einer Einführung in die Programmierung.\nZum einen ist es so dass Business Intelligence Tools (BI-Tools) sich zwar wunderbar dazu eignen Visualisierungen vorzunehmen und Dashboard zu erstellen. In der Praxis ist es jedoch typischerweise so, dass wir die Daten, die visualisiert werden sollen zunächst aufbereiten müssen. Eine große Herausforderung ist es also typischerweise Daten aufzubereiten. Damit sind nicht nur die Eliminierung von Fehlern und das Angleichen von Formaten gemeint, sondern es geht insbesondere oft auch darum, eine Datenstruktur zu finden,die für eine Visualisierung geeignet ist. Natürlich sind BI-Tools in der Lage, diese Bereinigungen und Formatveränderungen vorzunehmen. In der Praxis stellt sich jedoch oft heraus, dass diese “no Code”-Lösungen teilweise nichts praktikabel bzw nicht komfortabel sind. Es ist deshalb nicht verwunderlich, dass gängige BI-Tools eine Schnittstelle zu Programmiersprachen bereitstellen, die insbesondere für Datenanalyse und Datenaufbereitung geeignet sind (also z.B Python oder R).\nInsofern sehen wir den Einsatz der Programmierung nicht als Alternative zu Nutzung von BI-Tools oder Excel, sondern vielmehr als sehr sinnvolle Ergänzung. Wir möchten Sie damit befähigen ein weiteres und aus unserer Sicht besser geeignetes Werkzeug zu nutzen, um Daten aufzubereiten und zu analysieren. Sie können selber entscheiden, ob Sie diese Werkzeug als sinnvolle Ergänzung zu Excel und BI-Tools nutzen.\n\n\nAutomatisierung von repetitiven Aufgaben\nDie Diskrepanz zwischen technischen Möglichkeiten und unternehmerischer Realität ist oft groß. Schaut man sich Arbeitsabläufe in Unternehmen an, so stellt man fest, dass diese - trotz anderer technischer Möglichkeiten - oft durch manuelle Schnittstellenwechsel unterbrochen werden. Dateien werden manuell heruntergeladen und geöffnet und dann mit anderen Dateien zusammengeführt, bevor das Ergebnis dann per Email weitergeschickt wird, um dann wieder manuell geöffnet und mit anderen Daten verknüpft zu werden. Für diese Brüche gibt es natürlich “große” Lösungen. Diese bedingen jedoch oft den Erwerb einer neuen Standardsoftware sowie die Veränderung von unternehmensinternen Abläufen. Auch wenn diese “große” Lösung in der Theorie oft die wirtschaftlichste ist, dauert es - wenn überhaupt - Jahre bis zur Einführung, weil Investition gescheut und ein “weiter so” scheinbar einfacher ist. Die Alternative zur großen Lösung muss jedoch nicht das “weiter so” sein. Bereits geringe Programmierkenntnisse ermöglichen die (zumindest teilweise) Automatisierung der oben beschriebenen Brüche. Programmierkenntnisse helfen dabei, Potentiale für “kleine Lösungen” zu identifizieren und sogar selber zu implementieren. Letzteres ist aber gar nicht unbedingt von Nöten, da bereits das Erkennen von Automatisierungspotentialen eine Diskussion mit IT-Experten anstoßen kann, die sonst nicht geführt worden wäre. Die Identifikation von Verbesserungspotentialen in Arbeitsabläufen benötigt Experten, die diese Abläufe und Prozesse kennen (d.h. Fachexperten wie Sie) und wird typischerweise nicht von außen angestoßen.\n\n\nProblemlösungsfähigkeit verbessern\nSchließlich hilft die Fähigkeit zu programmieren, komplexe Probleme besser zu verstehen und zu lösen. Programmieren ist ein Prozess zum Erstellen von Algorithmen, bei denen es sich um eine Reihe von Anweisungen handelt, die ein Computer befolgen muss. In der Wirtschaft sind Studierende oft an komplexen Entscheidungen beteiligt, was die Fähigkeit erfordert, komplexe Probleme zu verstehen und zu lösen. Programmieren kann dabei helfen, indem es den Studierenden beibringt, wie man ein Problem in kleinere Stücke zerlegt und dann einen Schritt-für-Schritt-Plan erstellt, um es zu lösen."
  },
  {
    "objectID": "Chapters/01_Chapter/Coding.html#programmieren-software-entwicklung-vs.-coding",
    "href": "Chapters/01_Chapter/Coding.html#programmieren-software-entwicklung-vs.-coding",
    "title": "Werkzeug: Coding!",
    "section": "Programmieren: Software-Entwicklung vs. Coding",
    "text": "Programmieren: Software-Entwicklung vs. Coding\nEs besteht kein Zweifel daran, dass die Fähigkeit zu Programmieren wertvoll ist. Jedoch wollen wir an dieser Stelle kurz darauf eingehen, was mit “Programmieren” eigentlich konkret gemeint ist.\nKeines der drei genannten Argumente bedingt, dass Sie zu Softwareentwicklern werden. Softwareentwicklung ist ein hochspezialisiertes Gebiet, das eine jahrelange Ausbildung (ggf. auch ein Informatikstudium) und Erfahrung erfordert. Programmieren hingegen ist eine relativ einfache Fähigkeit, die innerhalb weniger Wochen erlernt werden kann.33 Anmerkung: jedoch bedarf es - wie bei jeder anderen Sache - auch viel Übung, um gut darin zu werden. \nHier besteht ein weit verbreitetes Missverständnis, bei dem viele die Fähigkeit zu Programmieren mit der spezialisierten Funktion oder Aufgabe verwechseln, die normalerweise von Personen mit Informatikhintergrund übernommen werden.\nEine Analogie: jeder von uns lernt in der Schule zu schreiben. Dennoch können (oder wollen) nur wenige von uns Romanautoren werden. Um Romanauthr zu werden, müssen wir schreiben können. Jedoch erwerben wir die Fähigkeit zu schreiben nicht, weil wir Romanautoren werden wollen oder sollen. Vielmehr handelt es sich beim Schreiben um eine Grundkompetenz, die wir für viele Arten der Kommunikation erwerben sollten.\nWir argumentieren also, dass Ihnen eine Teilmenge einer breiten Palette von Programmierkenntnissen beigebracht werden sollte. Dabei meinen wir die Grundkompetenz - in unserem Falle - für den Anwendungszweck der Datenanalyse. Um diese Verwechslungsgefahr zu vermeiden, sprechen wir von nun an von Coding, wenn wir von der Untergruppe von Programmierkenntnissen sprechen, die Sie erlernen werden."
  },
  {
    "objectID": "Chapters/01_Chapter/Coding.html#coden-in-zeiten-von-ki-tools",
    "href": "Chapters/01_Chapter/Coding.html#coden-in-zeiten-von-ki-tools",
    "title": "Werkzeug: Coding!",
    "section": "Coden in Zeiten von KI-Tools?",
    "text": "Coden in Zeiten von KI-Tools?\nNun mögen einige von Ihnen anmerken, dass es in Zeiten von immer besser werdenden KI-Tools (z.B. ChatGPT) keinen Bedarf mehr für die Fähigkeit zu Programmieren gibt. Dies mag in (vielleicht nicht mehr allzu) ferner Zukunft der Fall sein. Aktuell argumentieren wir jedoch, dass es gerade in Zeiten von KI-Tools sinnvoll ist, sich die Grundkompetenz Coden anzueignen. Es ist richtig, dass Tools wie ChatGPT in der Lage sind, Code auf Grundlage von natürlichsprachlichen Anweisungen zu schreiben.\n\n\n\nChatGPT\n\n\nAllerdings gibt es dennoch gute Gründe, warum es sinnvoll ist, Coden zu lernen, selbst wenn solche Tools zur Verfügung stehen.\nEs ist gerade erst für jemanden, der in der Lage ist, Code zu schreiben und zu verstehen, besonders sinnvoll, ChatGPT einzusetzen. Dies liegt daran, dass ChatGPT, wie jedes andere künstliche Intelligenz-System auch, Fehler machen kann (und auch tatsächlich macht). Wenn man jedoch in der Lage ist, Code zu lesen und zu verstehen, ist es einfacher, diese Fehler zu identifizieren und zu korrigieren und die (teilweise falschen) Lösungsvorschläge produktiv zu nutzen. ChatGPT kann außerdem dabei helfen, komplexe Programme schneller zu schreiben, indem es mögliche Lösungen vorschlägt oder Ihnen bei “logischen Blockaden” Lösungswege aufzeigt - selbt wenn diese teilweise falsch sein sollten. Auch dies funktioniert jedoch nur, wenn man bereits in der Lage ist, Code zu schreiben und seine komplexen Probleme gut in strukturierte Codeblöcke zu übersetzen, um von dieser Funktionalität profitieren zu können. Kurz gesagt, ChatGPT (und viele weitere KI-Tools) kann eine (sehr!) nützliche Unterstützung darstellen, wenn man bereits über Kenntnisse im Programmieren verfügt, während Personen ohne diese Kenntnisse wenig Nutzen aus dem Tool ziehen können.\nInsofern möchten wir Sie auch ermutigen und anregen, sich mit diesen KI-Tools zu beschäftigen und diese auch im Rahmen dieses Moduls für sich zu nutzen. Wir glauben, dass die Aussagen von McAfee und Brynjolfsson (2017) nun tatsächlich eintreffen: KI zerstört nicht notwendigerweise Ihren Job, sondern es stellt für diejenigen ein Risiko dar, die KI für sich nicht sinnvoll einsetzen.\nNutzen Sie die Ihnen zur Verfügung stehenden Mittel, um Ihre Fähigkeiten zu erweitern und zu verbessern. Seien Sie sich nur bewusst, dass KI Sie “nur” unterstützt, Ihnen aber nicht die Fähigkeit zum Programmieren ersetzen wird. Zumindest noch nicht…\n\n\n\n\nMcAfee, Andrew, und Erik Brynjolfsson. 2017. Machine, platform, crowd: harnessing our digital future. First edition. New York: W.W. Norton & Company."
  },
  {
    "objectID": "Chapters/01_Chapter/Warum_Python.html#python-ist-populär",
    "href": "Chapters/01_Chapter/Warum_Python.html#python-ist-populär",
    "title": "Warum Python?",
    "section": "Python ist populär",
    "text": "Python ist populär\nEin Entscheidungskriterium ist die Popularität. Python hat seit seiner Erfindung in den 90er Jahren enorm an Beliebtheit gewonnen und wird aktuell von sehr vielen Softwareentwicklern und Datenanalysten genutzt und ist im Ranking der beliebtesten Programmiersprachen2 mittlerweile auf Platz 1, d.h. auch vor Java oder C.2 Methodisch ist nicht eindeutig zu bestimmen, welche Programmiersprache die beliebteste ist. Insofern gibt es verschiedene Rankings. Hier wurde ein bekanntes Ranking ausgewählt. Es gibt jedoch auch andere Rankings. In diesen ist Python nicht immer auf Platz 1, aber in den meisten Fällen auf den ersten drei Plätzen zu finden.\n\n\n\n\n\n\nPopularität verschiedener Programmiersprachen\n\n\n\nEin Ranking der beliebtesten Programmiersprachen ist z.B. das Tiobe-Ranking. Dieses Ranking wird monatlich aktualisiert und basiert auf der Anzahl der Suchanfragen in Suchmaschinen.\n\n\nEine Programmiersprache kann gut oder schlecht sein; ganz unabhängig von ihrer Popularität. Dennoch ist die Popularität aus unserer Sicht ein sehr wichtiges Kriterium.\nZum einen ist es wesentlich einfacher Unterstützung zu finden, wenn man ein bestimmtes Problem in Python nicht lösen kann. Im Rahmen dieses Kurses werden Sie auf Programmierprobleme stoßen:\n\nIhr Code produziert Fehlermeldungen,\nSie wissen nicht mehr, wie die geeignete Funktion zum Visualisieren von Balkencharts ist oder\nIhnen fehlt der Ansatz, wie Sie ein Problem konkret angehen sollen.\n\nSo gibt es unzählige Lehrbücher und -tutorials, die einem beim Erlernen der Programmiersprachen helfen. Aus unserer Sicht jedoch noch wichtiger: es gibt eine sehr große Online-Community (z.B. Stackoverflow), die bei Fragen unterstützen kann und KI-Tools (z.B. ChatGPT) bieten deutlich bessere Unterstützung für populäre Programmiersprachen, da diese im Gegensatz zu weniger populären Sprachen deutlich mehr Daten zur Verfügung haben.\nZum anderen sorgt die Popularität von Python für eine praktische Relevanz in Unternehmen. Auch wenn Sie in Ihrem zukünftigen Job vielleicht nicht programmieren werden (müssen), ist es dennoch hilfreich zu wissen, dass Python mit hoher Wahrscheinlichkeit in Ihrem zukünftigen Unternehmen bereits genutzt wird. Selbst wenn dies nicht der Fall ist, lässt sich die IT-Abteilung leicht(er) überzeugen, eine bekannte Programmiersprache zu installieren, als eine unbekannte."
  },
  {
    "objectID": "Chapters/01_Chapter/Warum_Python.html#python-ist-einfacher",
    "href": "Chapters/01_Chapter/Warum_Python.html#python-ist-einfacher",
    "title": "Warum Python?",
    "section": "Python ist einfach(er)",
    "text": "Python ist einfach(er)\nDie grundlegenden Konzepte einer Programmiersprache sind oft sehr ähnlich und Programmieren lernen erfordert - wie jede andere Tätigkeit - Zeit und Erfahrung. Dennoch unterscheiden sich Programmiersprachen oft in Ihrer Komplexität für Anfänger. So gibt es Programmiersprachen, die darfür bekannt sind, sehr schwer erlernbar zu sein (z.B. Rust). Andere Programmiersprachen gelten als eher einfach zu erlernen. Python gilt als eine eher einfach zu erlernende Programmiersprache. Dies bedeutet nicht, dass Python einfach per se ist. Vielmehr ist damit oft die Syntax der Sprache gemeint, d.h. welche Begriffe und Worte fehlerfreie Programme liefen. Darüber hinaus sind andere Dinge - wie z.B. die Art der Typisierung - wichtig.\nBeispiel: stellen Sie sich vor, wir möchten das arithmetische Mittel einer Zahlenreihe berechnen (engl. mean).\nSchauen wir uns an, wie wir dies in Rust und Python umsetzen können.3 Klicken Sie auf die jeweiligen Tabs, um die Beispiele zu sehen.3 Beide Beispiele sind hier entnommen und im Falle für Python leicht adaptiert.\n\nRustPython\n\n\nfn sum(arr: &[f64]) -> f64 {\n    arr.iter().fold(0.0, |p,&q| p + q)\n}\n\nfn mean(arr: &[f64]) -> f64 {\n    sum(arr) / arr.len() as f64\n}\n\nfn main() {\n    let v = &[2.0, 3.0, 5.0, 7.0, 13.0, 21.0, 33.0, 54.0];\n    println!(\"mean of {:?}: {:?}\", v, mean(v));\n\n    let w = &[];\n    println!(\"mean of {:?}: {:?}\", w, mean(w));\n}\n\n\nfrom statistics import mean\nm = mean([2.0, 3.0, 5.0, 7.0, 13.0, 21.0, 33.0, 54.0])\nprint(m)\n\n\n\nWelche Sprache finden Sie intuitiver oder einfacher zu lesen? Die meisten Leser*innen werden das Beispiel in Python wegen der intuitiven Syntax einfacher finden. Auch wenn komplexe Programme in Python ähnlich komplex werden (können), wie in jeder anderen Programmiersprache, hat diese einfache(re) Syntax einen großen Vorteil: Sie werden zu Beginn schneller Fortschritte machen und können bereits nach kurzer Zeit praxisrelevante Skripte schreiben."
  },
  {
    "objectID": "Chapters/01_Chapter/Warum_Python.html#python-ist-datenanalyse",
    "href": "Chapters/01_Chapter/Warum_Python.html#python-ist-datenanalyse",
    "title": "Warum Python?",
    "section": "Python ist Datenanalyse",
    "text": "Python ist Datenanalyse\nEin Grund für die große Popularität und insbesondere das starke Wachstum von Python ist, dass Python insbesondere auch im Bereich der Datenanalyse eingesetzt wird. So hat sich Python als eine der weit verbreitetsten Sprachen im Bereich Datenaufbereitung, Machinelles Lernen und Deep Learning entwickelt. Durch die einfache Syntax ist Python sehr gut geeignet, um schnell Prototypen für komplexe Datenanalysethemen zu erstellen. Dies hat dazu geführt, dass in den letzten Jahren eine Vielzahl von Modulen und Packages (d.h. Bibliotheken, die die Standardsprache erweitern) entstanden sind. Diese Add-ons, sind teilweise von großen Unternehmen, wie Google oder Facebook, (mit)entwickelt worden und sind frei verfügbar. Die Lösung von komplexen Datenanalysethemen ist insofern durch die Vielzahl an geeigneten Hilfsmitteln sehr viel einfacher und mächtiger geworden und erfordert (teilweise) keine jahrelange Programmiererfahrung. Zusatzmodule wie pandas, numpy, matplotlib oder scikit-learn - diese werden Sie in den folgenden Kapiteln noch kennenlernen - haben Python zu einer Art Schweizer Taschenmesser der Datenanalyse - insbesondere auch für Programmieranfänger - gemacht."
  },
  {
    "objectID": "Chapters/01_Chapter/Warum_Python.html#python-ist-allzweckwaffe",
    "href": "Chapters/01_Chapter/Warum_Python.html#python-ist-allzweckwaffe",
    "title": "Warum Python?",
    "section": "Python ist Allzweckwaffe",
    "text": "Python ist Allzweckwaffe\nAuch andere Sprachen sind für das Thema Datenanalyse sehr geeignet; insbesondere R ist in den Bereichen Aufbereitung von Daten und statistischer Analyse führend. Jedoch hat Python einen Vorteil gegenüber einer Sprache wie R: Python ist eine sogenannte “all purpose programming language”, d.h. Python kann auch andere Dinge sehr gut. So wird Python auch zur Entwicklung von Software und Webapps oder Automatisierung von Prozessen genutzt. Der Einsatzzweck ist im Grunde nicht beschränkt. Im Rahmen dieses Kurses erscheint mir dies als Vorteil, da Sie als Einsteiger so in den Genuss einer Programmiersprache kommen, deren Einsatzzweck Sie für sich persönlich beliebig erweitern können. Das so vermittelte Programmierwissen kann so also zukünftig - sofern tatsächlich weitergehendes Interesse besteht - breit eingesetzt werden."
  },
  {
    "objectID": "Chapters/02_Chapter/Installation.html#grundlagen",
    "href": "Chapters/02_Chapter/Installation.html#grundlagen",
    "title": "Installation & Setup",
    "section": "Grundlagen",
    "text": "Grundlagen\nBevor wir uns inhaltlich in die Welt der Datenanalyse begeben, müssen wir uns zunächst mit der technischen Umgebung beschäftigen, in der wir unsere Analysen durchführen werden. Wir müssen uns also zunächst mit der Frage beschäftigen, wie wir Python nutzen können. Bis vor wenigen Jahren, war dies eine recht komplizierte Angelegenheit. Heute ist es jedoch sehr einfach, Python zu nutzen.\nPrinzipiell benötigen wir zwei Dinge, um Python nutzen zu können:\n\nEin Programm, in dem wir unseren Python-Code schreiben\nEin Programm, mit dem wir unseren Python-Code ausführen können\n\n\nProgramme, um Python-Code zu schreiben\nPython kann in verschiedenen Arten von Programmen geschrieben werden, jedes mit seinen eigenen Vor- und Nachteilen.\nEinfache Texteditoren: prinzipiell können Sie in jedem einfachen Texteditor Python (oder auch anderen) Code schreiben. Beispiele für einfache Texteditoren sind z.B. Notepad++. Auch wenn es möglich ist, Python-Programme in einem solchen Texteditor zu schreiben, ist dies nicht besonders effizient oder komfortabel, da einfache Texteditoren keine Syntaxhervorhebung oder Code-Formatierung bieten bzw. andere hilfreiche Funktionen, die für das Schreiben von Python-Code nützlich sind.\nSpezielle Texteditoren: Spezielle Texteditoren sind in der Regel leistungsfähiger als einfache Texteditoren und bieten zusätzliche Funktionen, die speziell für das Schreiben von Programm-Code benötigt werden (z.B. Syntaxhervorhebung, Debugging, Profiling und Integration mit Versionierungssystemen). Beispiele für spezielle Texteditoren sind Visual Studio Code oder Sublime.\nIntegrierte Entwicklungsumgebungen (IDE): Eine integrierte Entwicklungsumgebung ist ein umfassendes Werkzeug, das Code-Schreiben, Debugging, Profiling, Testing und viele andere Funktionen unterstützt. Sie bieten in der Regel eine visuelle Umgebung, in der man Code schreiben, Fehler finden und vieles mehr tun kann. IDEs sind in der Regel für fortgeschrittene Entwickler geeignet, die an komplexen Projekten mit vielen Dateien arbeiten. Beispiele für IDEs für Python sind PyCharm und IDLE.\nNotebooks: Jupyter Notebook siehe hier und Google Colab sind Programme, die es ermöglichen, Code, Text, Bilder und visuelle Elemente in einem einzigen Dokument zu kombinieren. Sie sind besonders nützlich für Datenanalyse-Projekte und für die Dokumentation von Code und Analyseergebnissen. Jupyter Notebooks können lokal auf dem eigenen Computer oder in der Cloud ausgeführt werden, während Google Colab in der Cloud ausgeführt wird.\nIm Rahmen dieses Kurses werden wir Notebooks nutzen (wir überlassen es dabei Ihnen, ob Sie Google Colab oder Jupyter Notebooks nutzen möchten).\n\n\nProgramme, um Python-Code auszuführen\nMan unterscheidet zwischen Programmiersprachen, die kompiliert werden müssen und solchen, die interpretiert werden.\nKompilierte Sprachen werden vor der Ausführung in Maschinencode übersetzt, der direkt von der Hardware des Computers ausgeführt werden kann. Dies bedeutet, dass kompilierter Code normalerweise schneller ausgeführt wird, aber auch, dass er nicht direkt von Menschen gelesen werden kann. Beispiele für Sprachen, die kompiliert werden müssen, bevor sie ausgeführt werden, sind C, C++ und Go.\nInterpretierte Sprachen werden während der Ausführung von einem speziellen Programm namens Interpreter “gelesen” und in Maschinencode übersetzt. Dies bedeutet, dass interpretierter Code langsamer ausgeführt wird als kompilierter Code, aber dass er von Menschen leichter gelesen und verstanden werden kann. Beispiele für interpretierte Sprachen sind Python, JavaScript und Ruby.\nUm Python Code auszuführen, benötigt man demnach Software, die den geschriebenen Text (siehe oben) für den Computer übersetzt, damit dieser den Code ausführt. Wir benötigen also eine Software, die unseren Text “interpretiert”. Dies mag sich komplex anhöhren (und die technische Umsetzung ist es in der Tat auch), jedoch müssen wir die technischen Details nicht kennen, um Python nutzen zu können. Wir müssen nur wissen, dass wir einen Interpreter benötigen, um Python-Code auszuführen. Diese wird standardmäßig mit Python installiert. Wir müssen also nur die richtige Software installieren, um sicherzustellen, dass unser geschriebener Python-Code auch ausgeführt werden kann."
  },
  {
    "objectID": "Chapters/02_Chapter/Installation.html#installation-von-schreib--und-ausführungssoftware",
    "href": "Chapters/02_Chapter/Installation.html#installation-von-schreib--und-ausführungssoftware",
    "title": "Installation & Setup",
    "section": "Installation von Schreib- und Ausführungssoftware",
    "text": "Installation von Schreib- und Ausführungssoftware\nIm Rahmen dieses Kurses werden wir Notebooks nutzen. Diese haben sich als sehr nützlich für die Zwecke der Datenanalyse herausgestellt und erfreuen sich in der Datenanalyse-Community großer Beliebtheit.\nWir überlassen es Ihnen, ob Sie Google Colab oder Jupyter Notebooks nutzen möchten, empfehlen jedoch die lokale Installation und werden auch im Rahmen der Veranstaltungen selber auf Jupyter Notebooks zurückgreifen. Sollten Sie jedoch Probleme bei der Installation haben oder prinzipiell eine Cloud-Lösung bevorzugen, können Sie auch Google Colab nutzen.\nEgal für welche Variante Sie sich entscheiden: beide Tools sind kostenfrei verfügbar und können genutzt werden, um Python-Code zu schreiben und auszuführen. Sie müssen sich über die technischen Details der Ausführung von Python-Code für die Zwecke dieser Veranstaltung also keine weiteren Gedanken mehr machen.\n\nVariante 1: Lokale Installation\nAlternativ können Sie Python auch lokal auf Ihrem Computer installieren. Hierfür gibt es verschiedene Varianten. Wir empfehlen Anaconda für die Installation von Python und viele der relevanten Bibliotheken zu nutzen.\nGehen Sie dafür auf die Webseite und laden sich Anaconda herunter und installieren das Programm. Das Programm ist für Windows und Mac jeweils kostenlos verfügbar.\n\n\n\nDownload Anaconda\n\n\nSobald dies geschehen ist, können Sie den sogenannten Anaconda Navigator öffnen und Jupyter Notebooks oder Jupyter Lab nutzen.\n\n\n\nAnaconda Navigator\n\n\n\n\nVariante 2: Google Colab\nEs ist keine Voraussetzung, dass Sie Python und Jupyter Notebooks lokal auf ihrem Computer installieren. Sie können stattdessen die von Google angebotene Cloud-Variante von Jupyter Notebooks nutzen: google colab. Das ganze ist kostenfrei verfügbar und Sie benötigen dafür lediglich einen Google-Account."
  },
  {
    "objectID": "Chapters/02_Chapter/Introduction_notebook.html#zellentypen",
    "href": "Chapters/02_Chapter/Introduction_notebook.html#zellentypen",
    "title": "Einführung Notebooks",
    "section": "Zellentypen",
    "text": "Zellentypen\nEin Notebook besteht aus einer Folge von Zellen. Jede Zelle kann einen von zwei Typen3 haben:3 Hinweis: der dritten Zelltyp _raw__ ist für unsere Zwecke nicht weiter relevant\n\nCode-Zelle\nMarkdown-Zelle\n\n\nCode-Zelle\nIn einer Code-Zelle können Sie Python-Code schreiben und ausführen. Die folgende Zelle ist eine Code-Zelle, die Python Code enthält (der Code ist zugegebenermaßen nicht sonderlich komplex).\n\n3 + 4\n\n7\n\n\nDas Ergebnis des ausgeführten Python-Code (in unserem Falle 7) wird erst dann berechnet, wenn Sie die Zelle ausführen. Dies tun Sie, in dem Sie die Zelle markieren und dann STRG + ENTER drücken.\nZellen werden zu Code-Zellen, wenn Sie den Typ der Zelle auf Code setzen. Dies können Sie mit dem Shortcut ESC + Y tun.\n\n\nMarkdown-Zelle\nIn einer Markdown-Zelle können Sie Text schreiben, der ihren Code erläutert. Markdown ist eine einfache Textformatierungssprache, die es Ihnen erlaubt, Text zu formatieren. Sie können zum Beispiel Text kursiv setzen, Fettschrift verwenden, Formeln erstellen, Listen erstellen, Links setzen, Bilder einfügen, Tabellen erstellen, etc.\n\n\n\n\n\n\n\n\nHinweis\n\n\n\nDas Skript, welches Sie gerade lesen, ist ein Notebook und besteht aus einer Folge von Markdown- und Code-Zellen.\n\n\nHier ein Beispiel für den Inhalt eine Markdown-Zelle:\n### Eine Überschrift der Ebene 3\n\nDies ist ein Absatz, der __fette__ und _kursive_ Schrift enthält. Außerdem ein [Link](https://www.google.com) und ein Bild:\n\n![Bild](https://upload.wikimedia.org/wikipedia/commons/thumb/3/38/Jupyter_logo.svg/100px-Jupyter_logo.svg.png)\n\nEine Tabelle:\n\n| Spalte 1 | Spalte 2 |\n|----------|----------|\n| 1        | 2        |\n| 3        | 4        |\n\nEine Formel:\n\n$$\n\\int_0^1 x^2 \\, dx = \\frac{1}{3}\n$$\n\nEine Liste:\n\n- Punkt 1\n- Punkt 2\n\nNach dem Ausführen der Zelle sehen Sie den Text, der in der Zelle steht. Dieser sieht dann wie folgt aus:\n\n\nEine Überschrift der Ebene 3\nDies ist ein Absatz, der fette und kursive Schrift enthält. Außerdem ein Link und ein Bild:\n\n\n\nBild\n\n\nEine Tabelle:\n\n\n\nSpalte 1\nSpalte 2\n\n\n\n\n1\n2\n\n\n3\n4\n\n\n\nEine Formel:\n\\[\n\\int_0^1 x^2 \\, dx = \\frac{1}{3}\n\\]\nEine Liste:\n\nPunkt 1\nPunkt 2\n\nZellen werden zu Markdown-Zellen, wenn Sie den Typ der Zelle auf Markdown setzen. Dies können Sie mit dem Shortcut ESC + M tun."
  },
  {
    "objectID": "Chapters/02_Chapter/Introduction_notebook.html#arbeitsablauf",
    "href": "Chapters/02_Chapter/Introduction_notebook.html#arbeitsablauf",
    "title": "Einführung Notebooks",
    "section": "Arbeitsablauf",
    "text": "Arbeitsablauf\nSie schreiben Programme in Jupyter Notebooks, indem Sie eine Folge von Zellen erstellen. Jede Zelle enthält entweder Python-Code oder Text, der Ihren Code erläutert. Die Summe aller Zellen ist Ihr Notebook und sieht dann z.B. so aus:\n\n\n\nBeispiel: Notebook"
  },
  {
    "objectID": "Chapters/02_Chapter/Introduction_notebook.html#shortcuts",
    "href": "Chapters/02_Chapter/Introduction_notebook.html#shortcuts",
    "title": "Einführung Notebooks",
    "section": "Shortcuts",
    "text": "Shortcuts\nUm die Arbeit mit Notebooks zu erleichtern, gibt es eine Reihe von Shortcuts, die Sie verwenden können. Die wichtigsten sind:\n\nESC + Y: Zelle zu Code-Zelle machen\nESC + M: Zelle zu Markdown-Zelle machen\nESC + A: Zelle oberhalb der aktuellen Zelle einfügen\nESC + B: Zelle unterhalb der aktuellen Zelle einfügen\nESC + D + D: Zelle löschen\nSHIFT + ENTER: Zelle ausführen und Cursor i nächste Zelle springen\nCTRL + ENTER: Zelle ausführen und Cursor in aktuelle Zelle bleiben"
  },
  {
    "objectID": "Chapters/02_Chapter/Introduction_notebook.html#nützliche-hilfen",
    "href": "Chapters/02_Chapter/Introduction_notebook.html#nützliche-hilfen",
    "title": "Einführung Notebooks",
    "section": "Nützliche Hilfen",
    "text": "Nützliche Hilfen\n\nTAB-Taste: Wenn Sie in einer Code-Zelle Python-Code schreiben, können Sie die TAB-Taste drücken, um eine Liste von möglichen Vervollständigungen anzuzeigen.\nSHIFT + TAB: Wenn Sie in einer Code-Zelle Python-Code schreiben, können Sie die SHIFT + TAB-Tastenkombination drücken, um die Dokumentation für die Funktion anzuzeigen, die Sie gerade schreiben."
  },
  {
    "objectID": "Chapters/02_Chapter/Python_Basics.html#einfache-arithmetik",
    "href": "Chapters/02_Chapter/Python_Basics.html#einfache-arithmetik",
    "title": "Kurze Einführung in Python",
    "section": "Einfache Arithmetik",
    "text": "Einfache Arithmetik\nBeginnen wir mit einfachen mathematischen Operationen. Das Beispiel zeigt einen Ausdruck bzw. Statement (im Folgenden verwenden wir den Begriff Statement) mit Berechnungen. Dieser Ausdruck ist in Python geschrieben:\n\n1 + (5 * 12) / 3 - 12\n\n9.0\n\n\nDas Ergebnis - wenn man die Zelle mit STRG + ENTER ausführt - wird unterhalb der Zelle angezeigt und beträgt 9.\nWie man sieht, kann man - analog zu einem Taschenrechner (oder Excel) - die bekannten arithmetischen Operatoren in Python verwenden.\n+ = Addition\n- = Subtraktion\n* = Multiplikation\n/ = Division\n** = Potenzieren (❗ die Schreibweise für \\(3^4\\) in Python lautet \\(3{**}4\\))\nWir werden diese Operatoren im Laufe dieses Kurses immer wieder verwenden. Jedoch nicht wie im obigen Beispiel dargestellt. Dieses Vorgehen ist nämlich meist nicht sinnvoll und wenig vorteilhaft gegenüber z.B. einem Taschenrechner. Auch in einem typischen Tabellenkalkulationsprogramm wie Excel würden wir anders vorgehen, da wird dort typischerweise nicht eine komplette Formel mit fest codierten Werten in eine Zelle schreiben. Dies ist umständlich, wenig flexibel und fehleranfällig. Gleiches gilt für Python.\n\n\n\nBeispiel: Excel\n\n\nIn Excel würden wir stattdessen die Werte auf verschiedene Zellen aufteilen und in einer Ergebniszelle verknüpfen. Dieses Vorgehen hat den Vorteil, dass die Annahmen für das Ergebnis viel transparenter und Änderungen in Annahmen auf einen Blick ersichtlich sind. So wird z.B. deutlich, dass es sich bei der \\(12\\), die in der Berechnung zwei Mal vorkommt (Zellen C5 und C7) um tatsächlich zwei verschiedene Annahmen handelt - sonst würden wir nicht zwei Inputparameter benötigen -, die nur zufällig den selben Wert haben.\n\n\n\nBeispiel: Excel (besser)\n\n\nIn Python können wir das gleiche Vorgehen verwenden. Wir können die Werte in Variablen speichern und dann die Berechnung in einer weiteren Variablen durchführen."
  },
  {
    "objectID": "Chapters/02_Chapter/Python_Basics.html#variablen-und-zuweisung",
    "href": "Chapters/02_Chapter/Python_Basics.html#variablen-und-zuweisung",
    "title": "Kurze Einführung in Python",
    "section": "Variablen und Zuweisung",
    "text": "Variablen und Zuweisung\nIn Python kann man Variablen wie Platzhalter für Werte verwenden. Man kann einer Variablen einen Wert zuweisen, indem man den Variablennamen schreibt, gefolgt von dem Zuweisungsoperator = und dem Wert, den man der Variablen zuweisen möchte. Wir schreiben also\n<variable> = <wert>\nZum Beispiel:\n\nzahl1 = 42\nzahl2 = 3.2\nsatz = \"Deutscher Meister wird nur der BVB\"\n\nNamen für Variablen sollten sinnvoll und prägnant (d.h. nicht zu lang) sein. In Python gibt es darüber hinaus die Konvention (siehe PEP 8), dass Variablen klein geschrieben werden und mehrere Wörter mit “_” getrennt werden.\nBeispiel:\n\nliste_zahlen = [1, 2, 3, 4, 5]\nanzahl_elemente = 5\n\nDiese Konvention ist für die Funktion eines Programms nicht entscheiden, jedoch erleichtert diese die Lesbarkeit des Codes.\nWir können nun das Konzept der Variablenzuweisung verwenden, um Berechnungen durchzuführen.\n\ncash_flow = 100\nzinssatz = 0.05\nkapitalwert = cash_flow / (1 + zinssatz)\nkapitalwert\n\n95.23809523809524\n\n\nNatürlich benötigen wir diese Art der Berechnungen keine Programmierkenntnisse, sondern könnten diese am Taschenrechner oder in Excel durchführen (in diesem Falle vermutlich die bessere Idee). Jedoch können wir das Grundkonzept der Variablenzuweisung auch für wesentlich komplexere Probleme oder Fragestellungen anwenden, bei denen ein Taschenrechner oder auch Excel nicht mehr geeignet wären. Was meinen wir mit komplex? Zum einen ist es Python egal, was wir in einer Variablen speichern. Wir können also auch ganz andere Dinge in Variablen speichern und dann mit diesen weiterarbeiten. So können wir z.B. große Datenmengen (z.B. ein Excelsheet oder eine Datenbank) einer Variabel zuweisen und dann mit dieser weiterarbieten (siehe unten). Zum anderen werden wir auch weitere Grundkonzepte kennenlernen, die es uns ermöglichen, mit Variablen zu arbeiten (z.B. könnnen wir Bedingungen anlegen, die abhängig von den Werten in Variablen sind).\nIn der untenstehenden Zelle wird eine Exceltabelle in eine Variable tabelle gespeichert. Wir können mit dieser Variable dann weiterarbeiten und z.B. die Daten in der Tabelle auswerten, neue Daten hinzufügen oder die Tabelle in eine andere Datei exportieren.\n\nimport pandas as pd \n\ndatei_pfad = \"../../_data/sales.xlsx\"\ntabelle = pd.read_excel(datei_pfad)\ntabelle\n\n\n\n\n\n  \n    \n      \n      Produkt\n      Preis\n      Menge\n      Datum\n    \n  \n  \n    \n      0\n      Apple\n      0.79\n      10\n      2022-01-01\n    \n    \n      1\n      Orange\n      1.99\n      1\n      2022-01-02\n    \n    \n      2\n      Orange\n      1.99\n      5\n      2022-01-03\n    \n    \n      3\n      Banana\n      0.79\n      1\n      2022-01-04\n    \n    \n      4\n      Apple\n      0.79\n      7\n      2022-01-05\n    \n    \n      5\n      Orange\n      1.99\n      2\n      2022-01-06\n    \n    \n      6\n      Banana\n      0.79\n      9\n      2022-01-07\n    \n    \n      7\n      Apple\n      1.99\n      4\n      2022-01-08\n    \n    \n      8\n      Apple\n      2.49\n      8\n      2022-01-09\n    \n    \n      9\n      Orange\n      2.49\n      7\n      2022-01-010"
  },
  {
    "objectID": "Chapters/02_Chapter/Python_Basics.html#datentypen",
    "href": "Chapters/02_Chapter/Python_Basics.html#datentypen",
    "title": "Kurze Einführung in Python",
    "section": "Datentypen",
    "text": "Datentypen\nUnser vorheriges Beispiel zeigt, dass Variablen ganz unterschiedliche Inhalte haben können. Wir haben Zahlen, einen Text und eine Tabelle in Variablen gespeichert. Wir sprechen in diesem Fall davon, dass eine Variable einen bestimmten Datentyp hat. In der untenstehenden Tabelle (Tabelle 6.1) sind die Datentypen unserer Variablen aufgelistet.\n\n\nTabelle 6.1: Datentypen der Beispielvariablen\n\n\nDatentyp\nBeispiel\nBeschreibung\n\n\n\n\nint\n42\nGanze Zahl\n\n\nfloat\n3.2\nGleitkommazahl\n\n\nstr\n\"Deutscher Meister wird nur der BVB\"\nText\n\n\nDataFrame1\ntabelle\nTabelle\n\n\n\n1 Konkret: pd.DataFrame\nWir können den Typ einer Variablen mit dem Befehl type() herausfinden.\nWir können also z.B. den Typ der Variablen zahl1 mit dem Befehl type(zahl1) herausfinden.\n\ntype(zahl1)\n\nint\n\n\nDen Typ der Daten können wir meist nicht am Namen der Variablen erkennen. Außerdem ist es so, dass wir den Datentyp einer Variabel verändern können, in dem wir ihr einen neuen Wert zuweisen.\n\nzahl1 = 0.12\ntype(zahl1)\n\nfloat\n\n\nIm obigen Beispiel haben wir der Variablen zahl1 einen neuen Wert zugewiesen. Dieser Wert ist eine Gleitkommazahl. Dadurch hat sich der Datentyp der Variablen von int zu float geändert.\nDer Typ der Variable hängt also von den Werten ab, den wir ihr zuweisen.\nNun mögen wir uns fragen, warum wir uns überhaupt mit dem Datentyp einer Variablen beschäftigen müssen. Der Datentyp einer Variable ist wichtig, da dieser bestimmt, welche Fähigkeiten2 eine Variable hat.2 Wir verwende bewusst den untechnischen Begriff der Fähigkeit, um zu verdeutlichen, was die jeweilige Variable kann. Wir müssen an dieser Stelle keine technischen Begriffe verwenden.\nPrinzipiell ist es aus zwei Gründen wichtig, die Fähigkeiten einer Variable zu kennen:\n\nWir sollten die Fähigkeiten einer Variable kennen, um Fehler zu vermeiden.\nWir können die Fähigkeiten einer Variable nutzen, um unsere Aufgaben zu lösen.\n\nLassen Sie uns zwei Beispiele betrachten, die zeigen, wie wir die Fähigkeiten einer Variable nutzen können.\n\nBeispiel 1: Fehler vermeiden\nWir haben bereits kennengelernt, dass wir mit Variablen einfache Rechnungen durchführen können. Wir können z.B. zwei Variablen addieren, indem wir die beiden Variablen mit dem +-Zeichen verknüpfen.\n\nzahl1 = 2\nzahl2 = 3\nzahl1 + zahl2\n\n5\n\n\nDiese Rechnung funktioniert, da die beiden Variablen zahl1 und zahl2 beide Zahlen sind. Was passiert jedoch, wenn wir zwei Variablen addieren, die keine Zahlen, sondern z.B. Texte sind?\n\ntext1 = \"12\"\ntext2 = \"3\"\ntext1 + text2\n\n'123'\n\n\nZu unserer Überraschung erhalten wir als Ergebnis nicht die Zahl 15, sondern den Text 123. Dies ist problematisch, da wir bei der Berechnung eigentlich eines von zwei Ergebnissen erwarten würden: (i) eine Fehlermeldung, da wir zwei Texte addieren wollen, oder (ii) das korrekte Ergebnis 15. Das Ergebnis 123 ist jedoch nicht das, was wir erwarten würden.\nDas Beispiel ist natürlich sehr simpel und konstruiert. In der Praxis werden wir jedoch häufiger mit solchen Fehlern konfrontiert, bei denen wir Fähigkeiten auf Datentypen anwenden, die anders sind als die Fähigkeiten, die wir eigentlich anwenden wollen. Wir sollten uns also immer bewusst sein, welche Fähigkeiten eine Variable hat und diese Fähigkeiten nutzen, um Fehler zu vermeiden.\n\n\nBeispiel 2: Fähigkeiten nutzen\nLassen Sie uns das vorherige Beispiel aufgreifen. Wir haben gesehen, dass wir Variablen vom Datentype str nicht im mathematischen Sinne addieren können. Wieso hat das Programm dann aber keinen Fehler ausgeworfen. Der Grund hierfür ist, dass wir die Operation + nicht für mathematische Berechnungen nutzen sollten, sondern für das Zusammenfügen von Texten. Wenn wir dies wissen, können wir uns diese Fähigkeit zu Nutze machen.\n\nvorname = \"Max\"\nnachname = \"Mustermann\"\nanrede = \"Lieber Herr \" + vorname + \" \" + nachname\nanrede\n\n'Lieber Herr Max Mustermann'\n\n\nAuch dies ist natürlich nur ein sehr simples Beispiel, um zu verdeutlichen, dass jede Datentyp unterschiedliche Fähigkeiten hat und wir diese nutzen, um unsere Aufgabenstellung zu lösen.\nWie stelle ich nun aber fest, welche Fähigkeiten eine Variable hat? Hier für gibt es verschiedene Wege:\n\nwir können die help()-Funktion nutzen, um uns die Dokumentation zu einem Datentyp anzeigen zu lassen.\n\n\nhelp(str) # Hilfe für den Datentyp str\n\n\nwir können die dir()-Funktion nutzen, um uns die Methoden eines Datentyps anzeigen zu lassen.\n\n\ndir(str) # Fähigkeiten für den Datentyp str\n\n\nwir können über .TAB-Vervollständigung die Fähigkeiten einer Variable anzeigen lassen.\n\n\n\n\nBeispiel: TAB completion\n\n\n\nwir können in die Dokumentation von Python bzw. die der jeweiligen Bibliothek3 schauen.\n\n3 Was das genau ist, wir in Sektion [TODO] erläutert"
  },
  {
    "objectID": "Chapters/02_Chapter/Python_Basics.html#fehlermeldungen",
    "href": "Chapters/02_Chapter/Python_Basics.html#fehlermeldungen",
    "title": "Kurze Einführung in Python",
    "section": "Fehlermeldungen",
    "text": "Fehlermeldungen\nIm Zuge Ihrer Arbeit mit Python werden Sie immer wieder Fehlermeldungen erhalten. Diese Fehlermeldungen sind Fluch und Segen zugleich. Was sind Fehlermeldungen und warum sind diese so wichtig? Fehlermeldungen sind eine Art Feedback (in diesem Fall von Python), das uns mitteilt, dass etwas nicht funktioniert.\nGerade als Anfänger können Fehlermeldungen sehr frustrierend sein. Schließlich weisen diese uns daraufhin, dass der Code, den wir geschrieben haben, nicht funktioniert. Machen Sie jedoch nicht den Fehler zu glaubne, dass Fehlermeldungen nur von Anfängern produziert werden. Auch erfahrene Programmierer stolpern quasi täglich über Fehlermeldungen. Der Unterschied zwischen erfahrenen Programmierern und absoluten Anfängern ist jedoch, dass letztere die Fehlermeldungen meist verwirrend und unverständlich finden und dieses deshalb nicht für sich zu nutzen wissen.\nFehlermeldungen geben uns nämlich wertvolle Hinweise darauf, was genau nicht funktioniert. Wir sollten uns also immer die Zeit nehmen, Fehlermeldungen zu lesen und zu verstehen. Oft werden wir dananch feststellen, dass der Fehler nicht in der Logik unseres Codes liegt, sondern in der Syntax (d.h. wir haben z.B. einen Tipfehler gemacht) und wir diesen schnell beheben können.\nLassen Sie uns ein Beispiel betrachten, bei dem wir eine Fehlermeldung erhalten.\n\nvorname = \"Max\"\nnachname = \"Mustermann\"\nanrede = \"Lieber Herr \" vorname + \" \" + nachname\nanrede\n\n\n\n\nBeispiel: Fehlermeldung\n\n\nDie rote Fehlermeldung weißt zunächst darauf hin, dass unser Code so nicht funktioniert. Wir sollten also zunächst versuchen, den Fehler zu finden. Die Fehlermeldung hilft uns dabei. Wir sollten dabei immer zunächst auf die letzte Zeile der Fehlermeldung schauen. Dort wird uns mitgeteilt, um welche Art des Fehlers es sich handelt. In unserem Fall handelt es sich um einen SyntaxError, d.h. wir haben dem Computer einen Befehl geben, den er nicht versteht bzw. kennt. Dies passiert häufig, wenn wir einen Tippfehler gemacht haben. Sobald wir dies wissen, müssen wir uns nur noch den Code finden, in dem der Fehler gemacht wurde. Auch hier hilft uns die Fehlermeldung, da Sie zum einen die Zeile des Codes darstellt, in der der Fehler aufgetreten ist und zum anderen mit dem ^-Symbol auf die Stelle im Code hinweist, der falsch ist. In unserem Fall ist der Fehler demnach, dass wir die Zeichenkette \"Lieber Herr \" nicht mit einem + an die Variable vorname anhängen.\nwir können nun den Fehler beheben und den Code erneut ausführen.\n\nvorname = \"Max\"\nnachname = \"Mustermann\"\nanrede = \"Lieber Herr \" + vorname + \" \" + nachname\nanrede\n\n'Lieber Herr Max Mustermann'\n\n\nEs gibt viele verschiedene Fehlermeldungen - einigen von Ihnen werden Sie in diesem Kurs produzieren - die teilweise auch komplexer sind bzw. schwerer zu beheben sind. Wir können (und wollen) an dieser Stelle nicht auf alle verschiedenen Fehler eingehen. Uns geht es darum, dass Sie sich durch Fehlermeldungen nicht abschrecken lassen und diese als Hilfsmittel nutzen, um Ihren Code zu verbessern. In der untenstehenden Tabelle (Tabelle 6.2) finden Sie eine Auswahl an häufigen Fehlermeldungen.\n\n\nTabelle 6.2: Beispiele: Fehlermeldungen\n\n\n\n\n\n\n\nFehlermeldung\nBeschreibung der Art des Fehlers\nkurzes Beispiel\n\n\n\n\nSyntaxError: EOL while scanning string literal\nFehler bei der Syntax, in dem ein String nicht richtig abgeschlossen wurde.\n“Max Muster\n\n\nNameError: name ‘y’ is not defined\nFehler bei der Verwendung einer Variable, die nicht definiert wurde.\nx = y + 2\n\n\nTypeError: unsupported operand type(s) for +: ‘int’ and ‘str’\nFehler beim Verwenden eines Operators mit ungültigen Typen.\n2 + “Text”\n\n\nIndexError: list index out of range\nFehler beim Zugriff auf ein Element einer Liste, das nicht existiert.\nmeine_liste[5]\n\n\nKeyError: ‘key’\nFehler beim Zugriff auf einen Schlüssel in einem Dictionary, der nicht existiert.\nmein_dictionary[‘alter’]"
  },
  {
    "objectID": "Chapters/02_Chapter/Python_Basics.html#importieren-von-modulen-und-bibliotheken",
    "href": "Chapters/02_Chapter/Python_Basics.html#importieren-von-modulen-und-bibliotheken",
    "title": "Kurze Einführung in Python",
    "section": "Importieren von Modulen und Bibliotheken",
    "text": "Importieren von Modulen und Bibliotheken\n\nBuilt-in Fähigkeiten\nWir haben in diesem Kapitel von Fähigkeiten gesprochen, die wir nutzen, um unsere Analysen durchzuführen. Lassen Sie uns an dieser Stelle ein Geheimnis lüften: Python bzw. die zur Verfügung stehenden Datentypen haben in Summe kaum die Fähigkeiten, die wir für unsere Analysen benötigen.\nDies mag zunächst überraschen ist jedoch vor dem Hintergrund, dass Python ursprünglich nicht für die Analyse von Daten entwickelt wurde, zu verstehen. Eine Standardinstallation von Python enthält nur die Grundfähigkeiten, von denen wir bereits einige kennengelernt haben. Mit Fähigkeiten meinen wir an dieser Stelle\n\ndie Datentypen, die Python zur Verfügung stellt und\ndie Funktionen, die wir auf diesen Datentypen anwenden können und\n\nWir haben bereits einige Datentypen kennengelernt, die Python zur Verfügung stellt. Diese sind z.B. int, float, str, list und dict. Auch haben wir einige Funktionen kennengelernt, die wir auf diesen Datentypen anwenden können. Diese sind z.B. print(), dir() und help(). Eine Liste der verfügbaren Funktionen finden Sie in der Python-Dokumentation.\nMit der Summe der verfügbaren Datentypen und Funktionen können wir theoretisch alle Analysen durchführen, die wir uns vorstellen können. Dies ist jedoch sehr aufwendig und würde viel Zeit in Anspruch nehmen. Wir müssten nämlich alle Funktionen, die wir im Rahmen eine Datenanalyse (d.h. der Erstellung von Diagrammen, der Berechnung von Kennzahlen, der Erstellung von Statistiken, etc.) benötigen, selbst schreiben. Dies ist nicht nur sehr aufwendig, sondern auch sehr fehleranfällig.\nGlücklicherweise gibt es jedoch eine Vielzahl an Modulen und Bibliotheken, die wir nutzen können, um die Fähigkeiten von Python zu erweitern. Diese Module und Bibliotheken sind bereits für uns geschrieben und müssen nur noch importiert werden. Wir werden in diesem Kurs einige Module und Bibliotheken kennenlernen, die wir nutzen können, um unsere Analysen durchzuführen.\n\n\nImportieren von Modulen und Bibliotheken\nWir können uns die zusätzlichen Fähigkeiten, die wir über den Import von Modulen und Bibliotheken erhalten, als eine Art “Erweiterung” von Python vorstellen - vergleichbar zu Add-ons in Excel. Da es sich bei Python um eine sogenannte Open-Source Sprache handelt, können die benötigten Erweiterungen im Internet eingesehen und kostenlos heruntergeladen werden.\nWir können diese Erweiterungen nutzen, in dem wir zwei Schritte durchführen:\n\nInstallieren der Erweiterung (einmalig)\nImportieren der Erweiterung\n\n\n\n\n\n\n\nInfo\n\n\n\nSofern Sie Python via Anaconda installiert haben oder google colab nutzen, sollten die von uns benötigten Module bereits vorinstalliert sein.\n\n\nFalls dies jedoch nicht der Fall ist, können Sie die Module über das Terminal typischerweise über den Befehl pip install <modulname> installieren4. Sie können die Module auch direkt in ihrem Jupyter Notebook installieren. Dazu müssen Sie lediglich den folgenden Code in einer Zeile ausführen:4 Hinweis: <modulname> ist hier ein Platzhalter für den jeweiligen Namen der Bibliothek oder des Moduls\nimport sys\n!{sys.executable} -m pip install <modulname>\nSobald die Erweiterung installiert ist, können wir diese in unserem Code importieren. Dies geschieht über den Befehl import. Wir haben dies bereits einmal in diesem Kurs gesehen, als wir eine Exceldatei in ein DataFrame eingelesen haben.\nDer Import von Modulen erfolgt typischerweise über drei Varianten:\n\nImportieren des gesamten Moduls\nImportieren einer Funktion aus einem Modul\nImportieren des gesamten Moduls unter einem anderen Namen\n\nLassen Sie uns dies anhand des Moduls statistics (welches wir im weiteren Verlaufe des Kurses nicht weiter benötigen) verdeutlichen\nBeispiel 1: Importieren des gesamten Moduls\n\nimport statistics\n\ndaten = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\navg = statistics.mean(daten)\navg\n\n5.5\n\n\nWir haben hier das gesamte Modul importiert und können nun alle Fähigkeiten, die dieses Modul zur Verfügung stellt, nutzen. In diesem Fall haben wir die Funktion mean() aus dem Modul statistics genutzt, um den Mittelwert der Daten zu berechnen.\nBeispiel 2: Importieren einer Funktion aus einem Modul\n\nfrom statistics import mean\n\ndaten = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\navg = mean(daten)\navg\n\n5.5\n\n\nWir haben hier nur die Funktion mean() aus dem Modul statistics importiert. Dies hat den Vorteil, dass wir nicht mehr den Namen des Moduls angeben müssen, wenn wir die Funktion nutzen wollen. Jedoch können wir auch nur die Funktion mean() aus dem Modul statistics nutzen, alle anderen Funktionen sind nicht verfügbar.\nBeispiel 3: Importieren des gesamten Moduls unter einem anderen Namen\n\nimport statistics as stat\n\ndaten = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\navg = stat.mean(daten)\navg\n\n5.5\n\n\nWir haben hier das gesamte Modul statistics importiert und ihm den Namen stat gegeben. Dies hat den Vorteil, dass wir nicht mehr den Namen des Moduls angeben müssen, wenn wir die Funktion nutzen wollen. Jedoch können wir auch alle Funktionen aus dem Modul statistics nutzen, auch wenn wir diese nicht explizit importiert haben.\nOft ist es so, dass sich in der Praxis Variante 3 für bestimmte Module durchgesetzt hat. So wird z.B. das Modul pandas - unser Schweizer Taschenmesser der Datenanalyse - meist unter dem Namen pd importiert.\n\nimport pandas as pd\n\nSie müssen sich dieser Konventionen nicht anpassen, wenn Sie nicht möchten. Jedoch ist es sinnvoll, da insbesondere Hilfen im Internet (oder auch von ChatGTP) häufig unter der Annahme, dass Sie die Module entsprechend importiert haben, geschrieben sind."
  },
  {
    "objectID": "Chapters/03_Chapter/Fallstudie_1.html#zwecke-der-fallstudie",
    "href": "Chapters/03_Chapter/Fallstudie_1.html#zwecke-der-fallstudie",
    "title": "FALLSTUDIE",
    "section": "Zwecke der Fallstudie",
    "text": "Zwecke der Fallstudie\nWir haben in Kapitel 1.4 bereits erläutert, dass es sich bei Business Analytics um einen Prozess handelt, der verschiedene Schritte umfasst. In diesem Kapitel werden wir nun einen ersten, vorsichtigen Blick auf einen konkreten Fall (“Fallstudie”). Die Fallstudie soll Ihnen einen Eindruck davon vermitteln, was Sie in diesem Kurs erwartet. Diese Fallstudie verfolgt somit zwei Ziele:\n\nAnwendungsbeispiele für die einzelnen Schritte des Analyseprozesses geben und\nProgrammierbeispiele für die einzelnen Schritte des Analyseprozesses vorzustellen.\n\n\n\n\n\n\n\nHinweis\n\n\n\nIn dieser Fallstudie werden Dinge vorgegriffen, die Sie noch nicht gelernt haben. Ziel der Fallstudie ist es ausdrücklich nicht, dass Sie jede Code-Zeile nachvollziehen können. Vielmehr sollen Sie einen Eindruck davon vermittelt bekommen, was Sie in diesem Kurs erwartet bzw. was Sie - vorausgesetzt Sie arbeiten kontinuierlich mit - am Ende in der Lage sind selber umzusetzen.\n\n\nWir haben uns bewusst für diesen Top-Down-Ansatz entschieden, d.h. wir beginnen mit dem großen Ganzen und gehen dann Schritt für Schritt in die Details.\nWarum glauben wir, dass dieser Ansatz sinnvoll ist?\nAls Analogie können Sie sich vorstellen, dass wir einem kleinen Kind beibringen wollen Fußball zu spielen. Sie würden nie auf die Idee kommen, dem kleinen Kind als erstes die korrekte Fußstellung beim Freistoß oder gar die Abseitsregel zu erläutern. Stattdessen würden Sie vielleicht mit dem Kind auf den nächsten Fußballplatz gehen und zuschauen, wie andere Fußball spielen. Sie würden also zunächst das große Ganze zeigen - ohne, dass das Kind alle Details bereits versteht - und es so neugierig auf das Spiel machen. Die Details und Feinheiten folgen dann Schritt für Schritt. Das Kind wird einen Ball haben und auch spielen wollen.\nGehen Sie diese Fallstudie also nicht mit dem Anspruch durch, alles bereits verstehen und selber umsetzen zu müssen. Vielmehr sollten Sie die übergeordnete Fragestellung durchdenken, um dann zu erleben, wie diese Fragestellung mittels Python beantwortet wird. Dies wird Sie hoffentlich für das große Ganze begeistern und so das Fundament für alles weitere legen."
  },
  {
    "objectID": "Chapters/03_Chapter/Fallstudie_1.html#fallstudie-fashion-avenue-gmbh",
    "href": "Chapters/03_Chapter/Fallstudie_1.html#fallstudie-fashion-avenue-gmbh",
    "title": "FALLSTUDIE",
    "section": "Fallstudie: Fashion Avenue GmbH",
    "text": "Fallstudie: Fashion Avenue GmbH\n\nEinleitung\nDie Fashion Avenue GmbH ist ein Einzelhandelsunternehmen, das sich auf den Verkauf von Bekleidung und Accessoires spezialisiert hat. Sie bietet eine breite Palette von Produkten für Frauen, Männer und Kinder an und betreibt in Bielefeld mehrere Filialen.\nDie Firma wurde vor mehreren Jahren von Anna Meyer - einer erfahrenen Einzelhändlerin - gegründet. Sie hat sich zum Ziel gesetzt, hochwertige und modische Produkte zu erschwinglichen Preisen anzubieten. Im Laufe der Jahre hat sich die Firma zu einem wichtigen Akteur in der Einzelhandelsbranche Bielefelds entwickelt und hat sich einen Namen als eine Marke gemacht, die für Qualität und Service bekannt ist. Die Mitarbeiter des Unternehmens “Fashion Avenue” sind ein engagiertes Team von Verkäufer:innen. Sie bestehen aus einer Mischung aus erfahrenen Verkäufern und neuen, jungen und motivierten Mitarbeitern, die die Marke repräsentieren und für ihre hohe Kundenorientierung und ihren ausgezeichneten Service bekannt sind.\nSeit einigen Monaten sind die Verkäufe jedoch rückläufig und Anna Meyer fragt sich, was der Grund dafür sein könnte. Es wurden auch bereits Gespräche mit einigen Mitarbeiter:innen geführt, jedoch kann niemand den Grund für den rückläufigen Umsatz erkennen.\nUm dies herauszufinden, hat das Unternehmen begonnen, Daten über alle Transaktionen des letzten Jahres zu sammeln und möchte nun diese Daten analysieren, um die Ursache für den Rückgang der Verkäufe zu finden.\n\n\nAnalyseprozess\nDer Analyseprozess besteht aus folgenden Schritten, die wir im Folgenden durchlaufen werden:\n\nProblemstellung\nEinlesen der Daten\nAufbereitung der Daten\nTransformation der Daten\nVisualisierung der Daten\nModellierung der Daten\nKommunikation der Ergebnisse (TODO: bedingt Interpretation der Ergebnisse)\n\nIn Kapitel 1.4 haben wir diesen graphisch wie folgt zusammengefasst:\n\n\n\nAnalyseprozess\n\n\nLassen Sie uns nun die einzelnen Schritte genauer betrachten.\n\nProblemstellung\nDer erste Schritt der Datenanalyse ist die Definition des Analyseproblems. Dieser Schritt beinhaltet noch keine Datenanalyse im eigentlichen Sinne, ist jedoch essenziell für alle weiteren Schritte. Denn: ohne ein klar umrissenes Analyseproblem ist es nicht möglich, die richtigen Daten auszuwählen, die richtigen Analysen durchzuführen etc. Wir laufen zudem Gefahr, dass wir uns mit unser Analyse “verzetteln”, d.h. wir Ergebnisse produzieren, jedoch keine Antwort auf die eigentliche Fragestellung liefern.\nMachen wir uns also zunächst klar, was wir herausfinden wollen. Aus der Einleitung wissen wir, dass die Verkäufe rückläufig sind und der Grund dafür bisher nicht identifiziert werden konnte. Die Fragestellung könnte also lauten:\n\nWas ist der Grund für den Rückgang des Umsatzes?\n\n\n\nEinlesen der Daten\nNatürlich fokussieren wir uns in diesem Kurs auf die technischen Aspekte des Einlesens von Daten. Jedoch ist es wichtig an dieser Stelle zu betonen, dass dem Einlesen der Daten natürlich mindestens zwei weitere Schritte vorausgehen:\nZunächst müssen wir uns die Frage stellen, welche Daten wir benötigen, um die Fragestellung zu beantworten. Wir müssen uns Fragen, welche Variablen für unsere Analyse relevant sind. Bei der Überlegung geht es explizit auch um Daten, die im Unternehmen vielleicht gar nicht vorliegen. Schließlich gibt es auch die Möglichkeit, Daten zu generieren, die wir für unsere Analyse benötigen. Neben der Identifizierung der benötigten Daten ist es dann wichtig zu überprüfen, ob und in welcher Form die Daten überhaupt vorhanden sind. Idealerweise liegen die benötigten Daten bereits intern vor - z.B. in Form einer bestehenden Datenbank oder Excel-Dateien. Sollte dies nicht der Fall sein, gibt es vielleicht die Möglichkeit Daten extern - z.B. über externe Datenanbieter - einzukaufen. Auch besteht ggf. die Möglichkeit Daten intern zu erheben (z.B. über Kundenumfragen) oder zu generieren (z.B. über die Simulation von Prozessen etc.).\nIn Summe sind diese Voranalysen wichtig und als Unternehmen sollten wir uns diese Zeit nehmen, um die Datenanalyse effizient und zielführend durchzuführen. Auch sollte dem Hang widerstanden werden “nur” interne und bereits verfügbare Daten zu analysieren. Dies mag auf den ersten Blick einfacher erscheinen, jedoch kann es sein, dass wir uns dadurch der Möglichkeit berauben, die Fragestellung sinnvoll oder zumindest besser zu beantworten.\nIn unserem vorliegenden Fall haben wir die für uns (zumindest im ersten Schritt) relevanten Daten bereits in Form von Transaktionsdaten vorliegen. Die Daten wurden aus dem ERP-System exportiert und liegen nun als CSV-Datei vor.\nUm die Daten einzulesen und weiterzuverarbeiten, verwenden wir die Python-Bibliothek pandas. Die Python-Bibliothek (siehe Dokumentation]) ist ein wichtiges Werkzeug für die Datenanalyse. Sie ermöglicht das einfache Einlesen, Verarbeiten und Manipulieren von Daten in Python. Mit pandas kann man schnell und einfach Tabellen (Dataframes) erstellen, in denen die Daten gespeichert werden. Die Bibliothek bietet zudem viele nützliche Funktionen, wie z.B. die Möglichkeit, Daten zu filtern, zu sortieren und zu aggregieren. Auch das Zusammenführen von mehreren Tabellen ist mit pandas sehr einfach möglich. Daher wird die Bibliothek oft in der Datenanalyse verwendet und ist ein wichtiger Bestandteil unseres “Analyse-Werkzeugkoffers” in Python.\nWir lesen die Daten nun mit der Funktion read_csv() ein und weisen die Daten der Variable df zu. Danach geben wir - mit der Funktion head()- die ersten fünf Zeilen der Tabelle aus, um einen ersten Überblick über die Daten zu erhalten.\n\n# Importieren der Bibliothek pandas\nimport pandas as pd \n\n# Einlesen der Daten (hier: csv-Datei)\ndf = pd.read_csv(\"../../_data/transactions_fashion_avenue.csv\")\n\n# Ausgabe der ersten fünf Zeilen der Tabelle\ndf.head()\n\n\n\n\n\n  \n    \n      \n      Kundenname\n      Alter\n      Zahlungsmethode\n      ø Preis\n      Menge\n      Datum\n      Uhrzeit\n    \n  \n  \n    \n      0\n      Gislinde Börner\n      29\n      EC-Karte\n      14.99\n      4\n      01.01.2022\n      10:24:44\n    \n    \n      1\n      Pierre Ullmann\n      28\n      Bar\n      17.99\n      4\n      01.01.2022\n      10:55:09\n    \n    \n      2\n      Rainer Birnbaum\n      57\n      Bar\n      43.99\n      4\n      01.01.2022\n      11:25:31\n    \n    \n      3\n      Ekaterina Binner\n      47\n      Kreditkarte\n      36.99\n      3\n      01.01.2022\n      11:30:28\n    \n    \n      4\n      Prof. Mandy Riehl\n      59\n      Bar\n      35.99\n      4\n      01.01.2022\n      11:46:50\n    \n  \n\n\n\n\n\n\nAufbereitung der Daten\nNachdem wir die Daten eingelesen haben, können wir uns nun mit der Aufbereitung der Daten beschäftigen. Hierzu gehören z.B. das Entfernen von fehlenden Werten, das Korrigieren von offensichtlich fehlerhaften Werte, das Umwandeln von Datentypen, das Umbennen von Spalten etc.\nSchauen wir uns zunächst an, ob Daten fehlen. Dazu verwenden wir die Funktion info(). Diese Funktion gibt uns Informationen über die Tabelle, wie z.B. die Anzahl der Zeilen, die Anzahl der Spalten, den Datentyp der Spalten, die Anzahl der nicht-leeren Werte etc.\n\n# Ausgabe von Informationen über die Tabelle\ndf.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 10000 entries, 0 to 9999\nData columns (total 7 columns):\n #   Column           Non-Null Count  Dtype  \n---  ------           --------------  -----  \n 0   Kundenname       10000 non-null  object \n 1   Alter            10000 non-null  int64  \n 2   Zahlungsmethode  10000 non-null  object \n 3   ø Preis          10000 non-null  float64\n 4   Menge            10000 non-null  int64  \n 5   Datum            10000 non-null  object \n 6   Uhrzeit          10000 non-null  object \ndtypes: float64(1), int64(2), object(4)\nmemory usage: 547.0+ KB\n\n\nWir sehen, dass es sich bei dem Datensatz um 10.000 Transaktionen handelt und dass keine der Spalten fehlende Werte enthält. Im vorliegenden Fall ist der Datensatz bereits realtiv sauber und wir müssen keine größeren Aufbereitungen vornehmen.\nLassen Sie uns lediglich zwei Dinge verändern, die uns das Leben in der weiteren Analyse vereinfachen werden:\n1. Umbennen der Spalte ø Preis1: wir bennen die Spalte in Preis um. Das erleichtert uns den Verweis auf die Spalte zukünftig etwas. Wir verwenden hierfür die Funktion rename().1 Hinweis: ein­fach­heits­hal­ber haben wir hier den durschnittlichen Preis der gesamten Transaktion geben, d.h. \\(\\frac{\\text{Preis der Transaktion}}{\\text{Anzahl gekaufter Produkte}} = ø Preis.\\) Reale Daten würden meist die Preise je Produkt enthalten.\n2. Ändern des Datentyps der Spalten Datum und Uhrzeit: die beiden Spalten Datum und Uhrzeit sind aktuell als object gespeichert. Der Datentyp object ist ein generischer Datentyp, der für alle Arten von Daten verwendet werden kann, er bietet sich an, wenn wir in einer Spalte z.B. sowohl Zahlen als auch Texte speichern wollen. Ein Nachteil des Datentyps object ist, dass wir mit diesen Daten nicht weiterrechnen bzw., dass viele für spezielle Datentypen entwickelte Funktionen nicht angewendet werden können. In unserem Fall handelt es sich bei den Spalten Datum und Uhrzeit um Datumsangaben und wir möchten diese Spalten später auch als solche verwenden. So können wir z.B. die Spalte Datum nutzen, um die Transaktionen nach Wochentagen oder Monaten zu gruppieren. Dies ist jedoch nur möglich, wenn die Spalte als Datumsangabe gespeichert ist. Wir können den Datentyp der Spalten Datum mit der Funktion astype() in den Datentype datetime64[ns] ändern.\n\n# Umbenennen der Spalte \"ø Preis\" in \"Preis\"\ndf = df.rename(columns={\"ø Preis\": \"Preis\"})\n\n# Ändern des Datentyps der Spalten \"Datum\" und \"Uhrzeit\"\ndf = df.astype({\"Datum\": \"datetime64[ns]\"})\n\nWir können nun nochmals die Funktion info() verwenden, um zu überprüfen, ob die Änderungen korrekt durchgeführt wurden.\n\n# Ausgabe von Informationen über die Tabelle\ndf.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 10000 entries, 0 to 9999\nData columns (total 7 columns):\n #   Column           Non-Null Count  Dtype         \n---  ------           --------------  -----         \n 0   Kundenname       10000 non-null  object        \n 1   Alter            10000 non-null  int64         \n 2   Zahlungsmethode  10000 non-null  object        \n 3   Preis            10000 non-null  float64       \n 4   Menge            10000 non-null  int64         \n 5   Datum            10000 non-null  datetime64[ns]\n 6   Uhrzeit          10000 non-null  object        \ndtypes: datetime64[ns](1), float64(1), int64(2), object(3)\nmemory usage: 547.0+ KB\n\n\nWie wir sehen können, wurden die Änderungen korrekt durchgeführt.\n\n\nErkenntnisse gewinnen: Transformation und Visualisierung\nWie bereits im Kapitel Kapitel 1.4 beschrieben, befinden wir uns nun in der explorativen Analysephase, in der sich die Schritte2:2 Hinweis: den Schritt der Modellierung lassen wir an dieser Stelle noch aus\n\nTransformation der Daten\nVisualisierung der Daten\n\noft iterativ wiederholen.\nWir haben einen “sauberen” Datensatz vorliegen und können uns nun mit der Fragestellung beschäftigen und versuchen, die Ursache für den Umsatzrückgang zu finden. In diesem Schritt ist es üblich, dass wir erste Hypothesen aufstellen und versuchen, diese mit den Daten und mittels einfacher statistischer Methoden (z.B. Durchschnitte über Gruppen bilden) zu überprüfen.\nIm ersten Schritt sollten wir jedoch zunächst überprüfen, ob die Ist-Situation - d.h. der vom Unternehmen beschriebene Umsatzrückgang - aus den Daten ersichtlich wird. Hierfür könnten wir z.B. die monatlichen Umsätze berechnen und schauen, ob wir einen negativen Trend beobarten können.\nSchauen wir uns dafür den Datensatz nochmals an:\n\n# Ausgabe der ersten 5 Zeilen\ndf.head()\n\n\n\n\n\n  \n    \n      \n      Kundenname\n      Alter\n      Zahlungsmethode\n      Preis\n      Menge\n      Datum\n      Uhrzeit\n    \n  \n  \n    \n      0\n      Gislinde Börner\n      29\n      EC-Karte\n      14.99\n      4\n      2022-01-01\n      10:24:44\n    \n    \n      1\n      Pierre Ullmann\n      28\n      Bar\n      17.99\n      4\n      2022-01-01\n      10:55:09\n    \n    \n      2\n      Rainer Birnbaum\n      57\n      Bar\n      43.99\n      4\n      2022-01-01\n      11:25:31\n    \n    \n      3\n      Ekaterina Binner\n      47\n      Kreditkarte\n      36.99\n      3\n      2022-01-01\n      11:30:28\n    \n    \n      4\n      Prof. Mandy Riehl\n      59\n      Bar\n      35.99\n      4\n      2022-01-01\n      11:46:50\n    \n  \n\n\n\n\nEs wird deutlich, dass die Information Umsatz nicht (explizit) im Datensatz enthalten ist. Wir können jedoch die Information Umsatz aus den Spalten Menge und Preis berechnen. Wir können dies über die Funktion assign() erreichen. Darüber hinaus können wir auch aus der Spalte Datum weitere Informationen (Monat, Woche etc.) extrahieren. Wir können dies mit der Funktion dt. erreichen.\n\n# Hinzufügen einer neuen Spalte \"Umsatz\", \"Wochentag\" und \"Monat\"\ndf = df.assign(Umsatz = df[\"Menge\"] * df[\"Preis\"], \n               Monat = df[\"Datum\"].dt.month, \n               Woche = df[\"Datum\"].dt.week,\n               Wochentag = df[\"Datum\"].dt.day_name())\n\n# Ausgabe der ersten fünf Zeilen der Tabelle\ndf.head()\n\n/var/folders/cj/9s881x057_12d_qyp3jcmb380000gn/T/ipykernel_13841/3851467052.py:4: FutureWarning:\n\nSeries.dt.weekofyear and Series.dt.week have been deprecated.  Please use Series.dt.isocalendar().week instead.\n\n\n\n\n\n\n\n  \n    \n      \n      Kundenname\n      Alter\n      Zahlungsmethode\n      Preis\n      Menge\n      Datum\n      Uhrzeit\n      Umsatz\n      Monat\n      Woche\n      Wochentag\n    \n  \n  \n    \n      0\n      Gislinde Börner\n      29\n      EC-Karte\n      14.99\n      4\n      2022-01-01\n      10:24:44\n      59.96\n      1\n      52\n      Saturday\n    \n    \n      1\n      Pierre Ullmann\n      28\n      Bar\n      17.99\n      4\n      2022-01-01\n      10:55:09\n      71.96\n      1\n      52\n      Saturday\n    \n    \n      2\n      Rainer Birnbaum\n      57\n      Bar\n      43.99\n      4\n      2022-01-01\n      11:25:31\n      175.96\n      1\n      52\n      Saturday\n    \n    \n      3\n      Ekaterina Binner\n      47\n      Kreditkarte\n      36.99\n      3\n      2022-01-01\n      11:30:28\n      110.97\n      1\n      52\n      Saturday\n    \n    \n      4\n      Prof. Mandy Riehl\n      59\n      Bar\n      35.99\n      4\n      2022-01-01\n      11:46:50\n      143.96\n      1\n      52\n      Saturday\n    \n  \n\n\n\n\nWir können nun den Umsatztrend über die Zeit untersuchen. Um den Trend in einer Zeitreihe zu analysieren bietet es sich an, die Zeitreihe graphisch darzustellen. Lassen Sie uns zunächst die Umsatzdaten in einem Liniendiagramm darstellen. Auch wenn dies nicht zwingend notwendig ist und es auch andere Ansätze gäbe, verwenden wir die Bibliothek seaborn (siehe hier). Die Bibliothek wurde für die Zwecke der explorativen Datenanalyse entwickelt und bietet eine Vielzahl von Funktionen, die es uns ermöglichen, die Daten schnell und einfach zu visualisieren. Um ein Liniendiagramm zu erstellen, verwenden wir die Funktion lineplot().\n\n# Laden der Bibliothek seaborn\nimport seaborn as sns\n\n# Erstellen eines Liniendiagramms\nline = sns.lineplot(data=df, x=\"Datum\", y=\"Umsatz\", ci=None);\n\n\n\n\n\n\n\n\nDer Trend ist deutlich erkennbar. Der Umsatz scheint kontinuierlich zu sinken. Jedoch ist der Umsatz sehr volatil, da wir jede einzelne Transaktion im Graphen visualisiert haben. Es macht daher Sinn, die Umsatzdaten zu aggregieren und dann zu visualisieren. Wir können z.B. die Umsätze einer Woche oder eines Monats aggregieren und dann in einem Liniendiagramm darstellen. Lassen Sie uns die Wochenumsätze betrachten. Hierfür müssen wir die Transaktionen für jede Woche im Jahr (d.h. Wochen 1 bis 52) aufsummieren. Dazu verwenden wir die Funktion groupby(), um die Daten zu gruppieren und die Funktion agg() um die Daten zu aggregieren. In unserem speziellen Fall wollen wir die Summe der Umsätze pro Woche berechnen.\n\n# Aggregieren der Umsätze pro Woche\numsatz_pro_woche = df.groupby(\"Woche\").agg({\"Umsatz\": \"sum\"})\nsns.lineplot(data=umsatz_pro_woche, x=umsatz_pro_woche.index, y=\"Umsatz\");\n\n\n\n\n\n\n\n\nWir sehen, dass der Umsatzrückgang auch für die Wochenumsätze deutlich erkennbar ist. Lassen Sie uns nun die Monatsumsätze betrachten. Hierfür müssen wir die Transaktionen für jeden Monat im Jahr (d.h. Monate 1 bis 12) aufsummieren. Der obige Code kann hierfür nahezu unverändert verwendet werden; lediglich die Spalte Woche muss durch die Spalte Monat ersetzt werden.\n\n# Aggregieren der Umsätze pro Monat\numsatz_pro_monat = df.groupby(\"Monat\").agg({\"Umsatz\": \"sum\"})\nsns.lineplot(data=umsatz_pro_monat, x=umsatz_pro_monat.index, y=\"Umsatz\");\n\n\n\n\n\n\n\n\nNachdem wir die Problembeschreibung validiert haben, können wir nun mit der Analyse der Ursachen beginnen. Lassen Sie uns dafür zunächst überlegen, was mögliche Ursachen für den Umsatzrückgang sein könnten.\nHier ein paar Beispiele die uns spontan in den Sinn kommen könnten3:3 Hinweis: an dieser Stelle ist es wichtig, dass Sie sich nicht auf eine einzige Ursache festlegen. Es können auch mehrere oder andere Ursachen für den Umsatzrückgang verantwortlich sein. Wichtig ist, dass Sie sich auf die Ursachen konzentrieren, die Sie im Rahmen der Analyse überprüfen können.\n\ndie Preise sind gesunken\ndie Nachfrage ist gesunken\ndas Zahlungsverhalten der Kunden hat sich verändert\ndie Kundenstruktur hat sich verändert\n\nLassen Sie uns nun die vier möglichen Ursachen genauer untersuchen und schauen, ob wir Hinweise auf die Ursache des Umsatzrückgangs finden können.\nUrsache 1: Preise sind gesunken\nEine mögliche Ursache für den Umsatzrückgang ist, dass die Kunden je Kauf (d.h. in unserem Fall je Transaktion bzw. je Zeile im Datensatz) weniger Geld ausgeben. Um dies zu überprüfen können wir schauen, ob der Preis je Transaktion über die Zeit gesunken ist. Lassen Sie uns dazu auch wieder eine monatliche Aggregation der Daten durchführen. Wir gruppieren also unsere Daten wieder nach Monat und aggregieren die Preise pro Monat. Für den Preis macht es jedoch keinen Sinn diesen zu summieren, sondern wir wollen den Durchschnittspreis pro Monat berechnen. Dazu verwenden wir die Funktion mean().\n\n# Aggregieren der Preise pro Monat\npreis_pro_monat = df.groupby(\"Monat\").agg({\"Preis\": \"mean\"})\npreis_pro_monat\n\n\n\n\n\n  \n    \n      \n      Preis\n    \n    \n      Monat\n      \n    \n  \n  \n    \n      1\n      35.201020\n    \n    \n      2\n      34.296389\n    \n    \n      3\n      34.269343\n    \n    \n      4\n      33.291337\n    \n    \n      5\n      32.795621\n    \n    \n      6\n      31.940061\n    \n    \n      7\n      31.249649\n    \n    \n      8\n      31.261226\n    \n    \n      9\n      30.259324\n    \n    \n      10\n      29.874706\n    \n    \n      11\n      29.948788\n    \n    \n      12\n      29.117427\n    \n  \n\n\n\n\n\nsns.lineplot(data=preis_pro_monat, x=preis_pro_monat.index, y=\"Preis\");\n\n\n\n\nDie Analyse zeigt deutlich, dass der durchschnitlliche Umsatz je Transaktion über die Zeit gesunken ist. Während Kunden im Januar 2022 noch durchschnittlich 35,20 € pro Transaktion ausgegeben haben, waren es im Dezember 2022 nur noch 29,11 €. Davon ausgehend, dass das Unternehmen die Preise in diesem Zeitraum nicht gesenkt hat, ist dies ein Indikator dafür, dass Kunden günstigere Produkte auszuwählen scheinen.\nUrsache 2: Nachfrage ist gesunken\nEine weitere mögliche Ursache für den Umsatzrückgang ist, dass die Nachfrage nach den Produkten des Unternehmens gesunken ist. Um dies zu überprüfen können wir schauen, ob die durchschnittliche Menge je Transaktion über die Zeit gesunken ist. Lassen Sie uns dazu auch wieder eine monatliche Aggregation der Daten durchführen. Wir gruppieren also unsere Daten wieder nach Monat und aggregieren die Mengen pro Monat. Für die Menge macht es jedoch keinen Sinn diese zu summieren, sondern wir wollen die durchschnittliche Menge pro Monat berechnen. Dazu verwenden wir die Funktion mean().\n\n# Aggregieren der Menge pro Monat \nmenge_pro_monat = df.groupby(\"Monat\").agg({\"Menge\": \"mean\"})\nmenge_pro_monat\n\n\n\n\n\n  \n    \n      \n      Menge\n    \n    \n      Monat\n      \n    \n  \n  \n    \n      1\n      4.329426\n    \n    \n      2\n      4.362451\n    \n    \n      3\n      4.274648\n    \n    \n      4\n      4.283111\n    \n    \n      5\n      4.078454\n    \n    \n      6\n      4.098660\n    \n    \n      7\n      4.040936\n    \n    \n      8\n      4.048349\n    \n    \n      9\n      3.880435\n    \n    \n      10\n      3.888235\n    \n    \n      11\n      3.953939\n    \n    \n      12\n      3.811893\n    \n  \n\n\n\n\n\nsns.lineplot(data=menge_pro_monat, x=menge_pro_monat.index, y=\"Menge\");\n\n\n\n\nAus der Analyse geht hervor, dass auch die durchschnittliche Menge je Transaktion über die Zeit gesunken ist. In Summe hat unsere (an dieser Stelle nur sehr oberflächliche) Analyse aufgezeigt, dass Kunden je Einkauf günstigere und kleinere Mengen kaufen.\nLassen Sie uns nun noch die beiden Ursachen 3 und 4 untersuchen, da wir uns zum einen nicht auf eine einzige Ursache festlegen wollen und zum anderen auch die anderen Ursachen nicht ausschließen können. Auch können uns die weiteren Analysen ggf. Aufschluss darüber geben, warum die Kunden günstigere und kleinere Mengen kaufen.\nUrsache 3: Zahlungsverhalten der Kunden hat sich verändert\nDer Datensatz enthält nicht viele Informationen über das Zahlungsverhalten. Wir haben lediglich die Information über die Zahlungsmethode. Schauen wir uns deshalb doch zunächst an, ob die Kunden eine Zahlungsmethode bevorzugen.\nDazu berechnen wir zunächst die Gesamtumsätze sowie Anzahl an Transaktionen je Zahlungsmethode. Wir können dies wieder über einen simple Gruppierung der Daten via groupby() und anschließender Aggregation der Daten via agg() erreichen.\n\n# Berechnen der Gesamtumsätze und Anzahl an Transaktionen je Zahlungsmethode\numsatz_je_zahlungsmethode = df.groupby(\"Zahlungsmethode\").agg({\"Umsatz\": [\"sum\", \"count\"]})\numsatz_je_zahlungsmethode\n\n\n\n\n\n  \n    \n      \n      Umsatz\n    \n    \n      \n      sum\n      count\n    \n    \n      Zahlungsmethode\n      \n      \n    \n  \n  \n    \n      Bar\n      476976.95\n      3370\n    \n    \n      EC-Karte\n      477491.92\n      3413\n    \n    \n      Kreditkarte\n      446887.50\n      3217\n    \n  \n\n\n\n\nDie Aufstellung zeigt, dass die Summe an Umsätzen sowie die Anzahl an Transaktionen sich nicht sehr stark unterscheiden, je nachdem ob die Kunden mit Kreditkarte, Bar oder per EC-Karte bezahlt haben.\nDurch die Aggregation der Umsätze kann Information verloren gehen, so dass es mit unter Sinn macht, die einzelnen Transaktionsdaten zu visualisieren. Hierzu eignen sich Diagramme, die Verteilungen und Streuungen von Daten darstellen. Ein solches Diagramm ist das sogenannte Stripplot. Dieses Diagramm zeigt die Verteilung der Datenpunkte in einem Diagramm an.\n\nsns.stripplot(data=df, x=\"Zahlungsmethode\", y=\"Umsatz\");\n\n\n\n\n\n\n\n\nEs wird deutlich, dass sich die Streuung der Umsätze je nach Zahlungsmethode nicht wesentilch unterscheiden.\nWir wissen nun, dass Kunden, die z.B. viel und teuer erkaufen nicht eine bestimmte Zahlungsmethode bevorzugen. Jedoch haben wir noch nicht untersucht, ob sich das Zahlungsverhalten der Kunden über die Zeit verändert hat.\nLassen Sie uns dies nun untersuchen. Dazu berechnen wir die Umsätze je Monat und je Zahlungsmethode. Das folgende Diagramm zeigt die Umsätze je Monat und je Zahlungsmethode.\n\nsns.catplot(data=df, x=\"Monat\", y=\"Umsatz\", kind=\"point\", hue=\"Zahlungsmethode\", ci=None);\n\n\n\n\n\n\n\n\nDie Analyse liefert uns keine neuen Erkenntnisse hinsichtlich der Ursache für den Umsatzrückgang. Die Umsätze je Monat und je Zahlungsmethode unterscheiden sich nicht wesentlich und insgesamt ist der Umsatzrückgang über die Zeit sehr ähnlich.\nLassen Sie uns deshalb die vierte mögliche Ursache untersuchen.\nUrsache 4: Kundenstruktur hat sich verändert\nAuch zur Kundenstruktur haben wir nur wenige Informationen. Wir wissen lediglich, wie alt die Kunden sind. Für weitere Analysen müssten wir uns mit dem Unternehmen in Verbindung setzen und weitere Informationen über die Kundenstruktur erhalten.\nBevor wir eine mögliche Veränderung der Kundenstruktur (in unserem Falle: Alter) analysieren, sollten wir zunächst die Verteilung der Kunden anhand ihres Alters untersuchen, um ein Gefühl für unsere Kundenstruktur zu bekommen. Wir können das über ein Histogramm visualisieren, welches wir mit der Funktion histplot() aus der Bibliothek seaborn erstellen können.\n\nsns.histplot(data=df, x=\"Alter\");\n\n\n\n\n\n\n\n\nDie Verteilung der Kunden zeigt, dass unsere Kunden grds. aus allen Altersgruppen kommen. Jedoch fällt auf, dass die Verteilung zwei Spitzen aufweist. Die erste Spitze liegt bei ca. 25 Jahren, die zweite Spitze bei ca. 55 Jahren.\nLassen Sie uns nun schauen, ob Kunden mit einem bestimmten Alter mehr oder weniger kaufen. Um dies zu untersuchen, berechnen wir einen neue Variable Altersgruppe, die die Kunden in Altersgruppen von 10 Jahren aufteilt, d.h. die Kunden werden in die Altersgruppen 0-20, 21-30, 31-40, …, 81-90, 91-100 eingeteilt.44 Hinweis: wir müssen diese Aufteilung in Gruppen nicht vornehmen, jedoch macht diese die Visualisierung der Daten einfacher.\n\n# Berechnen der Altersgruppen\ndf[\"Altergruppe\"] = pd.cut(df[\"Alter\"], \n                            bins=[0, 20, 30, 40, 50, 60, 70, 80, 90, 100], \n                            labels=[\"0-20\", \"21-30\", \"31-40\", \"41-50\", \"51-60\", \"61-70\", \"71-80\", \"81-90\", \"91-100\"])\ndf.head()\n\n\n\n\n\n  \n    \n      \n      Kundenname\n      Alter\n      Zahlungsmethode\n      Preis\n      Menge\n      Datum\n      Uhrzeit\n      Umsatz\n      Monat\n      Woche\n      Wochentag\n      Altergruppe\n    \n  \n  \n    \n      0\n      Gislinde Börner\n      29\n      EC-Karte\n      14.99\n      4\n      2022-01-01\n      10:24:44\n      59.96\n      1\n      52\n      Saturday\n      21-30\n    \n    \n      1\n      Pierre Ullmann\n      28\n      Bar\n      17.99\n      4\n      2022-01-01\n      10:55:09\n      71.96\n      1\n      52\n      Saturday\n      21-30\n    \n    \n      2\n      Rainer Birnbaum\n      57\n      Bar\n      43.99\n      4\n      2022-01-01\n      11:25:31\n      175.96\n      1\n      52\n      Saturday\n      51-60\n    \n    \n      3\n      Ekaterina Binner\n      47\n      Kreditkarte\n      36.99\n      3\n      2022-01-01\n      11:30:28\n      110.97\n      1\n      52\n      Saturday\n      41-50\n    \n    \n      4\n      Prof. Mandy Riehl\n      59\n      Bar\n      35.99\n      4\n      2022-01-01\n      11:46:50\n      143.96\n      1\n      52\n      Saturday\n      51-60\n    \n  \n\n\n\n\nNun können wir die Umsätze je Altersgruppe berechnen und visualisieren. Die untenstehende Tablle zeigt die durchschnittlichen Umsätze je Transaktion und je Altersgruppe.\n\n# Berechnen der Umsätze je Altersgruppe\numsatz_je_alter = df.groupby(\"Altergruppe\").agg({\"Umsatz\": [\"mean\", \"count\"]})\numsatz_je_alter\n\n\n\n\n\n  \n    \n      \n      Umsatz\n    \n    \n      \n      mean\n      count\n    \n    \n      Altergruppe\n      \n      \n    \n  \n  \n    \n      0-20\n      61.991679\n      137\n    \n    \n      21-30\n      63.482432\n      2738\n    \n    \n      31-40\n      66.317305\n      1629\n    \n    \n      41-50\n      193.778857\n      1024\n    \n    \n      51-60\n      203.674966\n      2827\n    \n    \n      61-70\n      204.916333\n      1448\n    \n    \n      71-80\n      203.356615\n      192\n    \n    \n      81-90\n      207.144000\n      5\n    \n    \n      91-100\n      NaN\n      0\n    \n  \n\n\n\n\nDie Tabelle zeigt deutlich, dass unterschiedliche Altergruppen unterschiedliche Umsätze generieren. Kunden bis ca. 40 Jahre geben im Schnitt deutlich weniger aus, als Kunden, die älter als 40 Jahre sind.\nAus der untenstehenden Visualisierung geht dies noch deutlicher hervor.\n\nsns.stripplot(data=df, x=\"Altergruppe\", y=\"Umsatz\", jitter=0.1);\n\n\n\n\nEs scheint also so, als ob sich die Kundenstruktur verändert hat. Kunden bis ca. 40 Jahre kaufen weniger oder günstiger ein, als Kunden, die älter als 40 Jahre sind.\nDiese Erkenntnis alleine reicht jedoch nicht aus, um die Ursache für den Umsatzrückgang zu finden. Wir müssen nun untersuchen, ob sich die Altersstruktur über die Zeit verändert hat. Eine Erklärung für den Umsatzrückgang könnte sein, dass das Unternehmen mit der Zeit mehr Kunden aus der Altersgruppe “unter 40” gewonnen hat, die weniger oder günstiger kaufen.\nUm dies zu untersuchen, berechnen wir zunächst eine neue Variable Kundengruppe mit den Werten “alt” (>40 Jahre) und “jung” (<=40 Jahre). Wir können dann im Anschluss schauen, ob der Anteil an jungen Kunden in den letzten Monaten gestiegen ist.\n\n# Berechnen der Kundengruppe\nimport numpy as np \ndf[\"Kundengruppe\"] = np.where(df[\"Alter\"] <= 40, \"jung\", \"alt\")\ndf.head()\n\n\n\n\n\n  \n    \n      \n      Kundenname\n      Alter\n      Zahlungsmethode\n      Preis\n      Menge\n      Datum\n      Uhrzeit\n      Umsatz\n      Monat\n      Woche\n      Wochentag\n      Altergruppe\n      Kundengruppe\n    \n  \n  \n    \n      0\n      Gislinde Börner\n      29\n      EC-Karte\n      14.99\n      4\n      2022-01-01\n      10:24:44\n      59.96\n      1\n      52\n      Saturday\n      21-30\n      jung\n    \n    \n      1\n      Pierre Ullmann\n      28\n      Bar\n      17.99\n      4\n      2022-01-01\n      10:55:09\n      71.96\n      1\n      52\n      Saturday\n      21-30\n      jung\n    \n    \n      2\n      Rainer Birnbaum\n      57\n      Bar\n      43.99\n      4\n      2022-01-01\n      11:25:31\n      175.96\n      1\n      52\n      Saturday\n      51-60\n      alt\n    \n    \n      3\n      Ekaterina Binner\n      47\n      Kreditkarte\n      36.99\n      3\n      2022-01-01\n      11:30:28\n      110.97\n      1\n      52\n      Saturday\n      41-50\n      alt\n    \n    \n      4\n      Prof. Mandy Riehl\n      59\n      Bar\n      35.99\n      4\n      2022-01-01\n      11:46:50\n      143.96\n      1\n      52\n      Saturday\n      51-60\n      alt\n    \n  \n\n\n\n\nDen Anteil an jungen Kunden je Monat berechnen wir, indem wir die Anzahl der jungen Kunden je Monat durch die Gesamtanzahl der Kunden je Monat teilen. Die untenstehende Tabelle zeigt die Ergebnisse.\n\n# Berechnen des Anteils an jungen Kunden je Monat\ndf.groupby([\"Monat\"]).agg({\"Kundengruppe\": [lambda group: sum(group == \"jung\")/len(group)]}).plot(legend=False);\n\n\n\n\nEs ist offensichtlich, dass der Anteil an jungen Kunden in den letzten Monaten gestiegen ist. Dies könnte die Ursache für den Umsatzrückgang sein.\nDie untenstehende Visualisierung zeigt die Entwicklung der Umsätze je Monat und je Kundengruppe (jung vs. alt). Die blaue Linie zeigt die Umsätze der jungen Kunden, die orangene Linie zeigt die Umsätze der alten Kunden. Es ist ersichtlich, dass die Umsätze der jeweiligen Kundengruppen in den letzten Monaten konstant geblieben sind. Dies bestätigt die Hypothese, dass der Umsatzrückgang auf den Anstieg des Anteils an jungen Kunden zurückzuführen ist und nicht darauf, dass sich die Umsätze der jeweiligen Kundengruppen verändert haben.\n\nsns.catplot(x=\"Monat\", y=\"Preis\", \n            data=df,  kind=\"point\", \n            palette={\"jung\": \"blue\", \"alt\": \"orange\"}, hue=\"Kundengruppe\");\n\n\n\n\n\n\n\n\n\n\nZwischenfazit\nLassen Sie uns - bevor wir mit unser Analyse fortfahren - ein kurzes Zwischenfazit ziehen und unsere bisherigen Erkenntnisse zusammenfassen.\n\nwir haben im ersten Schritt unsere Problem definiert (“Was ist der Grund für den Rückgang des Umsatzes?”)\nnachfolgend haben wir die Daten geladen und bereinigt. Die Bereinigung der Daten war nicht besonders umfassend, da der Datensatz bereits sehr sauber war. In der Praxis ist die Bereinigung der Daten jedoch ein sehr wichtiger Schritt, da die Daten in der Regel sehr unvollständig und fehlerhaft sind.\nwir haben dann im nächsten Schritt die Problembeschreibung bestätigt und festgestellt, dass es laut Daten tatsächlich einen Umsatzrückgang gab.\nanschließend haben wir vier Hypothesen für den Umsatzrückgang aufgestellt und diese mit Hilfe der Daten überprüft. Die Hypothesen waren:\n\ndie Preise sind gesunken\ndie Nachfrage ist gesunken\ndas Zahlungsverhalten der Kunden hat sich verändert\ndie Kundenstruktur hat sich verändert\n\nwir haben die Hypothesen mit Hilfe von sinnvollen Transformationen (Gruppierungen und Aggregationen) und Visualisierungen überprüft.\nwir konnten feststellen, dass die ersten beiden Hypothesen zutrafen. Im Zeitverlauf sind die Preise und Mengen tatsächlich gesunken. Jedoch hat uns diese Feststellung keinen Aufschluss darüber gegeben, was der Grund für die Preis- und Mengenänderungen ist.\nwir konnten keinen Zusammenhang zwischen der Zahlungsart und der Umsatzentwicklung entdecken.\nwir haben dann jedoch festgestellt, dass die Kundenstruktur sich verändert hat. Wir haben mehr Kunden aus der Altersgruppe “unter 40” gewonnen, die weniger oder günstiger kaufen. Dies könnte der Grund für den Umsatzrückgang sein.\n\n\nFazit: Der Umsatzrückgang ist auf die Veränderung der Kundenstruktur zurückzuführen. Wir haben mehr Kunden aus der Altersgruppe “unter 40” gewonnen, die weniger oder günstiger kaufen.\n\n\n\nErkenntnisse gewinnen: Modellieren\nUnsere bisherige Analyse basiert auf rein deskriptiven Statistiken. Um unsere Hypothesen zu überprüfen, haben wir unseren Datensatz im Grunde nur beschrieben. Wir haben Daten gruppiert und aggregiert und teilweise für verschiedene Dimensionen (z.B. nach Monat und Zahlungsmethode) visualisiert. Wir konnten dadurch bereits wichtige Erkenntnisse gewinnen. Im betriebswirtschaftlichen Kontext in der Praxis sind diese Analysen meist auch bereits ausreichend. Jedoch ist es oft auch hilfreich und sinnvoll die Daten mit Hilfe von Modellen zu analysieren. Das Modellieren von Daten ist ein wesentlicher Bestandteil von Business Analytics und hat drei wesentliche Vorteile, den es ermöglicht uns:\n\ngewonnene Erkenntnisse zu validieren\nZusammenhänge und Ursachen für Effekte (besser) zu verstehen\nDaten in die Zukunft zu prognostizieren\n\nWir werden an dieser Stelle noch nicht im Detail auf die Modellierung eingehen, sondern stattdessen nur ein kurzes Beispiel geben, um Ihnen einen Eindruck davon zu geben, wie wir mit Hilfe von Modellen Erkenntnisse gewinnen können.\nIn unserem Zwischenfazit haben wir zusammegefasst, dass der Umsatzrückgang auf die Veränderung der Kundenstruktur zurückzuführen ist. Der Anteil an jüngeren Kunden hat sich vergrößert und diese Kunden kaufen günstiger und weniger.\nLassen Sie uns diese Erkenntnis noch einmal alternativ graphisch darstellen. Wir gruppieren unsere Daten dafür pro Tag (d.h. wir aggregieren einzelne Transaktionen pro Tag) und schauen uns für jeden Tag den durschnittlichen Umsatz sowie das durchschnittliche Kundenalter an.5. Anschließend stellen wir die Daten in einem Streudiagramm dar. Wir sehen, dass es einen Zusammenhang zwischen Umsatz und Alter gibt. Je älter die Kunden sind, desto höher ist der Umsatz. Der Zusammenhang wird durch die Linie, die wir dem Graphen hinzugefügt haben visualisiert.5 Hinweis: wir nehmen hier den Median, um einzelnen Ausreißern kein großes Gewicht zu geben. Die Darstellung sähe jedoch sehr ähnlich aus, wenn wir das arithmethische Mittel nähmen.\n\numsatz_by_alter = df.groupby(\"Datum\").agg({\"Umsatz\": \"median\", \n                                           \"Alter\": \"median\"})\nsns.regplot(x=\"Alter\", y=\"Umsatz\", data=umsatz_by_alter, ci=True, \n            line_kws={\"color\": \"red\"}, scatter_kws={\"alpha\": 0.5});\n\n\n\n\nDie Linie ist ein Modell, das die Daten beschreibt. Im vorliegenden Fall handelt es sich um ein lineares Model bzw. eine lineare Funktion. Wir können diese Funktion auch als eine mathematische Gleichung beschreiben. In unserem Fall lautet die Gleichung:\n\\[\nUmsatz = \\alpha + \\beta \\cdot Alter = -28.8 + 3.4 \\cdot Alter\n\\]\nIhnen sollte das Modell bekannt vorkommen, da es sich um ein lineares Regressionsmodell handelt, welches typischerweise zu Beginn des Studiums in einer einführenden Statistikvorlesung behandelt wird. Das Ergebnis, d.h. die Werte für die sogenannten Regressionsparamter \\(\\alpha\\) und \\(\\beta\\) haben wir ebenfalls mit Python berechnet. Die Details sind an dieser Stelle (noch) nicht so wichtig, können jedoch im untenstehenden “Tab-Panel” eingesehen werden.\n\nRegression: CodeRegression: Output\n\n\n\nimport statsmodels.formula.api as smf\nmodel = smf.ols(formula=\"Umsatz ~ Alter\", data=umsatz_by_alter)\nresults = model.fit()\nresults.summary()\n\n\n\n\n\n\n\nOLS Regression Results\n\n  Dep. Variable:         Umsatz        R-squared:             0.725 \n\n\n  Model:                   OLS         Adj. R-squared:        0.724 \n\n\n  Method:             Least Squares    F-statistic:           956.1 \n\n\n  Date:             Fri, 24 Mar 2023   Prob (F-statistic): 9.67e-104\n\n\n  Time:                 09:09:04       Log-Likelihood:      -1612.6 \n\n\n  No. Observations:         365        AIC:                   3229. \n\n\n  Df Residuals:             363        BIC:                   3237. \n\n\n  Df Model:                   1                                     \n\n\n  Covariance Type:      nonrobust                                   \n\n\n\n\n               coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  Intercept   -28.8068     4.931    -5.841  0.000   -38.505   -19.109\n\n\n  Alter         3.3864     0.110    30.920  0.000     3.171     3.602\n\n\n\n\n  Omnibus:        0.198   Durbin-Watson:         2.043\n\n\n  Prob(Omnibus):  0.906   Jarque-Bera (JB):      0.083\n\n\n  Skew:          -0.024   Prob(JB):              0.959\n\n\n  Kurtosis:       3.056   Cond. No.               211.\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\nDas Modell ist nur ein Beispiel für ein Modell, das wir an dieser Stelle verwenden können oder sollten.\n\nes kann helfen unsere Erkenntnisse zu validieren: wir könnten dem Modell weitere Variablen hinzufügen und schauen, ob der Zusammenhang zwischen Umsatz und Alter weiterhin Bestand hat und ob dieser robust ist\nes kann helfen Zusammenhänge zu verstehen: wir können das Modell nutzen um zu verstehen, wie sich der Umsatz verändert, wenn wir das Alter der Kunden verändern. Wir können unser Model z.B. wie folgt interpretieren: wenn Kunden im Durchschnitt 1 Jahr älter sind, dann steigt der Umsatz im Durchschnitt um 3,4€. Wir können damit den Effekt der Veränderung der Kundenstruktur auf den Umsatz quantifizieren\nes kann helfen Daten in die Zukunft zu prognostizieren: wir können das Modell nutzen um zu schätzen, wie sich der Umsatz in Zukunft entwickeln wird. Wir können z.B. schätzen, dass der Umsatz nächstes Jahr um x € steigen wird, wenn wir y Jahre ältere Kunden anwerben.6\n\n\n\n6 Hinweis: dies beinhaltet aber viele Annahmen - z.B. dass die Zusammenhänge über die Zeit konstant bleiben oder dass nicht doch andere (versteckte) Faktoren eine Rolle spielen. Wir werden auf diese Aspekte an geeigneter Stelle noch im Detail eingehen."
  },
  {
    "objectID": "Chapters/04_Chapter/Deskriptive_Analyse.html",
    "href": "Chapters/04_Chapter/Deskriptive_Analyse.html",
    "title": "DESKRIPTIVE ANALYSE",
    "section": "",
    "text": "Die deskriptive Analyse ist eine wichtige Komponente der Datenanalyse, die einen Einblick in die Eigenschaften und Merkmale eines bestimmten Datensatzes ermöglicht. Sie ist gleichzeitig auch die Grundlage für viele weitere Analysemethoden. Davon ausgehend, dass Sie bereits durch ihre statistische Vorausbildung mit den grundlegenden Konzepten der deskriptiven Statistik vertraut sind, werden wir uns in diesem Kapitel auf die Anwendung dieser Konzepte auf die Datenanalyse konzentrieren. Wir werden sehen, dass es bei der deskriptiven Analyse im Grunde nur um die Zusammenfassung, Organisation und Darstellung von Daten geht. Auch werden wir feststellen, dass diese Form der Analyse methodisch oft einfach oder trivial erscheint, jedoch in der Praxis oft bereits entscheidende Erkenntnisse über die Daten liefern kann.\nIn diesem Kapitel werden wir uns mit den ersten vier Schritten des Analyseprozesses beschäftigen. Diese sind in unser bekannten Darstellung blau hervorgehoben.\n\n\n\nFokus der deskriptiven Analyse"
  },
  {
    "objectID": "Chapters/04_Chapter/Deskriptive_Problemstellung.html#einleitung",
    "href": "Chapters/04_Chapter/Deskriptive_Problemstellung.html#einleitung",
    "title": "Ausgangslage und Problemstellung",
    "section": "Einleitung",
    "text": "Einleitung\nDie Bau und Werken GmbH wurde in Recklinghausen gegründet und hat bis heute ihren Sitz in der Stadt. Seit ihrer Gründung im Jahre 1967 hat das Unternehmen eine bemerkenswerte Entwicklung durchgemacht. Anfangs konzentrierte sich das Unternehmen hauptsächlich auf kleinere kommunale Bauprojekte. Mit der Zeit jedoch wuchs die Firma und konnte sich auf größere Projekte spezialisieren.\nIm Laufe der Jahre hat die Bau und Werken GmbH ein starkes Netzwerk an Geschäftspartnern und Lieferanten aufgebaut, das es ihr ermöglichte, auch anspruchsvollere Projekte auszuführen. Mittlerweile bietet das Unternehmen eine breite Palette von Projekten an: von Baum- bis Tiefbauarbeiten bei städtischen U-Bahnen ist alles dabei. Durch ihren Fokus auf Qualität und Kundenzufriedenheit hat sich das Unternehmen einen hervorragenden Ruf in der Branche erworben.\nHeute ist die Bau und Werken GmbH deshalb ein wichtiger Akteur in der kommunalen Bau- und Instandhaltungsbranche und hat sich als zuverlässiger Partner für viele Kommunen und Städte etabliert. Trotz ihres Erfolgs blickt das Unternehmen stets nach vorn und sucht ständig nach Wegen, um seine Geschäftspraktiken zu verbessern und den Kunden noch bessere Dienstleistungen anzubieten.\nWie die gesamte Branche hat auch die Bau und Werken GmbH immer wieder mit Projekten zu kämpfen, die länger dauern als geplant und mehr kosten als budgetiert. Dies ist ein großes Hindernis für die Profitabilität des Unternehmens. Der Chef des Unternehmens, Maximilian Müller, ist sich dessen bewusst und weiß, dass hier sein Unternehmen hier ein großes Problem hat. Er hat es sich deshalb zur Aufgabe gemacht, dieses Problem in den Griff zu bekommen.\nMaximilian weiß, dass längere Bauverzögerungen zu Reduktion der Profitabilität führen, da die Auftraggeber typischerweise “on time”-Boni zahlen. Die Nichteinhaltung der budgetierten Kosten sind außerdem ein großes Problem. Natürlich plant das Unternehmen Kostenerhöhungen prinzipiell von vorne herein ein und wirtschaftet deshalb bei allen Projekten profitabel. Dennoch: die Marge wäre deutlich höher, wenn die Kosten von Anfang an realistischer geplant würden. Um das Problem der Verzögerungen und der Kostenüberschreitungen zu lösen, will er die Projekte der Vergangenheit genau untersuchen lassen, um zu analysieren, ob es bestimmte Muster gibt, die sich wiederholen bzw. ob es bestimmte Faktoren gibt, die die Verzögerungen und Kostenüberschreitungen verursachen.\nDiese Herausforderung ist von entscheidender Bedeutung für die Zukunft des Unternehmens und Maximilian ist entschlossen, das Problem zu lösen, um die Bau und Werken GmbH noch erfolgreicher und profitabler zu machen."
  },
  {
    "objectID": "Chapters/04_Chapter/Deskriptive_Problemstellung.html#datensatz",
    "href": "Chapters/04_Chapter/Deskriptive_Problemstellung.html#datensatz",
    "title": "Ausgangslage und Problemstellung",
    "section": "Datensatz",
    "text": "Datensatz\nDer Datensatz beinhaltet Informationen zu den Projekten, die das Unternehmen in den letzten Jahren durchgeführt hat. Die Daten wurden von einem Mitarbeiter der Bau und Werken GmbH in einer Excel-Tabelle erfasst und anschließend in das CSV-Format exportiert. Der Datensatz beinhaltet folgende Informationen:\n\nProject_ID: eindeutige Identifikationsnummer des Projekts\nName Projekt: Art des Projektes // Ort des Projektes (z.B. “Stadtpark // Eberhardtallee”)\nprojekt_Beginn: Baubeginn bzw. Beginn der Instandhaltungsarbeiten\nPlan Bau fertig: geplantes Bauende bzw. Instandhaltungsende\nFertig_IST: tatsächliches Bauende bzw. Instandhaltungsende\nKosten Plan: budgetierte Gesamtkosten des Projektes\nIst_Kosten: tatsächliche Gesamtkosten des Projektes\nProject_team: internes Team, welches das Projekt bearbeitet hat"
  },
  {
    "objectID": "Chapters/04_Chapter/Einlesen_Aufbereiten.html#einlesen-und-ersten-überblick-verschaffen",
    "href": "Chapters/04_Chapter/Einlesen_Aufbereiten.html#einlesen-und-ersten-überblick-verschaffen",
    "title": "Einlesen und Aufbereiten",
    "section": "Einlesen und ersten Überblick verschaffen",
    "text": "Einlesen und ersten Überblick verschaffen\nBeginnen wir mit der Analyse, in dem wir die Daten zunächst einlesen und aufbereiten. Dazu verwenden wir read_csv() aus dem Paket pandas. Bei read_csv() handelt es sich um eine Funktion1, der verschiedene Parameter übergeben werden können.1 genauer gesagt, um eine Methode eines Objektes.\n\n\n\n\n\n\nWas sind Funktionen?\n\n\n\n\n\nEine Funktion ist ein Teil eines Programms, der eine bestimmte Aufgabe erfüllt. Wir können Funktionen einsetzen, um Teile des Codes wiederverwendbar zu machen oder um bereits geschriebenen Code von anderen wiederzuverwenden. Das Konstrukt ist sehr mächtig und wir werden sowohl bereits implementierte Funktionen nutzen (wie alle Funktionen, die pandas bereitstellt), als auch eigene Funktionen schreiben lernen.\nLassen Sie uns eine Analogie aus Excel nutzen. Stellen Sie sich vor, wir haben eine Spalte mit vielen Werten und wir wollen wissen, um wieviele Werte es sich handelt, d.h. wir wollen die Anzahl an Werten bestimmen. Wir könnten diese Information z.B. benötigen, um einen Durchschnitt zu berechnen.\n\n\n\n\n\nNatürlich könnten wir die Anzahl an Werten selber und manuell zählen (es sind 10 Werte). Diese Lösung ist aber wenig sinnvoll, da wir so einen manuellen Schritt in unser “Programm” einbauen. Besser wäre es, wenn wir die Anzahl an Werten automatisiert bestimmen. Wir könnten uns über diese eigentlich triviale Aufgabe nun Gedanken machen. Jedoch müssen wir dies nicht, da es für diese spezielle Frage bereits eine Lösung in Excel gibt. Wir können die Funktion ANZAHL() nutzen, die Excel bzw. Microsoft bereits für den Anwender zur Verfügung gestellt hat. Die Funktion berechnet, wie viele Zellen in einem Bereich Zahlen enthalten.\n\n\n\n\n\nEin Großteil der Funktionalität von Excel geht auf die breite Palette an bereits verfügbaren Funktionen zurück. Gleiches gilt für Python. Wir können uns die Arbeit von anderen Anwendern zu Nutze machen und die bereits implementierten Funktionen nutzen.\nEine Funktion in Python ist prinzipiell sehr ähnlich zu einer Funktion in Excel. Eine Funktion hat einen Namen, der sie eindeutig identifiziert. Außerdem hat sie Parameter, die sie benötigt, um ihre Aufgabe zu erfüllen. Die Funktion ANZAHL() in Excel hat einen Parameter, der den Bereich angibt, in dem die Anzahl an Zahlen bestimmt werden soll. Die Funktion read_csv() in Python hat einen Parameter, der den Pfad zur Datei angibt, die eingelesen werden soll. Sowohl in Excel, als auch in Python gibt es Funktionen mit optionalen Parametern, die nicht zwingend angegeben werden müssen. Die Funktion Anzahl() in Excel hat einen optionalen Parameter, der angibt, ob auch leere Zellen gezählt werden sollen. Die Funktion read_csv() in Python hat eine Vielzahl von optionalen Parameter, die der Anwender beim Einlesen von Daten ggf. nutzen möchte oder muss (z.B. welche Spalten wir einlesen möchten, welches Datenformat wir verwenden, welches Dezimalkomma wir verwenden, etc.).\n\n\n\nDer wichtigste (und einzige zwingend notwendige) Parameter ist der Pfad zur Datei, die eingelesen werden soll. In unserem Fall ist der Pfad ../../_data/Construction.csv. Der Pfad ist relativ zum aktuellen Arbeitsverzeichnis, d.h. relativ zum Ort, in der sich das Notebook, mit dem wir arbeiten befindet.2. Die Funktion gibt ein sogenanntes DataFrame zurück, das wir in der Variable df speichern. Ein DataFrame ist eine Datenstruktur, die in der Regel Tabellen repräsentiert. In unserem Fall enthält das DataFrame die Daten aus der CSV-Datei. Prinzipiell können wir nahezu jedes gängige Format einlesen (z.B. txt, html, csv, excel, parquet etc.). Eine vollständige Liste der unterstützten Formate finden Sie in der Dokumentation von Pandas.32 Sofern sich die Datei in einer Cloud befindet, kann auch der Link zur Datei angegeben werden, d.h. die Datei muss nicht lokal abgespeichert sein3 siehe hier\n\nimport pandas as pd\n\ndf = pd.read_csv('../../_data/Construction.csv')\n\nDie Variable df enthält nun die Daten aus der CSV-Datei. Wenn das Einlesen der Daten erfolgreich war - d.h. wir keine Fehlermeldung erhalten haben -, können wir uns einen ersten Überblick verschaffen und uns den Inhalt der Variable df anschauen. Dazu verwenden wir die Funktion head().\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      Project_ID\n      Name Projekt\n      projekt_Beginn\n      Plan Bau fertig\n      Fertig_IST\n      Kosten Plan\n      Ist_Kosten\n      Project_team\n    \n  \n  \n    \n      0\n      HN-399443\n      Straßenbau // Jennifer-Buchholz-Ring\n      2014-09-01\n      2014-10-03\n      2014-10-02\n      219817.40\n      246192.34\n      Team 3\n    \n    \n      1\n      UD-626094\n      Elektroarbeiten // Langernstraße\n      2021-06-12\n      2021-08-16\n      2021-09-18\n      105683.14\n      144657.38\n      Team 3\n    \n    \n      2\n      IO-468103\n      Spielplatz // Dussen vanweg\n      2016-05-20\n      2016-06-29\n      2016-06-27\n      129851.26\n      136753.06\n      Team 1\n    \n    \n      3\n      OG-758899\n      Stadtpark // Lübsstr.\n      2014-11-11\n      2014-12-30\n      2015-01-28\n      181236.83\n      273996.91\n      Team 2\n    \n    \n      4\n      CZ-107835\n      Elektroarbeiten // Försterweg\n      2017-07-25\n      2017-10-08\n      2017-10-08\n      75205.92\n      77519.27\n      Team 4\n    \n  \n\n\n\n\nDie Funktion ist sehr hilfreich, um zu überprüfen, ob die Daten (zumindest augenscheinlich) korrekt eingelesen wurden. Außderm können wir so den Aufbau des Datensatzes erkennen.\n\n\n\n\n\n\nhead() und tail()\n\n\n\n\n\nDie Funktion head() gibt die ersten Zeilen eines DataFrames aus. Mit der Funktion tail() können wir die letzten Zeilen ausgeben lassen. Ohne weitere Angabe von Parametern geben head() und tail() jeweils fünf Zeilen aus. Wir können beide Funktionen auch mit einem Argument aufrufen, um die Anzahl der Zeilen anzupassen. Beispiel: df.head(10) gibt die ersten zehn Zeilen aus. Der Aufruf von head() ist also äquivalent zum Aufruf head(5).\n\n\n\nNachdem wir uns die Daten angeschaut haben, sollten wir einen weiteren, tieferen Blick auf die Daten werfen. Mit der Funktion info() können wir uns die Spaltennamen und die Datentypen der Spalten ausgeben lassen. Dies ist ein guter Einstieg, um zu verstehen, welche Daten wir vor uns haben.\n\ndf.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 10019 entries, 0 to 10018\nData columns (total 8 columns):\n #   Column           Non-Null Count  Dtype  \n---  ------           --------------  -----  \n 0   Project_ID       10019 non-null  object \n 1   Name Projekt     10019 non-null  object \n 2   projekt_Beginn   10019 non-null  object \n 3   Plan Bau fertig  10012 non-null  object \n 4   Fertig_IST       10019 non-null  object \n 5   Kosten Plan      10005 non-null  float64\n 6   Ist_Kosten       10019 non-null  float64\n 7   Project_team     10019 non-null  object \ndtypes: float64(2), object(6)\nmemory usage: 626.3+ KB\n\n\nDie Funktion info() gibt an, wieviele Zeilen (RangeIndex) und Spalten Data columns) der Datensatz hat und zusätzlich noch folgende Informationen je Spalte an:\n\n# Position der Spalte (beginnend bei 0)\nColumn Name der Spalte\nNon-Null Count Anzahl der nicht leeren Werte je Spalte\nDtype Datentyp der Spalte\n\nWir können mit der Funkton also einige wichtige Dinge im Rahmen der Datenaufbereitung erkennen. In unserem Fall sind z.B. nicht alle Spalten mit Werten gefüllt. Wir werden im nächsten Abschnitt detaillierter darauf eingehen. Die Spalten haben außerdem einen von zwei Datentypen: object und float. Der Datentyp float repräsentiert Gleitkommazahlen (z.B. \\(3.23\\)). Der Datentyp object repräsentiert Zeichenketten (d.h. Python interpretiert die Daten als Text). Da wir beim Einlesen der Daten keine Angabe über die Datentypen gemacht haben, hat Pandas die Datentypen automatisch ermittelt. In der Regel ist dies auch ein guter erster Ansatz. Wir sollten jedoch in einem nächsten Schritt die Datentypen überprüfen und ggf. anpassen. Denn je besser der Datentyp zum Inhalt der Spalte passt, desto besser können wir mit den Daten arbeiten. Wenn wir also z.B. Spalten mit ausschließlich Zahlen haben, sollten diese auch nicht als Datentyp object gespeichert werden, da wir dann mit dieser Spalte z.B. nicht rechnen können. Außerdem können wir bereits jetzt erkennen, dass die Spaltenbezeichungen inkonsistent und teilweise unglücklich gewählt sind. Wir werden im nächsten Abschnitt detaillierter darauf eingehen.\n\n\n\n\n\n\nPosition 0 in Python\n\n\n\n\n\nDie erste Position in Python ist immer 0. Das ist ein Konzept, das wir uns merken sollten. Wir werden es in den kommenden Kapiteln immer wieder verwenden. Die erste Zeile eines DataFrames hat die Position 0. Die erste Spalte hat die Position 0.\nDies ist zunächst etwas verwirrend und ungewohnt. Jedoch ist Python hier sehr konsistent, so dass wir uns schnell daran gewöhnen werden. Grundsätzlich gilt: die erste Position in Python hat immer den Index (d.h Zeile oder Spalte) 0."
  },
  {
    "objectID": "Chapters/04_Chapter/Einlesen_Aufbereiten.html#aufbereitung-der-daten",
    "href": "Chapters/04_Chapter/Einlesen_Aufbereiten.html#aufbereitung-der-daten",
    "title": "Einlesen und Aufbereiten",
    "section": "Aufbereitung der Daten",
    "text": "Aufbereitung der Daten\nBeginnen wir mit der Aufbereitung der Daten. Natürlich gibt es nicht den richtigen Weg, um Daten aufzubereiten. Jeder Datensatz ist unterschiedlich und nicht jede Analyse hat dieselben Anforderungen. Dennoch gibt es typische Aufgaben, die wir bei der Aufbereitung von Daten immer wieder vorfinden. Wir werden uns im Folgenden einige dieser Aufgaben ansehen und die entsprechenden Lösungen diskutieren.\n\nVariablennamen anpassen\nEs ist nicht zwingend notwendig Variablen neu zu benennen. Im vorliegenden Datensatz hat jede Variable zumindest eine den Inhalt beschreibende Bezeichnung. Jedoch ist die Bezeichnung von Spalten oft nicht intuitiv, inkonsistent oder unklar. Daher ist es sinnvoll, die Spalten so zu bennen, dass wir im Rahmen der Analyse (i) schnell erkennen, was in der Spalte steht und (ii) die Auswahl der Spalten intuitiv ist. Die Umbenennung von Variablen hat dabei natürlich immer eine subjektive Komponente, da jeder andere Vorstellungen hat, wie die Spalten benannt sein sollten.\nIn unserem Datensatz sind die Spaltennamen jedoch insbesondere auch nicht einheitlich benannt. So beginnen einige Spalten mit einem Großbuchstaben, andere mit einem Kleinbuchstaben. Bei einigen Spalten ist der englische Begriff project, bei anderen der deutsche Begriff projekt verwendet. Einige Spalten beinhalten Leerzeichen, andere sind mit _ verbunden. Dies sind alles Dinge, die die weitere Analyse ein Stück weit erschweren, da wir Spalten nicht intuitiv auswählen können.\nLassen Sie uns die Spalten deshalb vereinheitlichen, in dem wir:\n\nalle Spalten in Kleinbuchstaben umwandeln\nalle Leerzeichen durch _ ersetzen\nalle Begriffe project durch projekt ersetzen\nwo sinnvoll kürzere Begriffe verwenden\nBegriffe wie Plan und Ist einheitlich verwenden\n\nWir können die Spalten mit der Funktion rename() umbenennen. Die Funktion rename() erwartet als Parameter ein sog. Dictionary, in dem wir die alten Spaltennamen als Schlüssel und die neuen Spaltennamen als Werte angeben. Unser angepasster DataFrame sieht dann wie folgt aus:\n\ndf = df.rename(columns={'Project_ID': 'id', # Spalte \"Projekt_ID\" wird umbenannt in \"id\"\n                        'Name Projekt': 'name',\n                        'projekt_Beginn': 'beginn',\n                        'Plan Bau fertig': 'ende_plan',\n                        'Fertig_IST': 'ende_ist',\n                        'Kosten Plan': 'kosten_plan',\n                        'Ist_Kosten': 'kosten_ist',\n                        'Project_team': 'team'})\ndf.head()\n\n\n\n\n\n  \n    \n      \n      id\n      name\n      beginn\n      ende_plan\n      ende_ist\n      kosten_plan\n      kosten_ist\n      team\n    \n  \n  \n    \n      0\n      HN-399443\n      Straßenbau // Jennifer-Buchholz-Ring\n      2014-09-01\n      2014-10-03\n      2014-10-02\n      219817.40\n      246192.34\n      Team 3\n    \n    \n      1\n      UD-626094\n      Elektroarbeiten // Langernstraße\n      2021-06-12\n      2021-08-16\n      2021-09-18\n      105683.14\n      144657.38\n      Team 3\n    \n    \n      2\n      IO-468103\n      Spielplatz // Dussen vanweg\n      2016-05-20\n      2016-06-29\n      2016-06-27\n      129851.26\n      136753.06\n      Team 1\n    \n    \n      3\n      OG-758899\n      Stadtpark // Lübsstr.\n      2014-11-11\n      2014-12-30\n      2015-01-28\n      181236.83\n      273996.91\n      Team 2\n    \n    \n      4\n      CZ-107835\n      Elektroarbeiten // Försterweg\n      2017-07-25\n      2017-10-08\n      2017-10-08\n      75205.92\n      77519.27\n      Team 4\n    \n  \n\n\n\n\n\n\nDatentypen anpassen\nEin wichtiger Schritt bei der Aufbereitung von Daten ist die Überprüfung der Datentypen. Wir haben bereits gesehen, dass unser Datensatz nach dem Einlesen zwei Datentypen enthält: object und float64. Lassen Sie uns deshalb überlegen, ob diese Datentypen geeignet für unsere Analyse sind. Denn: die Datentypen beeinflussen die Art und Weise, wie wir mit den Daten arbeiten können.\n\nid: die Spalte beinhaltet Buchstaben, Zeichen und Zahlen (z.B. P-62602). Der Datentyp object ist also passend.\nname: die Spalte beinhaltet Text. Der Datentyp object ist also passend.\nbeginn: die Spalte beinhaltet Datumswerte. Der Datentyp object ist nicht passend. Wir sollten den Datentyp in ein Datumsformat umwandeln.\nende_plan: die Spalte beinhaltet Datumswerte. Der Datentyp object ist nicht passend. Wir sollten den Datentyp in ein Datumsformat umwandeln.\nende_ist: die Spalte beinhaltet Datumswerte. Der Datentyp object ist nicht passend. Wir sollten den Datentyp in ein Datumsformat umwandeln.\nkosten_plan: die Spalte beinhaltet Zahlen. Der Datentyp float64 erscheint passend.\nkosten_ist: die Spalte beinhaltet Zahlen. Der Datentyp float64 erscheint passend.\nteam: die Spalte beinhaltet Text. Der Datentyp object ist also passend.\n\nWir müssen also lediglich die Spalten mit Datumsinformationen in ein geeignetes Format umwandeln. Die Anpassung der Datumsformate können wir mit der Pandas-Funktion to_datetime() durchführen. Wir müssen dabei nur die Spalten angeben, die wir anpassen möchten. Die Funktion to_datetime() wandelt die Spalten dann in ein Datumsformate um. Wir überschreiben die Spalten einfach mit den neuen Werten.\n\ndf['beginn'] = pd.to_datetime(df['beginn'])\ndf['ende_plan'] = pd.to_datetime(df['ende_plan'])\ndf['ende_ist'] = pd.to_datetime(df['ende_ist'])\ndf.head()\n\n\n\n\n\n  \n    \n      \n      id\n      name\n      beginn\n      ende_plan\n      ende_ist\n      kosten_plan\n      kosten_ist\n      team\n    \n  \n  \n    \n      0\n      HN-399443\n      Straßenbau // Jennifer-Buchholz-Ring\n      2014-09-01\n      2014-10-03\n      2014-10-02\n      219817.40\n      246192.34\n      Team 3\n    \n    \n      1\n      UD-626094\n      Elektroarbeiten // Langernstraße\n      2021-06-12\n      2021-08-16\n      2021-09-18\n      105683.14\n      144657.38\n      Team 3\n    \n    \n      2\n      IO-468103\n      Spielplatz // Dussen vanweg\n      2016-05-20\n      2016-06-29\n      2016-06-27\n      129851.26\n      136753.06\n      Team 1\n    \n    \n      3\n      OG-758899\n      Stadtpark // Lübsstr.\n      2014-11-11\n      2014-12-30\n      2015-01-28\n      181236.83\n      273996.91\n      Team 2\n    \n    \n      4\n      CZ-107835\n      Elektroarbeiten // Försterweg\n      2017-07-25\n      2017-10-08\n      2017-10-08\n      75205.92\n      77519.27\n      Team 4\n    \n  \n\n\n\n\nDas Ergebnis ist für uns nicht ersichtlich. Wir können uns die Datentypen der einzelnen Spalten jedoch mit dem Attribut dtypes (oder aber mit .info()) anzeigen lassen und überprüfen, ob die Datentypen nun passend sind.\n\ndf.dtypes\n\nid                     object\nname                   object\nbeginn         datetime64[ns]\nende_plan      datetime64[ns]\nende_ist       datetime64[ns]\nkosten_plan           float64\nkosten_ist            float64\nteam                   object\ndtype: object\n\n\n\n\n\n\n\n\nWarum Datumsformate?\n\n\n\n\n\nDie Konvertierung der Daten in ein Datumsformat ist für uns in unserem Falle nicht ersichtlich. Schauen wir uns die Daten an, sehen diese für uns aus wie vor der Konvertierung. Jedoch hat Pandas die Daten intern in ein Datumsformat umgewandelt, was den Vorteil hat, dass wir auf verschiedene Funktionen zurückgreifen können, die nur mit Daten vom Datentyp datetime funktionieren. So können wir z.B. mit der Funktion dt.weekday den Wochentag auslesen.\n\ndf['beginn'].dt.weekday\n\n0        0\n1        5\n2        4\n3        1\n4        1\n        ..\n10014    2\n10015    6\n10016    5\n10017    3\n10018    1\nName: beginn, Length: 10019, dtype: int64\n\n\nDie Umwandlung in ein Datumsformat ist also nicht nur für die Darstellung der Daten sinnvoll, sondern auch für die weitere Analyse - zumindest dann, wenn wir mit Datumsformaten arbeiten möchten.\nWeitere Informationen zu den datetime-Funktionen finden Sie in der Dokumentation.\n\n\n\n\n\nDaten bereinigen\n\nFehlende Werte\nWir haben bereits gesehen, dass nicht alle Spalten mit Werten gefüllt sind. Jedoch wissen wir nicht, wo sich diese fehlenden Werte befinden. Diese Information wird jedoch benötigt, um zu entscheiden, wie wir mit den fehlenden Werten umgehen. Wollen wir Beobachtungen mit fehlenden Werte entfernen oder wollen wir z.B. die fehlenden Werte mit einem anderen Wert ersetzen? Sind die fehlenden Werte für unsere Analyse relevant?\nMit der Funktion isna() können wir herausfinden, wo sich fehlende Werte befinden. Schauen wir uns die Funktion isna() zunächst einmal an.\n\ndf.isna()\n\n\n\n\n\n  \n    \n      \n      id\n      name\n      beginn\n      ende_plan\n      ende_ist\n      kosten_plan\n      kosten_ist\n      team\n    \n  \n  \n    \n      0\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n    \n    \n      1\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n    \n    \n      2\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n    \n    \n      3\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n    \n    \n      4\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      10014\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n    \n    \n      10015\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n    \n    \n      10016\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n    \n    \n      10017\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n    \n    \n      10018\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n    \n  \n\n10019 rows × 8 columns\n\n\n\nDas Ergebnis der Funktion isna() ist ein DataFrame, der für jede Zelle einen Boolean-Wert zurückgibt. True bedeutet, dass die Zelle einen fehlenden Wert enthält, False bedeutet, dass die Zelle einen Wert enthält. In unserem Fall ist der Datensatz jedoch zu groß, als dass wir die Ausgabe der Funktion isna() komplett betrachten können. Idealerweise möchten wir nur die Zeilen sehen, die fehlende Werte enthalten.\nDies können wir mit der Funktion any() erreichen. Die Funktion any() gibt für jede Spalte oder Zeile einen Boolean-Wert zurück. True bedeutet, dass dort mindestens ein fehlender Wert enthalten ist. Mit dem Parameter axis können wir angeben, ob wir die Funktion any() für jede Spalte oder für jede Zeile ausführen möchten. axis=1 bedeutet, dass wir die Funktion any() für jede Zeile ausführen möchten.\n\ndf.isna().any(axis=1)\n\n0        False\n1        False\n2        False\n3        False\n4        False\n         ...  \n10014    False\n10015    False\n10016    False\n10017    False\n10018    False\nLength: 10019, dtype: bool\n\n\nWir können dieses Ergebnis nun zum Filtern der Zeilen verwenden. Wir erstellen einen Boolean-Index, der die gleiche Länge hat, wie der Datensatz und an den Stellen, an denen Werte fehlen, den Wert True hat4. Diesen Index können wir dann verwenden, um nur die Zeilen auszuwählen, die fehlende Werte enthalten.4 Hinweis: dieser wird oft als mask bezeichnet. Da es sich aber um eine einfache Variable handelt, können wir diese theoretisch benennen, wie wir möchten.\n\n# Index mit True (= Wert fehlt) oder False (= Wert vorhanden)\nmask = df.isna().any(axis=1) \n# Datensatz beinhaltet nur Zeilen mit True \ndf[mask]\n\n\n\n\n\n  \n    \n      \n      id\n      name\n      beginn\n      ende_plan\n      ende_ist\n      kosten_plan\n      kosten_ist\n      team\n    \n  \n  \n    \n      103\n      TH-593312\n      Elektroarbeiten // Börnerplatz\n      2018-05-07\n      2018-07-11\n      2018-07-17\n      NaN\n      69375.23\n      Team 3\n    \n    \n      740\n      NL-673150\n      Baumbestand // Loosplatz\n      2016-12-30\n      NaT\n      2016-12-31\n      16349.93\n      17892.16\n      Team 3\n    \n    \n      896\n      LM-748355\n      Baumbestand // Martha-Bohlander-Ring\n      2019-10-17\n      2019-10-22\n      2019-10-24\n      NaN\n      40637.75\n      Team 4\n    \n    \n      996\n      AY-257302\n      Elektroarbeiten // Eimerstr.\n      2014-02-03\n      2014-02-28\n      2014-03-01\n      NaN\n      112867.21\n      Team 2\n    \n    \n      1059\n      RR-446242\n      Elektroarbeiten // Florentine-Kambs-Allee\n      2013-06-16\n      2013-07-24\n      2013-07-23\n      NaN\n      164730.94\n      Team 1\n    \n    \n      1272\n      RG-867589\n      Stadtpark // Gottlieb-Plath-Platz\n      2015-03-05\n      NaT\n      2015-04-14\n      199994.22\n      233881.99\n      Team 4\n    \n    \n      1357\n      ZL-002818\n      Elektroarbeiten // Margrafweg\n      2015-10-13\n      NaT\n      2016-01-24\n      94817.15\n      132951.83\n      Team 3\n    \n    \n      1406\n      XJ-212266\n      Elektroarbeiten // Xenia-Hermighausen-Weg\n      2020-12-17\n      2021-01-30\n      2021-02-03\n      NaN\n      75133.80\n      Team 4\n    \n    \n      1887\n      HT-062324\n      Ubahn // Norma-Kade-Platz\n      2013-08-26\n      2013-09-30\n      2013-09-11\n      NaN\n      -84308.51\n      Team 4\n    \n    \n      2477\n      TQ-532646\n      Landschaftsbau // Ester-Eberhardt-Platz\n      2015-11-07\n      2015-11-08\n      2015-11-12\n      NaN\n      75117.33\n      Team 2\n    \n    \n      2916\n      CN-702227\n      Spielplatz // Werneckeweg\n      2019-08-12\n      2019-09-06\n      2019-09-26\n      NaN\n      70094.61\n      Team 2\n    \n    \n      3678\n      RY-972392\n      Elektroarbeiten // Textorallee\n      2014-04-05\n      2014-06-24\n      2014-06-14\n      NaN\n      119277.88\n      Team 1\n    \n    \n      4724\n      HV-603533\n      Landschaftsbau // Grein Grothring\n      2018-03-12\n      2018-03-25\n      2018-03-28\n      NaN\n      187268.32\n      Team 4\n    \n    \n      5652\n      ZL-598081\n      Baumbestand // Tintzmannallee\n      2014-12-04\n      2014-12-04\n      2014-12-04\n      NaN\n      27231.15\n      Team 4\n    \n    \n      5948\n      HZ-310515\n      Spielplatz // Mansplatz\n      2021-11-21\n      2022-01-30\n      2022-03-08\n      NaN\n      236494.73\n      Team 2\n    \n    \n      6034\n      VE-356952\n      Landschaftsbau // Samira-Trupp-Straße\n      2013-08-27\n      NaT\n      2013-09-17\n      -31296.91\n      -36533.15\n      Team 2\n    \n    \n      6398\n      CG-482764\n      Landschaftsbau // Süßebierstraße\n      2021-08-31\n      NaT\n      2021-09-12\n      116367.77\n      105993.50\n      Team 1\n    \n    \n      6672\n      HK-793800\n      Baumbestand // Bolanderstr.\n      2014-05-26\n      NaT\n      2014-06-07\n      60701.91\n      68018.53\n      Team 3\n    \n    \n      7894\n      RV-503877\n      Elektroarbeiten // Anselm-Römer-Allee\n      2013-06-20\n      2013-10-11\n      2013-10-12\n      NaN\n      80977.58\n      Team 2\n    \n    \n      8853\n      VY-133979\n      Baumbestand // Ansgar-Lindau-Weg\n      2017-02-17\n      NaT\n      2017-03-12\n      44071.21\n      49196.77\n      Team 1\n    \n    \n      9759\n      LR-884749\n      Stadtpark // Steven-Döhn-Weg\n      2021-11-10\n      2021-12-17\n      2021-12-07\n      NaN\n      172659.20\n      Team 3\n    \n  \n\n\n\n\nDas Ergebnis ist ein Ausschnitt unseres Datensatzes, der nur die Zeilen enthält, die fehlende Werte enthalten. Da wir keine weiteren Informationen zu den fehlenden Daten haben und auch keinen systematischen Fehler entdecken können, der zu den fehlenden Daten führt, werden wir die fehlenden Werte einfach entfernen. Dies können wir mit der Funktion dropna() erreichen.\n\ndf = df.dropna()\ndf.info()\n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 9998 entries, 0 to 10018\nData columns (total 8 columns):\n #   Column       Non-Null Count  Dtype         \n---  ------       --------------  -----         \n 0   id           9998 non-null   object        \n 1   name         9998 non-null   object        \n 2   beginn       9998 non-null   datetime64[ns]\n 3   ende_plan    9998 non-null   datetime64[ns]\n 4   ende_ist     9998 non-null   datetime64[ns]\n 5   kosten_plan  9998 non-null   float64       \n 6   kosten_ist   9998 non-null   float64       \n 7   team         9998 non-null   object        \ndtypes: datetime64[ns](3), float64(2), object(3)\nmemory usage: 703.0+ KB\n\n\nDer angepasste Datensatz enthält nun keine fehlenden Werte mehr, ist jedoch auch um einige Zeilen kleiner geworden. Gemessen an der Größe des Gesamtdatensatzes scheint dies jedoch vernachlässigbar zu sein.\n\n\nDuplikate\nÄhnlich wie bei den fehlenden Werten können wir mit der Funktion duplicated() herausfinden, ob es Duplikate in unserem Datensatz gibt. Mit Duplikaten meinen wir hier, dass es Zeilen gibt, die im Datensatz mehrfach vorkommen. Das Vorgehen zur Identifikation von Duplikaten ist ähnlich wie bei den fehlenden Werten.\n\n\n\n\n\n\nBeispiel: Duplikate\n\n\n\n\n\nDa wir bei Duplikaten - anders als bei fehlenden Werte - eigentlich immer Zeilen und nicht Spalten identifizieren wollen, benötigen wir die Funktion any() nicht. Die Funktion duplicated() kann auch mit dem Parameter subset verwendet werden. Mit diesem Parameter können wir angeben, innerhalb welchen Spalten wir Duplikate suchen möchten. Mit dem Parameter keep können wir angeben, ob wir die erste, die letzte oder alle Duplikate identifizieren möchten.\n\nexample = pd.DataFrame({'Spalte A': [1, 2, 3, 2, 4, 5],\n                   'Spalte B': ['Hund', 'Katze', 'Vogel', 'Katze', 'Fisch', 'Vogel'],\n                   'Spalte C': [19, 43, 1, 43, 127, 21]})\nexample\n\n\n\n\n\n  \n    \n      \n      Spalte A\n      Spalte B\n      Spalte C\n    \n  \n  \n    \n      0\n      1\n      Hund\n      19\n    \n    \n      1\n      2\n      Katze\n      43\n    \n    \n      2\n      3\n      Vogel\n      1\n    \n    \n      3\n      2\n      Katze\n      43\n    \n    \n      4\n      4\n      Fisch\n      127\n    \n    \n      5\n      5\n      Vogel\n      21\n    \n  \n\n\n\n\nWir können die Methode duplicated() nutzen, um Duplikate in Zeilen eines DataFrames zu identifizieren. Sie gibt eine Boolesche Reihe (True, False, True etc.) zurück, die angibt, ob eine Zeile ein Duplikat ist oder nicht. Standardmäßig werden dabei alle Spalten berücksichtigt, jedoch kann auch eine Teilmenge von Spalten angegeben werden, die bei der Suche nach Duplikaten berücksichtigt werden soll.\n\nexample.duplicated()\n\n0    False\n1    False\n2    False\n3     True\n4    False\n5    False\ndtype: bool\n\n\nDie Funktion gibt an, dass die Zeile mit dem Index 3 ein Duplikat ist. Wir können wegen der kleinen Größe des Datensatzes direkt sehen, dass die Zeile eine Duplikat zur Zeile mit dem Zeilindex 1 ist. Bei größeren Datensätzen ist dies oft nicht mehr mit dem bloßen Auge ersichtlich. Wir wollen aber ggf. wissen, welche Zeilen Duplikate sind. Wir können uns beide Zeilen über den Parameter keep=False anzeigen lassen\n\nexample.duplicated(keep=False)\n\n0    False\n1     True\n2    False\n3     True\n4    False\n5    False\ndtype: bool\n\n\nNun können wir - analog zu unserem Vorgehen bei den fehlenden Werte - die Duplikate im Datensatz identifizieren.\n\nmask = example.duplicated(keep=False)\nexample[mask]\n\n\n\n\n\n  \n    \n      \n      Spalte A\n      Spalte B\n      Spalte C\n    \n  \n  \n    \n      1\n      2\n      Katze\n      43\n    \n    \n      3\n      2\n      Katze\n      43\n    \n  \n\n\n\n\nWir müssen nun entscheiden, ob wir diesen Eintrag entfernen wollen oder nicht. In diesem Fall scheint es sinnvoll, den doppelten Eintrag zu entfernen, da wir in der Regel davon ausgehen, dass die Daten nur einmal erfasst wurden. Wir können dies mit der Methode drop_duplicates() tun. Diese entfernt standardmäßig die zweite Zeile einer doppelten Beobachtung. Wir können dies jedoch ebenfalls mit dem Parameter keep ändern (d.h. alle Duplikate entfernen oder nur die erste oder letzte Zeile).\n\nexample.drop_duplicates()\n\n\n\n\n\n  \n    \n      \n      Spalte A\n      Spalte B\n      Spalte C\n    \n  \n  \n    \n      0\n      1\n      Hund\n      19\n    \n    \n      1\n      2\n      Katze\n      43\n    \n    \n      2\n      3\n      Vogel\n      1\n    \n    \n      4\n      4\n      Fisch\n      127\n    \n    \n      5\n      5\n      Vogel\n      21\n    \n  \n\n\n\n\nMit dem Parameter subset können wir angeben, in welchen Spalten wir Duplikate suchen möchten.\n\nexample = pd.DataFrame({'Spalte A': [1, 2, 3, 2, 4, 5],\n                   'Spalte B': ['Hund', 'Dinosaurier', 'Vogel', 'Katze', 'Fisch', 'Vogel'],\n                   'Spalte C': [19, 43, 1, 43, 127, 21]})\nexample\n\n\n\n\n\n  \n    \n      \n      Spalte A\n      Spalte B\n      Spalte C\n    \n  \n  \n    \n      0\n      1\n      Hund\n      19\n    \n    \n      1\n      2\n      Dinosaurier\n      43\n    \n    \n      2\n      3\n      Vogel\n      1\n    \n    \n      3\n      2\n      Katze\n      43\n    \n    \n      4\n      4\n      Fisch\n      127\n    \n    \n      5\n      5\n      Vogel\n      21\n    \n  \n\n\n\n\nIn unserem leicht abgeänderten Beispieldatensatz unterscheiden sich die Zeilen mit den Zeilenindex 1 und 3 nur in der Spalte Spalte B.\nMöchten wir nun Duplikate in den Spalten Spalte A und Spalte C finden und die Inhalte der Spalte B ignorieren, können wir dies wie folgt tun.\n\nexample.drop_duplicates(subset=['Spalte A', 'Spalte C'])\n\n\n\n\n\n  \n    \n      \n      Spalte A\n      Spalte B\n      Spalte C\n    \n  \n  \n    \n      0\n      1\n      Hund\n      19\n    \n    \n      1\n      2\n      Dinosaurier\n      43\n    \n    \n      2\n      3\n      Vogel\n      1\n    \n    \n      4\n      4\n      Fisch\n      127\n    \n    \n      5\n      5\n      Vogel\n      21\n    \n  \n\n\n\n\n\n\n\nLassen Sie uns nun analysieren, ob es überhaupt Duplikate in unserem Datensatz gibt uns dafür alle Spalten berücksichtigen. Mit der Funktion sum() können wir die Anzahl der Duplikate ermitteln.\n\ndf.duplicated().sum()\n\n19\n\n\nDa es Duplikate gibt, können sollten wir uns diese nun genauer anschauen. Dafür nutzen wir zusätzlich zur Funktion duplicated() noch die Funktion sort_values(). Diese Funktion sortiert die Werte in einem DataFrame. Wir sortieren hier die Werte nach der Spalte id und geben dann die ersten 10 Zeilen aus.\n\nmask = df.duplicated(keep=False)\ndf[mask].sort_values('id').head(10)\n\n\n\n\n\n  \n    \n      \n      id\n      name\n      beginn\n      ende_plan\n      ende_ist\n      kosten_plan\n      kosten_ist\n      team\n    \n  \n  \n    \n      1438\n      AS-665106\n      Landschaftsbau // Junckengasse\n      2013-03-30\n      2013-04-20\n      2013-04-20\n      112341.63\n      171362.50\n      Team 4\n    \n    \n      5617\n      AS-665106\n      Landschaftsbau // Junckengasse\n      2013-03-30\n      2013-04-20\n      2013-04-20\n      112341.63\n      171362.50\n      Team 4\n    \n    \n      8235\n      BD-570752\n      Landschaftsbau // Seifertstr.\n      2021-10-27\n      2021-11-12\n      2021-11-14\n      83103.02\n      125081.94\n      Team 4\n    \n    \n      2591\n      BD-570752\n      Landschaftsbau // Seifertstr.\n      2021-10-27\n      2021-11-12\n      2021-11-14\n      83103.02\n      125081.94\n      Team 4\n    \n    \n      1337\n      DH-600607\n      Straßenbau // Tröstplatz\n      2015-01-06\n      2015-01-25\n      2015-01-26\n      189536.10\n      210407.91\n      Team 1\n    \n    \n      6452\n      DH-600607\n      Straßenbau // Tröstplatz\n      2015-01-06\n      2015-01-25\n      2015-01-26\n      189536.10\n      210407.91\n      Team 1\n    \n    \n      8931\n      FN-033852\n      Elektroarbeiten // Gerhard-Sontag-Allee\n      2015-08-17\n      2015-09-17\n      2015-09-19\n      69545.97\n      92323.52\n      Team 2\n    \n    \n      5015\n      FN-033852\n      Elektroarbeiten // Gerhard-Sontag-Allee\n      2015-08-17\n      2015-09-17\n      2015-09-19\n      69545.97\n      92323.52\n      Team 2\n    \n    \n      1586\n      GM-576339\n      Ubahn // Dörrplatz\n      2019-05-06\n      2019-09-08\n      2019-11-04\n      398069.61\n      427038.79\n      Team 3\n    \n    \n      2200\n      GM-576339\n      Ubahn // Dörrplatz\n      2019-05-06\n      2019-09-08\n      2019-11-04\n      398069.61\n      427038.79\n      Team 3\n    \n  \n\n\n\n\nIn unserem Falle erscheint es sinnvoll, die Duplikate zu entfernen. Wir können dies mit der Funktion drop_duplicates() tun. Diese Funktion entfernt standardmäßig die zweite Zeile einer doppelten Beobachtung. Wir können dies jedoch ebenfalls mit dem Parameter keep ändern (d.h. alle Duplikate entfernen oder nur die erste oder letzte Zeile).\n\ndf = df.drop_duplicates()\n\nWir können nun nochmals überprüfen, ob es noch Duplikate gibt und stellen fest, dass unsere Daten nun auf Dopplungen bereinigt sind.\n\ndf.duplicated().any()\n\nFalse\n\n\n\n\nFalsche Werte\nEin wichtiger Schritt bei der Datenbereinigung ist die Überprüfung der Daten auf (offensichtlich) falsche Werte. Nicht immer ist bereits bei der Aufbereitung der Daten erkennbar, ob ein Wert plausibel ist oder nicht. Häufig werden falsche Werte erst bei der Analyse der Daten sichtbar. Jedoch können und sollten einige Plausibilitätsprüfungen bereits bei der Aufbereitung der Daten durchgeführt werden.\nIm vorliegenden Datensatz können wir z.B. folgende Plausibilitätsprüfungen durchführen:\n\nIst der Wert der Spalte id eindeutig?\nSind die Werte für Kosten (kosten_plan und kosten_ist) plausibel, d.h. sind die Kosten positiv (bzw. haben alle das gleiche Vorzeichen)?\nSind die Werte für die Datumsspalten (beginn, ende_plan und ende_ist) plausibel, d.h. (i) liegen die Werte in der Vergangenheit und (ii) ist das Enddatum nach dem Startdatum?\n\nPlausibilitätsprüfung 1: Ist der Wert der Spalte id eindeutig?\nEine id sollte immer eindeutig sein. Im vorliegenden Fall sollte die Anzahl an eindeutigen Werten der Anzahl an Zeilen entsprechen. Ist dies der Fall, so ist die id eindeutig. Falls nicht, wurde die id mehrfach vergeben. Nicht-eindeutige ids müssen nicht problematisch sein, jedoch können dann Fehler bei z.B. der Zusammenführung von Datensätzen oder bei der Aggregation von Daten auftreten. Wir können mit der Funktion unique() überprüfen, ob die id eindeutig ist.\n\nunique_ids = df['id'].unique()\n\nWir können nun überprüfen, ob die Anzahl der eindeutigen Werte der Anzahl der Zeilen entspricht.\n\nlen(unique_ids) == len(df)\n\nTrue\n\n\nDa der Wert True zurückgegeben wird, ist die id eindeutig und wir können uns sicher sein, dass keine Projekt-ID mehrfach vergeben wurde.\nPlausibilitätsprüfung 2: Sind die Werte für Kosten (kosten_plan und kosten_ist) plausibel, d.h. sind die Kosten positiv (bzw. haben alle das gleiche Vorzeichen)?\nLassen Sie uns zunächst die Kostenwerte für die Spalte kosten_plan betrachten und überprüfen, ob es Werte gibt, die negativ sind. Dazu müssen wir überprüfen, ob es Werte gibt, die kleiner als 0 sind. Die können wir mit dem Operator < überprüfen.\n\ndf['kosten_plan'] < 0\n\n0        False\n1        False\n2        False\n3        False\n4        False\n         ...  \n10014    False\n10015    False\n10016    False\n10017    False\n10018    False\nName: kosten_plan, Length: 9979, dtype: bool\n\n\nWir erhalten eine Series mit True und False Werten. True bedeutet, dass die Bedingung erfüllt ist, d.h. dass der Wert kleiner als 0 ist. False bedeutet, dass die Bedingung nicht erfüllt ist, d.h. dass der Wert größer oder gleich 0 ist. Wir können nun überprüfen, ob es überhaupt Werte gibt, die kleiner als 0 sind, in dem wir z.B. die bereits bekannte Funktion any() nutzen.\n\n(df['kosten_plan'] < 0).any()\n\nTrue\n\n\nDa der Wert True zurückgegeben wird, gibt es Werte, die kleiner als 0 sind. Lassen Sie uns diese Werte nun betrachten. Das vorgehen ist ähnlich wie bei der Überprüfung auf Duplikate. Wir können die Werte mit dem Operator < filtern und anschließend die Zeilen mit den negativen Kostenwerten ausgeben. Wir nutzen hier eine komprimierte Schreibweise ohne die Zwischenspeicherung in einer Variablen mask.\n# Variante 1\nmask = df['kosten_plan'] < 0\ndf[mask]\n\n# Variante 2\ndf[df['kosten_plan'] < 0]\nDie Schreibweise ist ohne die Zwischenspeicherung in einer Variablen mask etwas kompakter und auch in der Praxis häufiger anzutreffen. Wir stellen an dieser Stelle noch eine weitere - und aus unserer Sicht elegantere - Möglichkeit vor, um die Zeilen mit negativen Kostenwerten auszugeben.\nWir können die Funktion query nutzen, um unseren Datensatz nach Bedingungen zu filtern. Die Bedingung wird als String übergeben. In unserem Fall wollen wir alle Zeilen ausgeben, die einen negativen Kostenwert haben. Die Bedingung lautet also kosten_plan < 0.\n\n# Datensatz nach Bedingung filtern\ndf.query('kosten_plan < 0')\n\n\n\n\n\n  \n    \n      \n      id\n      name\n      beginn\n      ende_plan\n      ende_ist\n      kosten_plan\n      kosten_ist\n      team\n    \n  \n  \n    \n      149\n      ZU-631931\n      Ubahn // Mohammed-Lehmann-Straße\n      2018-11-14\n      2019-04-02\n      2019-08-27\n      -97953.64\n      -138873.05\n      Team 1\n    \n    \n      187\n      UO-403669\n      Straßenbau // Cichoriusallee\n      2013-06-15\n      2013-09-03\n      2013-09-03\n      -6600.91\n      -11154.49\n      Team 2\n    \n    \n      203\n      TA-448220\n      Ubahn // Miesgasse\n      2018-10-27\n      2019-03-14\n      2019-06-01\n      -10881.99\n      -12512.99\n      Team 1\n    \n    \n      228\n      DD-996695\n      Landschaftsbau // Hertrampfstraße\n      2019-12-19\n      2019-12-20\n      2020-01-22\n      -12584.56\n      -16566.35\n      Team 2\n    \n    \n      290\n      XX-607121\n      Baumbestand // Sandro-Schönland-Weg\n      2016-10-08\n      2016-10-21\n      2016-10-23\n      -28178.22\n      -30112.10\n      Team 4\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      9850\n      MP-329043\n      Elektroarbeiten // Anica-Hövel-Ring\n      2014-08-01\n      2014-10-04\n      2014-10-01\n      -18000.60\n      -20083.99\n      Team 1\n    \n    \n      9874\n      FY-695742\n      Baumbestand // Trubinplatz\n      2020-11-05\n      2020-11-20\n      2020-11-22\n      -2681.19\n      -3792.85\n      Team 2\n    \n    \n      9917\n      BQ-152614\n      Baumbestand // Sylwia-Flantz-Gasse\n      2018-09-12\n      2018-09-15\n      2018-09-17\n      -40725.60\n      -46026.01\n      Team 1\n    \n    \n      9921\n      AG-880105\n      Landschaftsbau // Natalja-Sölzer-Platz\n      2013-11-03\n      2013-11-23\n      2013-11-25\n      -13156.08\n      -13550.75\n      Team 4\n    \n    \n      9964\n      UT-072682\n      Stadtpark // Roswita-Ebert-Weg\n      2014-10-18\n      2014-11-25\n      2014-11-24\n      -27341.83\n      -45652.98\n      Team 2\n    \n  \n\n213 rows × 8 columns\n\n\n\nEine nicht unerhebliche Anzahl an Projekten hat Kosten mit negativen Werten. Auch wird deutlich, dass die Kostenwerte für die Spalte kosten_ist dann ebenfalls negativ sind. Dies kann verschiedene Ursachen haben, z.B. dass die Kosten beim erfassen der Daten falsch eingegeben wurden. In Realität würde man hier weitere Informationen zu den Daten einholen (d.h. mit Fachexperten oder der Projektleitung) und Möglichkeiten zur Datenkorrektur zu eruieren. In unserem Fall werden wir die Daten eliminieren, um keinen Fehler in der weiteren Analyse zu erzeugen.\nDer einfachste Weg, die Zeilen mit negativen Vorzeichen zu eliminieren, ist es den Datensatz mit query zu filtern. Wir können die Bedingung kosten_plan >= 0 nutzen, um alle Zeilen auszugeben, die einen positiven Kostenwert haben.\n\ndf = df.query('kosten_plan >= 0')\n\nWir könnten nun die gleiche Überprüfung für die Spalte kosten_ist durchführen. Stattdessen filtern wir aber alle Zeilen, die einen negativen Kostenwert haben, aus dem Datensatz heraus.\n\ndf = df.query('kosten_ist >= 0')\n\nWir hätten die beiden letzten Schritte auch in einem Schritt durchführen können, indem wir die Bedingungen mit einem & verknüpft hätten.\ndf = df.query('kosten_plan >= 0 & kosten_ist >= 0')\nPlausibilitätsprüfung 3: Sind die Werte für die Datumsspalten (beginn, ende_plan und ende_ist) plausibel?\nBei der nun folgenden Überprüfung kommt uns zugute, dass wir die Spalten bereits in den Datentyp datetime konvertiert haben. Wir können nun die Werte für die Spalten beginn, ende_plan und ende_ist miteinander vergleichen.\nWir können zwei Überprüfungen durchführen:\n\nSind alle Projekte bereits beendet?\nLiegt das Datum für ende_plan nach dem Datum für beginn?\n\nLassen Sie uns mit der ersten Überprüfung beginnen. Wir können die Funktion max() nutzen, um das jüngste Datum in der Spalte ende_ist zu ermitteln. Dieses Datum sollte in der Vergangenheit liegen.\n\ndf['ende_ist'].max()\n\nTimestamp('2023-03-09 00:00:00')\n\n\nDas letzte Projekt wurde in der Vergangeheit beendet, d.h. wir können sicher sein, dass alle Projekte bereits beendet sind (und somit annehmen, dass auch alle Kosten bereits berücksichtigt wurden). Eine weitere Plausibiltätsprüfung ist, ob das Datum für ende_ist nach dem Datum für beginn liegt. Falls nicht, läge ein offensichtlicher Datenfehler vor. Wir können dies mit dem Operator < überprüfen. Da wir die Spalten bereits in den Datentyp datetime konvertiert haben, liegt diese Funktionalität bereits vor; mit dem Datentyp object wäre dies nicht möglich gewesen. Wir können also unseren Datensatz filtern für Zeilen, bei denen gilt, dass ende_ist kleiner oder gleich beginn ist.\n\ndf.query('ende_ist <= beginn')\n\n\n\n\n\n  \n    \n      \n      id\n      name\n      beginn\n      ende_plan\n      ende_ist\n      kosten_plan\n      kosten_ist\n      team\n    \n  \n  \n    \n      1251\n      TG-362669\n      Baumbestand // Cornelia-Schönland-Gasse\n      2017-08-16\n      2017-08-16\n      2017-08-16\n      55507.48\n      69877.32\n      Team 2\n    \n    \n      1584\n      CF-360814\n      Baumbestand // Adlerallee\n      2014-10-18\n      2014-10-19\n      2014-10-18\n      80258.30\n      80779.37\n      Team 4\n    \n    \n      2456\n      JV-632251\n      Baumbestand // Gabriel-Gude-Platz\n      2014-11-03\n      2014-11-03\n      2014-11-03\n      31973.47\n      35215.93\n      Team 3\n    \n    \n      2764\n      WS-799566\n      Stadtpark // Hesseplatz\n      2014-04-21\n      2014-04-22\n      2014-04-21\n      175422.45\n      270521.71\n      Team 2\n    \n    \n      3319\n      QC-047279\n      Stadtpark // Heiko-Pohl-Straße\n      2020-06-24\n      2020-07-12\n      2020-06-24\n      92345.10\n      117155.99\n      Team 3\n    \n    \n      3786\n      VL-461771\n      Landschaftsbau // Löfflerweg\n      2017-07-06\n      2017-07-07\n      2017-07-06\n      159544.85\n      160881.59\n      Team 1\n    \n    \n      4180\n      LN-394370\n      Ubahn // Miodrag-Harloff-Gasse\n      2021-03-28\n      2021-05-02\n      2021-03-28\n      196492.47\n      315066.68\n      Team 4\n    \n    \n      4192\n      TB-549699\n      Landschaftsbau // Ruppertweg\n      2020-12-20\n      2020-12-21\n      2020-12-20\n      112881.59\n      176922.78\n      Team 4\n    \n    \n      4426\n      GD-117672\n      Baumbestand // Editha-Roskoth-Allee\n      2021-11-30\n      2021-11-30\n      2021-11-30\n      18952.82\n      20637.53\n      Team 3\n    \n    \n      4732\n      JS-419276\n      Baumbestand // Noackstraße\n      2013-07-03\n      2013-07-03\n      2013-07-03\n      95666.74\n      102161.85\n      Team 4\n    \n    \n      5032\n      LN-496169\n      Elektroarbeiten // Kerstin-Kreusel-Gasse\n      2019-09-13\n      2019-09-13\n      2019-09-13\n      216360.70\n      376378.86\n      Team 2\n    \n    \n      5154\n      XN-964801\n      Baumbestand // Marc-Tlustek-Gasse\n      2014-06-01\n      2014-06-03\n      2014-06-01\n      32613.11\n      38129.52\n      Team 1\n    \n    \n      6035\n      NG-005840\n      Stadtpark // Kaulgasse\n      2018-09-12\n      2018-09-15\n      2018-09-12\n      98942.58\n      106455.00\n      Team 4\n    \n    \n      6046\n      VB-997519\n      Baumbestand // Isabella-Caspar-Allee\n      2021-09-04\n      2021-09-04\n      2021-09-04\n      47069.77\n      49991.91\n      Team 4\n    \n    \n      6139\n      KM-743270\n      Landschaftsbau // Hentschelgasse\n      2015-05-20\n      2015-05-20\n      2015-05-20\n      61717.02\n      69451.95\n      Team 2\n    \n    \n      6656\n      FH-375317\n      Baumbestand // Wilmsplatz\n      2013-11-02\n      2013-11-02\n      2013-11-02\n      13688.48\n      14638.31\n      Team 4\n    \n    \n      7118\n      OH-717871\n      Landschaftsbau // Horst-Günter-Tröst-Platz\n      2014-07-13\n      2014-07-13\n      2014-07-13\n      141555.26\n      146860.57\n      Team 1\n    \n    \n      7511\n      VH-400747\n      Stadtpark // Zahngasse\n      2018-01-05\n      2018-01-18\n      2018-01-05\n      136581.01\n      164558.76\n      Team 3\n    \n    \n      8262\n      BB-659680\n      Baumbestand // Schottinring\n      2020-07-29\n      2020-07-29\n      2020-07-29\n      50229.52\n      65361.78\n      Team 2\n    \n    \n      9047\n      XI-154940\n      Baumbestand // Inken-Staude-Weg\n      2017-07-25\n      2017-07-26\n      2017-07-25\n      88698.82\n      96346.85\n      Team 1\n    \n    \n      9071\n      JC-187276\n      Landschaftsbau // Röhrdanzstraße\n      2017-08-25\n      2017-08-25\n      2017-08-25\n      60098.23\n      58506.66\n      Team 1\n    \n    \n      10015\n      YH-661176\n      Baumbestand // Nohlmansweg\n      2017-02-12\n      2017-02-12\n      2017-02-12\n      50864.60\n      70050.16\n      Team 4\n    \n  \n\n\n\n\nSchauen wir uns die Projekte näher an, so stellt man fest, dass es sich ausschließlich um Projekte handelt, die am gleichen Tag begonnen und beendet wurden. Dies ist prinzipiell nicht ausgeschlossen, jedoch erscheint dies - zumindest für einige Projekte - sehr unwahrscheinlich, da gleichzeitig hohe Kosten veranschlagt wurden, was auf eine längere Projektlaufzeit schließen lässt. Wir müssen jetzt entscheiden, ob wir diese Projekte eliminieren wollen oder nicht. Da wir uns in diesem Fall nicht sicher sind, ob die Daten korrekt sind, werden wir die Projekte nicht eliminieren. Stattdessen fügen wir eine neue Spalte hinzu, die die Dauer des Projektes in Tagen angibt. Wir können dann im Rahmen unserer Analyse die Projekte mit einer Projektdauer von 0 Tagen herausfiltern und analysieren, ob dies unsere Ergebnisse beeinflusst.\n\ndf['dauer'] = df['ende_ist'] - df['beginn']\n\nWir haben nun einige wesentliche und offensichtliche Aufbereitugnsschritte durchlaufen und können mit der eigentlichen Analyse der Daten beginnen."
  },
  {
    "objectID": "Chapters/04_Chapter/Einlesen_Aufbereiten.html#übersicht-genutzter-methoden",
    "href": "Chapters/04_Chapter/Einlesen_Aufbereiten.html#übersicht-genutzter-methoden",
    "title": "Einlesen und Aufbereiten",
    "section": "Übersicht genutzter Methoden",
    "text": "Übersicht genutzter Methoden\nEs wurden folgende Funktionen und Methoden genutzt:\n\nread_csv(): Einlesen der Daten\nhead(): Anzeigen der ersten Zeilen\ntail(): Anzeigen der letzten Zeilen\ninfo(): Anzeigen der Datentypen und der Anzahl der nicht fehlenden Werte\nrename(): Umbenennen der Spalten\nto_datetime(): Konvertieren der Spalten in den Datentyp datetime\nisna(): Überprüfen, ob Werte fehlen\nany(): Überprüfen, ob Spalte oder Zeile False enthält\ndropna(): Eliminieren von Zeilen mit fehlenden Werten\nduplicated(): Überprüfen, ob es Duplikate gibt\ndrop_duplicates(): Eliminieren von Duplikaten\nunique(): Ermitteln der einzigartigen Werte\nquery(): Filtern von Zeilen nach Bedingung\nmax(): Ermitteln des höchsten Wertes\n\nDies ist ein Test"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "LITERATUR",
    "section": "",
    "text": "Almazmomi, Najah, Aboobucker Ilmudeen, and Alaa A. Qaffas. 2021.\n“The Impact of Business Analytics Capability on Data-Driven\nCulture and Exploration: Achieving a Competitive Advantage.”\nBenchmarking: An International Journal 29 (4): 1264–83. https://doi.org/10.1108/BIJ-01-2021-0021.\n\n\nGluchowski, Peter. 2016. “Business Analytics –\nGrundlagen, Methoden Und\nEinsatzpotenziale.” HMD Praxis Der\nWirtschaftsinformatik 53 (3): 273–86. https://doi.org/10.1365/s40702-015-0206-5.\n\n\nMcAfee, Andrew, and Erik Brynjolfsson. 2017. Machine, Platform,\nCrowd: Harnessing Our Digital Future. First edition. New York: W.W.\nNorton & Company.\n\n\nPopovič, Aleš, Ray Hackney, Rana Tassabehji, and Mauro Castelli. 2018.\n“The Impact of Big Data Analytics on Firms’ High Value Business\nPerformance.” Information Systems Frontiers 20 (2):\n209–22. https://doi.org/10.1007/s10796-016-9720-4.\n\n\nSeiter, Mischa. 2019. Business Analytics:\nWie Sie Daten Für Die\nSteuerung von Unternehmen Nutzen. 2.,\nkomplett überarbeitete und erweiterte. München: Vahlen.\n\n\nShanks, Graeme, Rajeev Sharma, Peter Seddon, and Peter Reynolds. 2010.\n“The Impact of Strategy and\nMaturity on Business Analytics\nand Firm Performance: A\nReview and Research\nAgenda.” ACIS 2010 Proceedings, January. https://aisel.aisnet.org/acis2010/51.\n\n\nWickham, Hadley, and Garrett Grolemund. 2016. R for\nData Science: Import,\nTidy, Transform, Visualize, and\nModel Data. \"O’Reilly Media, Inc.\"."
  },
  {
    "objectID": "Appendices/Modules/Pandas.html#sec-pandas",
    "href": "Appendices/Modules/Pandas.html#sec-pandas",
    "title": "Pandas",
    "section": "Allzweckwaffe für Datenanalyse",
    "text": "Allzweckwaffe für Datenanalyse\nIm Kontext der Datenanalyse haben wir es oft mit tabularischen Daten zu tun, d.h. wir arbeiten mit Informationen und/oder Daten, die in Tabellen gespeichert sind. Typischerweise haben diese Informationen Beschriftungen und sind von unterschiedlichen Typen. Stellen wir uns dies in Excel vor: wir haben mehrere Spalten, jede Spalte hat eine Bezeichnung, die beschreibt, um welche Information es sich handelt. Einige Spalten haben numerische Informationen (z.B. Umsätze), andere haben text-basierte Informationen (z.B. Namen oder Orte), wieder andere haben ein Datum als Spalteninhalt. Pandas ist das Modul, welches für den Umgang mit dieser Art von Daten gemacht wurde.\nPandas unterstützt alle Schritte des Analyseprozesses, in dem es hierfür sinnvolle Funktionalitäten bereitstellt. Die gesamte Bandbreite an Funktionalitäten ist so groß, dass es unmöglich ist, diese im Rahmen eines Kurses zu erläutern und vorzustellen. Zusätzlich ist es so, dass Pandas am Besten im Zusammenspiel mit anderen Modulen (z.B. mit dem Visualisierungsmodul seaborn) eingesetzt wird.\nIn diesem Kapitel werden wir einige wichtige Grundlagen von Pandas vorstellen. Wir nutzen hierfür - wie in den vorherigen Kapiteln auch - kleine Beispieldaten. Im nächsten Kapitel werden wir dann realistischere Datensätze verwenden, um wichtige Konzepte quasi am “lebenden Objekt” zu erlernen. Wir glauben, dass Sie zunächst einige wichtige Grundlagen verstanden haben müssen. Um diese zu erläutern benötigen wir keine “echten” Daten. Ab einem gewissen Punkt geht es jedoch nicht mehr darum isolierte Funktionen vorzustellen, sondern vielmehr den tatsächlichen Prozess der Datenanalyse zu durchlaufen, um dann auf reale Probleme zu stoßen und praxirelevante Lösungsansätze vorzustellen."
  },
  {
    "objectID": "Appendices/Modules/Pandas.html#grundlagen",
    "href": "Appendices/Modules/Pandas.html#grundlagen",
    "title": "Pandas",
    "section": "Grundlagen",
    "text": "Grundlagen\nWir stellen im Folgenden ausgewählte Grundlagen des Moduls vor. Hierbei fokussieren wir uns auf\n\ndas Importieren des Moduls\ndas Erstellen von Series und Dataframes\n\n\nImportieren von Pandas\nBevor wir Pandas nutzen können, müssen wir das Modul zunächst importieren1. Wir können hier prinzipiell wieder vorgehen, wie wir dies in Kapitel Kapitel 6.5.2 bereits kennengelertn haben. Pandas wird typischerweise als gesamtes Modul mit einem Alias importiert. Der Alias pd ist dabei der Standard, der von 99% der Pandas-Nutzer verwendet wird.1 Wir gehen davon aus, dass das Modul bereits installiert ist.\n\nimport pandas as pd\n\n\n\nSeries und Dataframes\nDas Herzstück von Pandas ist der sogenannte Dataframe (pd.Dataframe). Sie können sich ein Dataframe als eine Art Excel-Sheet bzw. eine Excel-Tabelle vorstellen. So wie in Excel jede Tabelle aus einer Ansammlung aus Spalten besteht, ist ein Dataframe in Pandas eine Ansammlung von Series (pd.Series).\nSchauen wir uns das untenstehende Excelbeispiel an.\n\n\n\nBeispiel: Excel\n\n\nDie abgebildete Excel-Tabelle ist in dieser Analogie ein dataframe. Jeder ihrer Spalten eine series. Die Spalte “Preis” wäre eine Series mit floats, d.h. mit Dezimalzahlen. Die Spalte “Datum” wäre eine Series vom Typ str oder von einem Typ, der ein Datum abbilden kann (diesen Datentyp werden wir in diesem Kapitel noch kennenlernen).\nSchauen wir uns an, wie wir diese Daten in pandas abbilden."
  },
  {
    "objectID": "Appendices/Modules/Pandas.html#erstellen-von-series",
    "href": "Appendices/Modules/Pandas.html#erstellen-von-series",
    "title": "Pandas",
    "section": "Erstellen von Series",
    "text": "Erstellen von Series\nWenn wir die Spalte Menge nachbilden möchten, so können wir z.B. folgendes schreiben:\n\nmenge = pd.Series([23, 1, 12, 6, 89,4], name=\"Menge\")\nmenge\n\n0    23\n1     1\n2    12\n3     6\n4    89\n5     4\nName: Menge, dtype: int64\n\n\nDie Series hat den Namen Menge und enthält die Werte 23, 1, 12, 6, 89 und 4. Die Werte sind alle vom Typ int.2 Darüber hinaus beinhaltet die Series einen Index. Der Index ist eine Art Beschriftung der einzelnen Werte. In diesem Fall ist der Index eine fortlaufende Nummerierung. Ein Index in Python beginnt immer bei 0.2 Hinweis:genauer gesagt vom Typ int64. Die Zahl hinter int macht deutlich, dass es eine 64-bit Integer ist und gibt uns damit Aufschluss über den abbildbaren Wertebereich (und damit auch über den benötigten Speicher) des Datentyps. Diese technischen Feinheiten sind für unsere Zwecke an dieser Stelle nicht relevant. Weitere Informationen dazu finden Sie z.B. hier.\nErstellen wir nun eine Series, die die Spalte Preis abbildet und speichern diese in der Variable “preis”. Wir schreiben den ersten Wert mit Dezimalpunkt, da die Series dann automatisch erkennt, dass es sich um den Datentyp float handelt.\n\npreis = pd.Series([10., 5, 9, 3, 5, 10], name=\"Preis\") # Der Dezimalpunkt signalisiert \"float\"\npreis\n\n0    10.0\n1     5.0\n2     9.0\n3     3.0\n4     5.0\n5    10.0\nName: Preis, dtype: float64\n\n\nWir könnten den Datentyp auch explizit angeben. Der untenstehende Code gibt z.B. an, dass die Series vom Datentyp str sein soll, so dass die Elemente Texte sind. In Pandas wird dies dann als object bezeichnet.\n\npreis_str = pd.Series([10., 5, 9, 3, 5, 10], name=\"Preis\", dtype=\"object\")\npreis_str \n\n0    10.0\n1       5\n2       9\n3       3\n4       5\n5      10\nName: Preis, dtype: object"
  },
  {
    "objectID": "Appendices/Modules/Pandas.html#erstellen-von-dataframes",
    "href": "Appendices/Modules/Pandas.html#erstellen-von-dataframes",
    "title": "Pandas",
    "section": "Erstellen von Dataframes",
    "text": "Erstellen von Dataframes\nDataframes können manuell erstellt werden oder aus externen und einzulesenden Daten erstellt werden. Wir gehen hier zunächst auf die manuelle Erstellung ein. Diese benötigen wir in der Praxis oft gar nicht, weil wir meist auf externe Daten (z.B. Excel-, CSV-, SQL- oder TXT-Dateien) zugreifen. Dennoch ist es wichtig zu verstehen, wie Dataframes manuell erstellt werden können, da dies (i) hilft, den grundsätzlichen Aufbau von Dataframes zu verstehen und (ii) oft nützlich ist, um kleinere Datensätze zu Testzwecken zu erzeugen.\n\nManuelle Erstellung\nWir können Dataframes auf verschiedene Art und Weisen erstellen. Wir gehen hier auf eine Variante ein, die oft genutzt wird und die aus unserer Sicht aller meisten Anwendungsfälle abdeckt: wir erstellen Dataframes über sogenannte Dictionaries (dict). Ein Python-Dictionary ist ein Datentyp, der eine Sammlung von Schlüssel-Wert-Paaren darstellt. Jeder Schlüssel ist eindeutig und wird verwendet, um seinen zugehörigen Wert abzurufen. Hier ist ein Beispiel:\n\ntelefonbuch = {\"Max\": \"555-555-1212\", \n                \"Anna\": \"555-555-1213\", \n                \"Peter\": \"555-555-1214\"}\n\nIn diesem Beispiel ist das Dictionary telefonbuch mit drei Schlüssel-Wert-Paaren gefüllt, wobei jeder Name (Max, Anna, Peter) einen Telefonnummer-Wert hat. WIr können jedoch auch (fast) beliebige Datentypen als Werte verwenden.\n\nfamilienmitglieder = {\"Max\": [\"Frau\", \"Kind1\",\"Kind2\"], \n                     \"Anna\": [\"Mann\", \"Kind\"], \n                     \"Peter\": []} # Peter ist single...\n\nWenn wir ein Dataframe mittels Dictionary erzeugen, dann werden die Schlüssel als Spaltenbezeichnung übernommen und die Werte dann als Daten unterhalb der Spaltenbezeichnung eingelesen.\nHier der Beispielcode für ein Dataframe, welcher die Spalten (in dem Falle Series) “Preis” und “Menge” beinhaltet.\n\ndf = pd.DataFrame({\"Menge\": [23, 1, 12, 6, 89, 4],    \n                   \"Preis\":[10., 5, 9, 3, 5, 10]})\ndf\n\n\n\n\n\n  \n    \n      \n      Menge\n      Preis\n    \n  \n  \n    \n      0\n      23\n      10.0\n    \n    \n      1\n      1\n      5.0\n    \n    \n      2\n      12\n      9.0\n    \n    \n      3\n      6\n      3.0\n    \n    \n      4\n      89\n      5.0\n    \n    \n      5\n      4\n      10.0\n    \n  \n\n\n\n\nWir sehen, dass die Schlüssel Menge und Preis als Spaltenbezeichnung übernommen wurden. Die einzelnen Werte sind im obigen Fall von Datentyp list (eine Liste).\nWir können hier jedoch auch andere Datentypen nutzen. So können wir auch Series einfügen. Dies ist sehr praktisch, da wir im Rahmen unser Analysen mit verschiedenen Datentypen arbeiten werden.\n\ndf = pd.DataFrame({\"Menge\": menge,    \n                   \"Preis\":preis})\ndf \n\n\n\n\n\n  \n    \n      \n      Menge\n      Preis\n    \n  \n  \n    \n      0\n      23\n      10.0\n    \n    \n      1\n      1\n      5.0\n    \n    \n      2\n      12\n      9.0\n    \n    \n      3\n      6\n      3.0\n    \n    \n      4\n      89\n      5.0\n    \n    \n      5\n      4\n      10.0\n    \n  \n\n\n\n\n\n\nEinlese von Daten\nWie bereits angesprochen ist es realistisch, dass wir bereits über Daten verfügen und diese in Pandas einlesen möchten. Hierfür bietet Pandas eine Vielzahl an Methoden, die es ermöglicht nahezu jeden Dateityp einzulesen. Wir werden uns in den nächsten Kapiteln auf das Einlesen von Excel- und CSV-Dateien beschränken. Das grundsätzliche Vorgehen ist jedoch - unabhängig vom Dateityp - sehr ähnlich.\nPandas bietet die Methoden zum Einlesen von externen Daten via pd.read_ an.\nWir können uns über pd.read_<tab> anzeigen lassen, welche Funktionen zur Verfügung stehen und uns die gewünschte raussuchen.\n\n\n\nBeispiel: read-Funktionen\n\n\nZum Einlesen der gewünschten Daten benötigen wir typischerweise den Dateipfad, d.h. den Ort, in welchem die Datei auf dem Computer oder in der Cloud hinterlegt ist.\nBeispiel 1: Einlesen einer Excel-Datei mit relativer Pfad-Angabe\n\nfpath = \"../../_data/sales.xlsx\"\nsales = pd.read_excel(fpath)\nsales\n\n\n\n\n\n  \n    \n      \n      Produkt\n      Preis\n      Menge\n      Datum\n    \n  \n  \n    \n      0\n      Apple\n      0.79\n      10\n      2022-01-01\n    \n    \n      1\n      Orange\n      1.99\n      1\n      2022-01-02\n    \n    \n      2\n      Orange\n      1.99\n      5\n      2022-01-03\n    \n    \n      3\n      Banana\n      0.79\n      1\n      2022-01-04\n    \n    \n      4\n      Apple\n      0.79\n      7\n      2022-01-05\n    \n    \n      5\n      Orange\n      1.99\n      2\n      2022-01-06\n    \n    \n      6\n      Banana\n      0.79\n      9\n      2022-01-07\n    \n    \n      7\n      Apple\n      1.99\n      4\n      2022-01-08\n    \n    \n      8\n      Apple\n      2.49\n      8\n      2022-01-09\n    \n    \n      9\n      Orange\n      2.49\n      7\n      2022-01-010\n    \n  \n\n\n\n\nDer Dateipfad ist immer relativ zum aktuellen Arbeitsverzeichnis des Notebooks, in dem wir arbeiten. In dem angegebenen Dateipfad ../../_data/sales.xlsx bezieht sich ../ auf ein Verzeichnis, das eine Ebene höher als das aktuelle Verzeichnis liegt. Da es zwei Mal ../ im Pfad gibt, befindet sich die Datei sales.xlsx in einem Verzeichnis, das zwei Ebenen höher als das aktuelle Verzeichnis liegt. Der Unterordner _data befindet sich in diesem höher liegenden Verzeichnis. Daher befindet sich die Datei sales.xlsx in dem Unterverzeichnis _data einer Ebene höher als das aktuelle Verzeichnis.\n\n\n\n\n\n\nBeispiel Dateipfad\n\n\n\n|--- _data  \n       |_sales1.xlsx  \n|--- Chapters  \n       |_ Chapter_01  \n       |_ Chapter_02  \n           |_ Notebook.ipynb  \n\n\nDa die Pfadangaben sich zwischen den Betriebssystemen unterscheiden, ist es empfehlenswert, die pathlib-Bibliothek zu verwenden, um die Pfadangaben zu erstellen.\n\nfrom pathlib import Path\nfpath = Path.cwd() /\"..\" / \"..\" /  \"_data\" / \"sales.xlsx\"\nsales = pd.read_excel(fpath)\nsales\n\n\n\n\n\n  \n    \n      \n      Produkt\n      Preis\n      Menge\n      Datum\n    \n  \n  \n    \n      0\n      Apple\n      0.79\n      10\n      2022-01-01\n    \n    \n      1\n      Orange\n      1.99\n      1\n      2022-01-02\n    \n    \n      2\n      Orange\n      1.99\n      5\n      2022-01-03\n    \n    \n      3\n      Banana\n      0.79\n      1\n      2022-01-04\n    \n    \n      4\n      Apple\n      0.79\n      7\n      2022-01-05\n    \n    \n      5\n      Orange\n      1.99\n      2\n      2022-01-06\n    \n    \n      6\n      Banana\n      0.79\n      9\n      2022-01-07\n    \n    \n      7\n      Apple\n      1.99\n      4\n      2022-01-08\n    \n    \n      8\n      Apple\n      2.49\n      8\n      2022-01-09\n    \n    \n      9\n      Orange\n      2.49\n      7\n      2022-01-010\n    \n  \n\n\n\n\nBeispiel 2: Einlesen einer Excel-Datei via Link\n\nlink = \"https://www.dropbox.com/s/oub2za2hu8yp8oj/sales1.xlsx?dl=1\"\n\ndf = pd.read_excel(link)\ndf\n\n\n\n\n\n  \n    \n      \n      Produkt Nr.\n      Preis\n      Menge\n      Datum\n    \n  \n  \n    \n      0\n      3\n      10\n      23\n      2021-01-01\n    \n    \n      1\n      2\n      5\n      1\n      2021-01-02\n    \n    \n      2\n      1\n      9\n      12\n      2021-01-03\n    \n    \n      3\n      4\n      3\n      6\n      2021-01-04\n    \n    \n      4\n      2\n      5\n      89\n      2021-01-05\n    \n    \n      5\n      3\n      10\n      4\n      2021-01-06\n    \n  \n\n\n\n\nWenn sich die Datei auf einem Server oder einer Cloud befindet (hier: Dropbox), dann können wir die Datei auch direkt via Link einlesen. Der Vorteil ist, dass wir uns keine Gedanken darüber machen müssen, wo die Datei auf dem Computer gespeichert ist. Der Nachteil ist jedoch, dass wir (i) die Datei nicht lokal speichern können und (ii) die Ladezeiten oft länger sind.\nDie jeweiligen pd.read_-Methoden haben eine Vielzahl von optionalen Parametern, die man übergeben kann. Wir werden an dieser Stelle nicht auf alle eingehen. Grundsätzlich können Sie aber quasi davon ausgehen, dass es für alle typischen Optionen, die man beim Einlesen von Daten benötigt (z.B. nur bestimmte Spalten, nur bestimmte Tabellenblätter, Auswahl der Spaltenbezeichnung, Auswahl von Datentypen, Datumsformat) eine optionale Auswahlmöglichkeit innerhalb der Methode gibt. Sie können diese via pd.read_<..>? aufrufen lassen."
  },
  {
    "objectID": "Appendices/Modules/Pandas.html#aufbau-eines-dataframes",
    "href": "Appendices/Modules/Pandas.html#aufbau-eines-dataframes",
    "title": "Pandas",
    "section": "Aufbau eines Dataframes",
    "text": "Aufbau eines Dataframes\nEin Dataframe ist immer eine Tabelle, mit zwei Achsen:\n\nvertikale Achse: wird als Index bezeichnet\nhorizontale Achse: werden als Columns (d.h. Spalten) bezeichne\n\nDer Index eines Dataframes ist standardmäßig numerisch und beginnt bei 0. Der Index ist vergleichbar mit den Zeilenbezeichnungen in Excel.\n\n\n\nBeispiel: Index in Excel\n\n\nWir greifen auf Index und Spalten wie folgt zu:\n\nidx = df.index\nidx\n\nRangeIndex(start=0, stop=6, step=1)\n\n\n\ncols = df.columns\ncols\n\nIndex(['Produkt Nr.', 'Preis', 'Menge', 'Datum'], dtype='object')\n\n\nÜber diese Logik können wir auch neue Werte für den Index oder die Spaltenbezeichnung erstellen.\n\ndf.index = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"]\ndf.columns = [\"H1\", \"H2\", \"H3\", \"H4\", ]\ndf\n\n\n\n\n\n  \n    \n      \n      H1\n      H2\n      H3\n      H4\n    \n  \n  \n    \n      A\n      3\n      10\n      23\n      2021-01-01\n    \n    \n      B\n      2\n      5\n      1\n      2021-01-02\n    \n    \n      C\n      1\n      9\n      12\n      2021-01-03\n    \n    \n      D\n      4\n      3\n      6\n      2021-01-04\n    \n    \n      E\n      2\n      5\n      89\n      2021-01-05\n    \n    \n      F\n      3\n      10\n      4\n      2021-01-06"
  },
  {
    "objectID": "Appendices/Modules/Pandas.html#auswahl-von-daten",
    "href": "Appendices/Modules/Pandas.html#auswahl-von-daten",
    "title": "Pandas",
    "section": "Auswahl von Daten",
    "text": "Auswahl von Daten\nIm Rahmen einer Datenanalyse wird es vorkommen, dass wir uns auf bestimmte Zeilen oder Spalten eines Dataframes beziehen wollen. Schauen wir uns an, wie wir dies in Python machen.\nLeider gibt es verschiedene Ansätze, um Daten zu selektieren. Wir werden hier ein paar gängige Ansätze vorstellen. Im Rahmen ihrer Online-Recherchen werden Sie ggf. auf weitere Ansätze stoßen. Hierbei gibt es oft kein “richtig” oder “falsch”. Schauen Sie, was für Sie ein passender Ansatz ist. Wir geben darüber hinaus an den relevanten Stellen Hinweise, wenn bestimmte Ansätze sich als besonders geeignet herausstellen.\n\nAuswahl von Spalten\nEin einfacher und gängiger Weg, bestimmte Spalten zu selektieren, ist die Auswahl via Spaltenname.\nDatensatz:\n\ndf = sales.copy()\ndf\n\n\n\n\n\n  \n    \n      \n      Produkt\n      Preis\n      Menge\n      Datum\n    \n  \n  \n    \n      0\n      Apple\n      0.79\n      10\n      2022-01-01\n    \n    \n      1\n      Orange\n      1.99\n      1\n      2022-01-02\n    \n    \n      2\n      Orange\n      1.99\n      5\n      2022-01-03\n    \n    \n      3\n      Banana\n      0.79\n      1\n      2022-01-04\n    \n    \n      4\n      Apple\n      0.79\n      7\n      2022-01-05\n    \n    \n      5\n      Orange\n      1.99\n      2\n      2022-01-06\n    \n    \n      6\n      Banana\n      0.79\n      9\n      2022-01-07\n    \n    \n      7\n      Apple\n      1.99\n      4\n      2022-01-08\n    \n    \n      8\n      Apple\n      2.49\n      8\n      2022-01-09\n    \n    \n      9\n      Orange\n      2.49\n      7\n      2022-01-010\n    \n  \n\n\n\n\nHier ein Beispiel für die Selektion einer Spalte. Das Resultat ist eine Series.\n\n# Auswahl der Spalte \"Menge\"\nmenge = df[\"Menge\"]\nmenge\n\n0    10\n1     1\n2     5\n3     1\n4     7\n5     2\n6     9\n7     4\n8     8\n9     7\nName: Menge, dtype: int64\n\n\n\n\n\n\n\n\nAchtung: Auswahl von Spalten\n\n\n\nSie werden in verschiedenen Quellen auch die folgende Variante finden\nmenge = df.Menge\nDer Ansatz ist grundsätzlich korrekt, jedoch nicht empfehlenswert. Hintergrund ist, dass dieser nicht funkioniert, wenn Spaltenbezeichnungen mit einer Zahl beginnen oder Leerzeichen beinhalten. Eine Spalte Produkt Nr. ist so bspw. nicht selektierbar. Sie müssen also auf die von uns präferierte Variante zurückgreifen. Insofern bietet sich die von uns dargestellte Notation besser an, da diese immer funktioniert.\n\n\nNatürlich können wir aber auch mehrere Spalten gleichzeitig auswählen. Wir machen dies, in dem wir eine Liste mit Spaltennamen übergeben. Das Resultat ist dann ein Dataframe.\n\n# Auswahl der Spalten \"Menge\" und \"Preis\"\nmenge_preis = df[[\"Menge\", \"Preis\"]]\nmenge_preis\n\n\n\n\n\n  \n    \n      \n      Menge\n      Preis\n    \n  \n  \n    \n      0\n      10\n      0.79\n    \n    \n      1\n      1\n      1.99\n    \n    \n      2\n      5\n      1.99\n    \n    \n      3\n      1\n      0.79\n    \n    \n      4\n      7\n      0.79\n    \n    \n      5\n      2\n      1.99\n    \n    \n      6\n      9\n      0.79\n    \n    \n      7\n      4\n      1.99\n    \n    \n      8\n      8\n      2.49\n    \n    \n      9\n      7\n      2.49\n    \n  \n\n\n\n\nWir können die gleiche Funktionalität auch mit einer speziellen Funktion .filter erreichen.\n\ndf.filter(items=[\"Menge\", \"Preis\"])\n\n\n\n\n\n  \n    \n      \n      Menge\n      Preis\n    \n  \n  \n    \n      0\n      10\n      0.79\n    \n    \n      1\n      1\n      1.99\n    \n    \n      2\n      5\n      1.99\n    \n    \n      3\n      1\n      0.79\n    \n    \n      4\n      7\n      0.79\n    \n    \n      5\n      2\n      1.99\n    \n    \n      6\n      9\n      0.79\n    \n    \n      7\n      4\n      1.99\n    \n    \n      8\n      8\n      2.49\n    \n    \n      9\n      7\n      2.49\n    \n  \n\n\n\n\nDie Funktion bietet weitere Möglichkeiten. So können wir Spalten auch nach bestimmten (einfachen) Logiken filtern. Wir könnten z.B. interessiert sein an allen Spalten, die mit “P” beginnen. Dies können wir einfach erreichen mit\n\ndf.filter(like=\"P\")\n\n\n\n\n\n  \n    \n      \n      Produkt\n      Preis\n    \n  \n  \n    \n      0\n      Apple\n      0.79\n    \n    \n      1\n      Orange\n      1.99\n    \n    \n      2\n      Orange\n      1.99\n    \n    \n      3\n      Banana\n      0.79\n    \n    \n      4\n      Apple\n      0.79\n    \n    \n      5\n      Orange\n      1.99\n    \n    \n      6\n      Banana\n      0.79\n    \n    \n      7\n      Apple\n      1.99\n    \n    \n      8\n      Apple\n      2.49\n    \n    \n      9\n      Orange\n      2.49\n    \n  \n\n\n\n\n\n\nAuswahl von Zeilen\nSie können auch spezifische Zeilen auswählen. Die üblichste Variante ist dies über die Methoden iloc und loc zu machen.\n\nAuswahl auf Basis von Index- oder Spaltennumerierung: iloc\nAuswahl auf Basis von Index- oder Spaltenbezeichnung: loc\n\nWichtig: auch in Pandas ist das erste Elemente wie bei allen anderen Datentypen für die dies relevant war sowohl für Index, als auch für Columns immer an der Stelle 0.\nGerade für Beginner ist die Auswahl via iloc und loc zunächst verwirrend. Lassen Sie uns deshalb einige typische Beispiele durchgehen.\nZur Übersicht stellen wir hier noch einmal unseren Ausgangsdatensatz dar:\n\n\n\n\n\n\n  \n    \n      \n      Produkt\n      Preis\n      Menge\n      Datum\n    \n  \n  \n    \n      0\n      Apple\n      0.79\n      10\n      2022-01-01\n    \n    \n      1\n      Orange\n      1.99\n      1\n      2022-01-02\n    \n    \n      2\n      Orange\n      1.99\n      5\n      2022-01-03\n    \n    \n      3\n      Banana\n      0.79\n      1\n      2022-01-04\n    \n    \n      4\n      Apple\n      0.79\n      7\n      2022-01-05\n    \n    \n      5\n      Orange\n      1.99\n      2\n      2022-01-06\n    \n    \n      6\n      Banana\n      0.79\n      9\n      2022-01-07\n    \n    \n      7\n      Apple\n      1.99\n      4\n      2022-01-08\n    \n    \n      8\n      Apple\n      2.49\n      8\n      2022-01-09\n    \n    \n      9\n      Orange\n      2.49\n      7\n      2022-01-010\n    \n  \n\n\n\n\nBeispiel 1: Auswahl der Zeilen 0, 3 und 5\n\ndf.iloc[[0, 3, 5]]\n\n\n\n\n\n  \n    \n      \n      Produkt\n      Preis\n      Menge\n      Datum\n    \n  \n  \n    \n      0\n      Apple\n      0.79\n      10\n      2022-01-01\n    \n    \n      3\n      Banana\n      0.79\n      1\n      2022-01-04\n    \n    \n      5\n      Orange\n      1.99\n      2\n      2022-01-06\n    \n  \n\n\n\n\nBeispiel 2: Auswahl der ersten 3 Zeilen\n\ndf.iloc[:3]\n\n\n\n\n\n  \n    \n      \n      Produkt\n      Preis\n      Menge\n      Datum\n    \n  \n  \n    \n      0\n      Apple\n      0.79\n      10\n      2022-01-01\n    \n    \n      1\n      Orange\n      1.99\n      1\n      2022-01-02\n    \n    \n      2\n      Orange\n      1.99\n      5\n      2022-01-03\n    \n  \n\n\n\n\nBeispiel 3: Auswahl der ersten 4 Zeilen und der Spalten 0 und 2\n\ndf.iloc[:4, [0, 2]]\n\n\n\n\n\n  \n    \n      \n      Produkt\n      Menge\n    \n  \n  \n    \n      0\n      Apple\n      10\n    \n    \n      1\n      Orange\n      1\n    \n    \n      2\n      Orange\n      5\n    \n    \n      3\n      Banana\n      1\n    \n  \n\n\n\n\nBeispiel 4: Auswahl der ersten drei Zeilen und Spalten\n\ndf.iloc[:3, :3]\n\n\n\n\n\n  \n    \n      \n      Produkt\n      Preis\n      Menge\n    \n  \n  \n    \n      0\n      Apple\n      0.79\n      10\n    \n    \n      1\n      Orange\n      1.99\n      1\n    \n    \n      2\n      Orange\n      1.99\n      5\n    \n  \n\n\n\n\nDie oberen Beispiele haben wir iloc genutzt: wir greifen jeweils auf die Numerierung der Spalte oder des Index zu. D.h. wir greifen auf z.B. Zeile 2 oder Spalte 0 zu.\nSobald wir auf den Index oder die Spalte mit einer bestimmten Bezeichnung zugreifen wollen, dann müssen wir loc nutzen. Dies ist insbesondere im Falle der Spalten sehr häufig der Fall.\nBeispiel 5: Auswahl der Zeilen 2 und 4 und der Spalten “Menge” und “Preis”\n\ndf.loc[[2, 4], [\"Menge\", \"Preis\"]]\n\n\n\n\n\n  \n    \n      \n      Menge\n      Preis\n    \n  \n  \n    \n      2\n      5\n      1.99\n    \n    \n      4\n      7\n      0.79\n    \n  \n\n\n\n\nIn vielen Anwendungsfällen wollen wir die Spalten über die Spaltenbezeichnungen auswählen, d.h. wir müssen loc nutzen. In weniger häufigen, aber durch aus teilweise relevanten Anwendungsfällen ist ggf. auch der Index nicht standardmäßig nummeriert. Auch dann müssen wir loc nutzen.\nHier ein Beispiel mit einem angepassten Index:\n\ndf.index = list(\"abcdefghij\") # Abkürzung für [\"a\", \"b\", ..., \"e\"]\ndf\n\n\n\n\n\n  \n    \n      \n      Produkt\n      Preis\n      Menge\n      Datum\n    \n  \n  \n    \n      a\n      Apple\n      0.79\n      10\n      2022-01-01\n    \n    \n      b\n      Orange\n      1.99\n      1\n      2022-01-02\n    \n    \n      c\n      Orange\n      1.99\n      5\n      2022-01-03\n    \n    \n      d\n      Banana\n      0.79\n      1\n      2022-01-04\n    \n    \n      e\n      Apple\n      0.79\n      7\n      2022-01-05\n    \n    \n      f\n      Orange\n      1.99\n      2\n      2022-01-06\n    \n    \n      g\n      Banana\n      0.79\n      9\n      2022-01-07\n    \n    \n      h\n      Apple\n      1.99\n      4\n      2022-01-08\n    \n    \n      i\n      Apple\n      2.49\n      8\n      2022-01-09\n    \n    \n      j\n      Orange\n      2.49\n      7\n      2022-01-010\n    \n  \n\n\n\n\nBeispiel 6: Auswahl der Zeilen “a” und “c” und der Spalten “Menge” und “Preis”\n\ndf.loc[[\"a\", \"c\"], [\"Menge\", \"Preis\"]]\n\n\n\n\n\n  \n    \n      \n      Menge\n      Preis\n    \n  \n  \n    \n      a\n      10\n      0.79\n    \n    \n      c\n      5\n      1.99"
  }
]